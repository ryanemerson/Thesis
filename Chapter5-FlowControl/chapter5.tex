\chapter{ABcast Flow Control}

% **************************** Define Graphics Path **************************
    \graphicspath{{Chapter5-FlowControl/Figs/Vector/}{Chapter5-FlowControl/Figs/}}

This chapter introduces a flow control protocol, AFC, that has been designed specifically for use with the \textsf{ABcast} protocol.  It utilises the measurements of \textsf{ABcast}'s DMC to dynamically control a node's broadcast rate in the furtherance of avoiding a recipient node becoming overawed by network traffic and suffering from a buffer overflow, or similarly preventing the underlying network from becoming saturated by outgoing broadcasts.  

The remainder of this chapter is structured as follows: First we introduce the rational behind our approach, stating the limitations of the existing flow control protocol provided by JGroups and why this is not suitable for \textsf{ABcast}.  This is followed by an in-depth look at how AFC works and the calculations required to control a node's broadcast rate.  We then briefly describe how AFC is implemented, before discussing the current limitations of our solution.  

\section{Rational}
The \textsf{ABcast} protocol described in this section functions as expected when each node's throughput is low and the number of requests per second is approximately five hundred.  However, the \textsf{ABcast} protocol discussed thus far has no flow-control, therefore as the number of requests per second increases, the protocol starts to become saturated by requests and performance deteriorates.  If the broadcast rate of a node is not restricted over a large period of time (or number of messages), it is possible for a 'snowball' effect to occur causing the system to become irrevocably unstable.  We define a 'snowball' effect as a situation whereby the load on the network continues to increase, causing latencies to become increasingly large until the system can no longer function.  In such an event, the sending of additional messages in an attempt to mitigate the problem only exasperates the situation as the extra messages ultimately contribute to the underlying problem.  

The \textsf{ABcast} protocol is even more susceptible to network saturation and the 'snowball' effect, as it utilises PSM to calculate probabilistic guarantees for each broadcast. This is a consequence of \textsf{ABcast}'s reliance on assumption \textbf{A4}.  If the underlying network becomes unstable as its load continues to increase, then the reliability of the DMC's calculations will deteriorate due to the network's latencies becoming increasingly unpredictable.  The unpredictability of the network in such circumstances, could cause the DMC to underestimate the latency that will be encountered by future transmissions; hence assumption \textbf{A4} is compromised.  Such an underestimation, could result in more messages being delivered by \textsf{Aramis}, which in turn increases the probability that a message's \emph{abcast} guarantees will be compromised due it being \emph{rejected} by \textsf{ABcast}.  

Ultimately, a flow-control mechanism is required by \textsf{ABcast} nodes to ensure that the number of requests issued by a node, per second, does not adversely effect the performance of the underlying communication network.  The P2P \emph{abcast} protocol currently used by Infinispan, TOA, utilises a flow control scheme provided by JGroups, called \textsf{UFC} \citep{JGroupsUFC}, to control each node's broadcast rate.  \textsf{UFC} is based upon the \emph{sliding-window} approach to flow control \citep{bertsekas1992DataNetworksFC}, however in the JGroups literature they describe the sliding window concept in terms of a finite number of \emph{credits} that are maintained by each node; for completeness we utilise the same terminology when referring to \textsf{UFC}.  The \textsf{UFC} protocol is based on the premise that a node's credits are expended when a new broadcast is sent and reimbursed when a destination node confirms receipt of the original broadcast.  

%The current implementation of \textsf{UFC} is static in its nature and does not allow for the dynamic adjustment of its window size (i.e. increasing the maximum number of credits) based upon the current network conditions.  

Informally, the \textsf{UFC} protocol works as follows: A receiving node, $N_j$ reimburses the sending node $N_i$'s credits, by sending a response message to $N_i$ with $x$ amount of credits; where $x$ is equal to the number of bytes received by $N_j$.  If a sending node  attempts to broadcast a message equal to $y$ bytes, but its remaining credits $rc < y$ then the sending of a message $m$ becomes blocked until a a receiving node reimburses the sender for its earlier broadcasts or a configurable timeout period expires.  

The \textsf{UFC} approach works well for deterministic \emph{abcast} protocols such as TOA, however it is not well suited for use with \textsf{ABcast}, due \textsf{Aramis}'s reliance on the DMC's calculations for generating its probabilistic guarantees.  As previously stated, the \textsf{Aramis} protocol is heavily reliant on assumption \textbf{A4}, consequently, it is necessary to ensure that the DMC's observed latencies do not fluctuate unpredictably in a manner that would undermine \textbf{A4}.  The \textsf{UFC}'s independence from the DMC, ultimately means that it cannot determine whether the current load on the network is having an adverse effect on the latencies been measured by the DMC, and hence, \textsf{UFC} cannot take action in the event of the DMC's measurements deteriorating.  Similarly, the \textsf{UFC} approach can become overly-restrictive when utilised by \textsf{ABcast}, as it is possible for a node to block the broadcasting of a message due to insufficient credits even if the DMC is operating as expected and no large increases in network latencies have been observed.  Finally, the \textsf{UFC} protocol requires additional messages to reimburse each node's credit when a broadcast has been received by a destination.  This additional bandwidth requirement could alternatively be utilised by the underlying \emph{abast} protocol to increase its throughput if it were not required by \textsf{UFC}.  

Ultimately, a bespoke flow-control scheme is required by \textsf{ABcast} to ensure that optimal level of throughput are maintained under heavy loads.  Such a protocol should utilise the DMC's measurements to control the send rate of \emph{abcast}s and to preserve the validity of assumption A4 where possible.  The remainder of this chapter details the design and implementation of such a protocol.  

\section{Protocol Design}
    In contrast to the \textsf{UFC} approach to flow control, our approach does not require additional messages, or the concept of a finite number of credits to restrict a node's transmission rate.  Instead, our solution depends entirely on the latencies measured by the DMC and its associated calculations.  Consequently, our flow control protocol is tightly coupled with the \textsf{ABcast} protocol and is not applicable for more traditional based \emph{abcast} protocol such as TOA.  The remainder of this section describes the rational behind our approach.  
    
    Assumption \textbf{A4} states:
    
    \begin{quotation}
            The maximum delay, $x_{mx}$, estimated by observing $NT_P$ number of transmissions from the recent past, will \emph{not} be exceeded during $NT_F$ number of future transmissions that unfold next, where $NT_F \leq NT_P$ with a high probability ($1 - q$).
        \end{quotation}
    
    As previously stated, when \textsf{ABcast} nodes start to become overwhelmed by broadcasts it is possible for \textbf{A4} to be undermined, resulting in future transmissions exceeding $x_{mx}$.  Therefore, we propose a new flow control protocol that utilises a \emph{rate-based} scheme \citep{bertsekas1992DataNetworksFC} that reduces a node's current broadcast rate if it starts to observe latencies greater than the last $x_{mx}$ value calculated.  We call this protocol ABcast Flow Control (AFC), and the basic design concept is as follows: A node is sending/receiving broadcast messages to/from a group of nodes and as per the \textsf{ABcast} protocol, the latencies encountered by such messages are recorded by each node.  In the event that one or more of these latencies exceed the current $x_{mx}$ value, it is the responsibility of AFC protocol to ensure that the local node adopts a lower broadcasting rate until a new $x_{mx}$ value is calculated.  At which time, the newly calculated $x_{mx}$ takes into account the large latencies observed in the recent past, reducing the probability of assumption \textbf{A4} being compromised by the latencies encountered in the near future.  if latencies continue to exceed $x_{mx}$ then the node's broadcast rate will become increasingly restricted, whereas if no violations of $x_{mx}$ occur, then no restrictions are placed on the node's broadcast rate. Thus a node's broadcast rate is only restricted when violations of assumption \textbf{A4} have occurred recently.  

    Unlike \textsf{UFC}, our approach restricts the sending rate of a node, $N_i$, based upon the messages it receives, not the rate at which $N_i$'s messages are received at other nodes in the network.  This may seem counter-intuitive, however the \textsf{ABcast} protocol is based upon the assumption that the latencies observed by a given node, $N_i$, are representative of the latencies that will be encountered by $N_i$'s broadcasts sent to other nodes.  Therefore the AFC protocol simply utilises this assumption and applies it to flow control.  The AFC protocol is designed upon the assumption that if $N_i$'s observed latencies repeatedly exceed $N_i.x_{mx}$, then it is highly probable that $N_i$'s message buffer or the underlying network is approaching saturation.  In which case It is very likely that another node, $N_j$, will also be observing increased latencies due to similar circumstances, therefore it is necessary for $N_i$'s broadcast rate to be lowered in order to reduce the load on $N_j$.  This assumption is especially apt in the \emph{AmaaS} model when the \textsf{SCast} protocol is used, as each $c$-node randomly selects an $s$-node when sending a multicast request, resulting in client requests being evenly distributed between $s$-nodes.  Therefore each $s$-node is likely to issue approximately the same number of \emph{abcast}s, and thus, each node receives approximately the same number of messages.          
    
    Figure \ref{fig:abcast_components_afc} shows the new \textsf{ABcast} components diagram with the AFC protocol included.  Note that the AFC protocol is the highest in the stack, i.e. closest to the application, as all \emph{abcast} messages must be sent via the AFC protocol.  
        
    \begin{figure}[!h] 
        \centering    
         \includegraphics[width=0.8\textwidth]{components_with_fcc}
         \caption[\textsf{ABcast} Protocol Components with AFC]{\textsf{ABcast} Protocol Components with AFC}
         \label{fig:abcast_components_afc}
    \end{figure}        
    
    The remainder of this section focuses on the calculations used by a node to regulate its broadcast rate.  
    
    \subsection*{AFC Calculations}     
     This section introduces the variables utilised by AFC, explaining their significance and why they are required, before detailing the calculations that utilise these variables to alter a node's broadcast rate.  The calculations presented in this section assume that a single message $m$ is being broadcast.  The steps required by AFC to initiate a broadcast of $m$ are as follows: An application sends $m$ down the protocol stack, and upon receipt of $m$, the AFC protocol performs all of the calculations presented in this section in order to calculate a flow control delay.  This delay must be observed between the time of a node's previous broadcast and the broadcasting of $m$.  Upon expiration of this delay, $m$ can be sent to \textsf{ABcast} for broadcasting.  
   
   \subsubsection*{Protocol Parameters}  
   When the AFC protocol receives $m$ from the application, it polls the DMC to determine the number of latencies that have exceeded $x_{mx}$ at the present time.  These latencies are used to calculate the flow control delay for $m$ that ensures it, and subsequent messages, are broadcast at the new rate.  Recall that the DMC utilises a $NT_F = 10\%$ of $NT_P$, where $NT_P = 1000$, and a new $x_{mx}$ value is calculated after every $NT_F$ latency has been recorded; in this case a new $x_{mx}$ value is calculated after every $100$ latencies observed.    We consider a latency $x$ to have exceeded $x_{mx}$ if $x > x_{mx}$ and $x$ is recorded between subsequent calculations of $x_{mx}$; e.g. $\#x \mod NT_F \neq 0$, where $\#x$ is the index at which $x$ is recorded in relation to all other latencies.  Once the $100$ latencies required to recalculate $x_{mx}$ have been received, then $x_{mx}$ is recalculated, and the values that previously exceeded $x_{mx}$ are now used to define the new $x_{mx}'$. 
    
    When a latency $x$ exceeds $x_{mx}$, we refer to this as a Marginal Peak $Mp$, as $x_{mx}$ is the boundary (margin) value and $Mp$ a latency that has peaked beyond the margin; we record $Mp$ as the difference between $x$ and $x_{mx}$, thus $Mp = x - x_{mx}$.  It is possible that multiple $x$ values will exceed $x_{mx}$, in which case we record all $Mp$ values and refer to the total number of $Mp$ values as $\#Mp$.   Once all $Mp$ values have been recorded, we calculate the variable $\mu$; which along with the current $x_{mx}$ value determines the amount that a node's broadcast rate should be restricted.  We calculate $\mu$ as:
    
    \begin{equation*}
		     \begin{aligned}
		         \mu = \frac{Mp + Mp'+,\ldots,+ Mp''}{Max(\#Mp, Ss)}
		     \end{aligned}
    \end{equation*} 
    
    Where $Ss$, which stands for sample size, is an integer constant defined before runtime that is used as a divisor when $\#Mp < Ss$.  The purpose of $Ss$ when calculating $\mu$, is to reduce the effects of a small number ($\#Mp < Ss$) of $Mp$ values from severely restricting a node's broadcast rate.  For example, consider  $x_{mx} = 2ms$ and a single $Mp$ occurs where latency $x$, $x = 4ms$, this would result in $Mp = 2ms$.  If the calculation of $\mu$ did not utilise $Ss$, then $\mu = 2$, which would produce a large flow control delay, which in turn would result in the broadcast rate being reduced significantly and the flow control becoming overly restrictive.  However, when we utilise the $Ss$ constant, the influence of a small number of latencies on the calculated $\mu$ value can be reduced; a large $Ss$ value marginalises their impact, whilst a small $Ss$ value promotes it.  The level of flow control required by a system is determined by its use case, therefore we defined $Ss$ as a constant specified before runtime in order to allow system administrators greater control of the protocols behaviour.  
    
    The $\mu$ variable is used alongside $x_{mx}$ to calculate $\gamma$, where $\gamma$ is calculated as:
    
    \begin{equation*}
		     \begin{aligned}
		         \gamma = \frac{x_{mx} + \mu}{x_{mx}}
		     \end{aligned}
    \end{equation*} 
    
    \subsubsection*{Calculating a New Broadcast Rate}
    Let $\lambda_1$ represent the current broadcast rate of a given node, which we calculate as follows:

    \begin{equation*}
		     \begin{aligned}
		         \lambda_1 = \frac{1}{m'.Fc_{ts} - m''.Fc_{ts}}
		     \end{aligned}
    \end{equation*} 
    
    Where $m.FC_{ts}$ is a timestamp recorded for each message as it is sent to \textsf{ABcast} for broadcasting and $m''$ and $m'$ are the two messages that were handled by AFC prior to $m$, where $m''.Fc_{ts} < m'.Fc_{ts}$.  If $m''$ and $m'$ do not exist it is not possible to calculate the current broadcast rate, as no two messages have previously been sent, instead a default delay is utilised between broadcasts.  
    
    The timestamps $m''.Fc_{ts}$ and $m'.Fc_{ts}$ are recorded by a node's local clock, however it is irrelevant whether a node's local clock or the  synchronised clock provided by \textsf{ABcast} is utilised for recording the timestamp as calculating the broadcast rate is a local operation.  Therefore, as long as the same clock is used for recording all AFC timestamps the difference between message broadcast times will be adequate for calculating the current broadcast rate.   
    
    Once the current broadcast rate has been ascertained, it can be used alongside the $\gamma$ variable to calculate a new broadcast rate for this node and the duration of the delay that needs to be observed by $m$ to implement it.  A new delay duration for $m$ is not required if $\gamma = 0$ as this means that no $Mp$ values have been observed and $\lambda_1$ does not need to be reduced, therefore the delay value utilised by the previous message is still valid.  Conversely if $\gamma > 1$, we know that at least one $Mp$ has occurred and it is necessary for $\lambda_1$ to be reduced to a smaller rate.  We represent this new smaller rate as $\lambda_2$, where $\lambda_2 < \lambda_1$ and $\lambda_2$ is calculated as follows:
    
      \begin{equation*}
		     \begin{aligned}
		         \lambda_2 = \lambda_1 \times e ^{ ({\frac{1-\gamma}{C}})}
		     \end{aligned}
    \end{equation*} 
    
    To reduce a node's broadcast rate it is necessary to increase the delay observed between each subsequent message broadcast.  This is achieved by increasing $\lambda_1$ by $\delta$ such that:
    
    \begin{equation} \label{eq:rate_plus_delta}
		     \begin{aligned}
		         \frac{1}{\lambda_1} + \delta = \frac{1}{\lambda_2}
		     \end{aligned}
    \end{equation} 
    
    \begin{equation}
		     \begin{aligned}
		         \delta = (\frac{1}{\lambda_2} - \frac{1}{\lambda_1}) = \frac{\lambda_1 - \lambda_2}{\lambda_1 \times \lambda_2}
		     \end{aligned}
    \end{equation} 
        
    \begin{equation}
		     \begin{aligned}
		         \lambda_1 - \lambda_2 = \lambda_1 \times [1 - e ^{ ({\frac{1-\gamma}{C}})}]
		     \end{aligned}
    \end{equation} 
    
    Where $C$ is an integer constant defined before runtime, with a higher value corresponding to a more restrictive flow control protocol.  
    Therefore:
    
    \begin{equation}
		     \begin{aligned}
		         \delta = \frac{1 - e ^{ ({\frac{1-\gamma}{C}})}}{\lambda_2} = \frac{1}{\lambda_2} \times [1 - e ^{ ({\frac{1-\gamma}{C}})}]
		     \end{aligned}
    \end{equation}
    
    \begin{equation}
		     \begin{aligned}
		         \lambda_2 = \lambda_1 \times e ^{ ({\frac{1-\gamma}{C}})}
		     \end{aligned}
    \end{equation}     
    
    \begin{equation}
		     \begin{aligned}
		         \frac{1}{\lambda_2} = \frac{1}{\lambda_1} \times \frac{1}{e ^{ ({\frac{1-\gamma}{C}})}}
		     \end{aligned}
    \end{equation}
    
    Thus:
    
     \begin{equation}
		     \begin{aligned}
		         \delta = [1 - e ^{ ({\frac{1-\gamma}{C}})}] \times \frac{1}{\lambda_1} \times \frac{1}{e ^{ ({\frac{1-\gamma}{C}})}}
		     \end{aligned}
    \end{equation}
    
    Hence:
    
        \begin{equation}
		     \begin{aligned}
		         \delta = \frac{1}{\lambda_1}  \times   \frac{1 - e ^{ ({\frac{1-\gamma}{C}})}}{e ^{ ({\frac{1-\gamma}{C}})}}
		     \end{aligned}
    \end{equation}
    
    We then calculate the delay for $m$ simply as $m.Fc_ts = m'.Fc_ts + \delta$, based upon equation \ref{eq:rate_plus_delta} and assuming that the timestamp and delta are calculated using the same unit of time.  If the previous message, $m'$ was sent longer than $\delta$ time in the past, then $m$ can be broadcast instantly as the rate between broadcasts is still maintained.  
    
    The calculation of $\delta$ presented above is a very dynamic solution that can result in a node's $\delta$ value fluctuating dramatically over a period of time.  Although a dynamic flow control solution that reacts to the changing conditions of the network is desirable, early experiments showed that calculating $\delta$ as above resulted in very small delays being calculated for the majority of broadcasts.  This resulted in the broadcast rate of sending nodes not being restricted sufficiently over an extended period of time, causing larger numbers of $Mp$ values to suddenly appear. This sudden emergence of $Mp$ values results in the calculated $\delta$ value being extraordinarily high and the system's throughput becoming excessively restricted.  Over time, as new $x_{mx}$ values are calculated and $Mp$ values stop appearing, the flow control begins to increase the broadcast rate, however we found that $\delta$ would eventually become too small again, causing a cycle to occur that consistently repeated itself.  Ultimately, solely utilising $\delta$ causes AFC to retrospectively react to a saturated network, when the purpose of AFC is to be proactive and stop saturation from occurring.  
    
    Our solution, was to propose two new constants $\delta_{min}$ and $\delta_{max}$, which set a lower and upper bound on the calculated $\delta$ value, respectively.  Therefore the new $\delta$ must satisfy $\delta_{min} \leq \delta \leq \delta_{max}$.  The lower bound $\delta_{min}$ is necessary to ensure that all broadcasts are sent at a constant, predictable rate, which helps to stabilise $x_{mx}$ and reduces the chances of a single node receiving a large number of requests within a very short period of time.  Conversely, the upper bound $\delta_{max}$ ensures that if a large number of $Mp$s occur between $x_{mx}$ calculations, the calculated $\delta$ value will not be excessively large and thus wont overly restrict a node's broadcast rate.  Like the other constants used by AFC, $\delta_{min}$ and $\delta_{max}$ are determined before runtime, and therefore appropriate values can be set for each depending on the network environment and intended workflow. 
    
    \subsubsection*{Message Buckets}
    So far we have only considered a solution that manages application requests on a per message basis.  
    %So far our solution has only considered the sending of a single message $m$ at a given time
    % Need to utilise a Delta_min and Delta_max value in order to ensure that the broadcasting of messages occurs at a constant rate and that delta is not excessively high
    % Introduce the concept of buckets
\section{Limitations}

\section{Summary}
This chapter presented AFC a new flow control protocol that is bespoke to \textsf{ABcast}.  AFC utilises the probabilistic calculations of \textsf{ABcast}'s DMC to asses the networks current state.  The values observed by the DMC are then used to determine whether a node's current broadcast rate needs to be increased or decreased in order to maintain optimal levels of throughput across the network.  With the ultimate aim of trying to reduce the probability of an \textsf{Abcast} message being rejected due to assumption \textbf{A4} being violated.    