\chapter{Performance Evaluation}

    \graphicspath{{Chapter6-PerformanceEvaluation/Figs/Vector/}{Chapter6-PerformanceEvaluation/Figs/}}
    

% Probing validation experiments    
\section{DMC Validation}

% Emulated Transactions
\section{AmaaS}
	To test our hypothesis that the \textsf{AmaaS} model can improve the scalability of Infinispan's distributed transactions, we developed an experiment that emulates the workflow of these transactions by replicating the \emph{amcast} messages sent by Infinispan when executing total order transactions ($\S$ \ref{sec:to_commit}).  This experiment does not utilise Infinispan, or implement a basic transaction manager, rather it focuses purely on replicating the underlying communication stages required by Infinispan transactions.  Existing research \citep{Ruivo:2011:ETO:2120967.2121604} has already shown the benefits of utilising a total order protocol instead of 2PC, therefore our experiments concentrate on the performance of the underlying \emph{amcast} protocol used to coordinate these transactions.  In our experiments, if a new \emph{amcast} protocol can demonstrably increase the throughput and reduce the latency of \emph{amcast} messages, as the number of destinations increase,  then we can infer that the scalability of the Infinispan system will be improved by adopting this protocol.  Therefore, if our experiments show that the \textsf{AmaaS} model consistently outperforms P2P, then we assume our hypothesis to be true. Furthermore, if the \textsf{AmaaS} service utilises the \textsf{ABcast} protocol and performs similarly to when a deterministic protocol is used, we can conclude that the \textsf{ABcast} protocol provides low-latency message delivery in the absence of node failures and within the context of \textsf{AmaaS}.  
	
   In order to compare and contrast the performance of the \textsf{AmaaS} and P2P approach, it was necessary for two experiments to be created.  The first experiment was designed to evaluate the latency and throughput of the \textsf{AmaaS} model when utilised with different \emph{abcast} protocols.  This experiment not only allows the performance of the \textsf{AmaaS} model to be evaluated, but also for the the performance of the underlying \emph{abcast} protocol utilised between service nodes to be compared using workloads typical of \textsf{AmaaS}.  These experiments utilised a simplified version of the \textsf{SCast} Protocol ($\S$ \ref{sec:scast_protocol}) to coordinate interactions between client nodes, $c$-nodes, and the service's nodes, $s$-nodes; with $c$-nodes representing Infinispan nodes executing transactions.  
   
   The second experiment was designed to measure the performance of \emph{amcast} requests when utilising the P2P approach.  This experiment utilises the same workloads and parameters as the first experiment, however, as per the P2P model, no $s$-nodes are present and consequently there is no need for the \textsf{SCast} protocol.  Instead, the TOA protocol is executed directly between $c$-nodes when emulating transactions.  Utilising the same experiment structure and workload between both experiments enables us to compare the performance of the two system models across a consistent environment.  Thus it is possible for the performance of the TOA protocol, when utilised in both the P2P and \textsf{AmaaS} models, to be contrasted with \textsf{ABcast}s performance in \textsf{AmaaS}.  
	
	The rest of this section is structured as follows.  First we detail the \textsf{AmaaS} experiment and the environment in which it was executed.  We then describe the modifications required by this experiment in order to measure the performance of the P2P model, before presenting the results both sets of experiments.  Finally, we analyse the results of these experiments and present an evaluation of our findings.  
		
	\subsection{Experimentation}\label{ssec:emulated_transaction_experiments}
	\subsubsection*{AmaaS}
	We implemented an \textsf{AmaaS} service using the JGroups\citep{JGroups} framework with $n=2$ and $3$ $s$-nodes.  All nodes in the experiment utilised commodity PCs of \emph{3.4GHz Intel Core i7-3770} CPU and 8GB of RAM, running \emph{Fedora 20} and communicating over Gigabit Ethernet. The $s$-nodes and $c$-nodes utilised in our experiments are a part of a large university cluster, hence communication delays between them can be quite volatile as they are influenced by other network traffic and by jobs launched by other users.
	
	Our experiments are based upon a heavily modified version of an existing performance test available in the JGroups\citep{JGroups} framework, which mimics the partial replication of key/values in Infinispan\citep{Infinispan}.  In these experiments we utilise ten $c$-nodes in the same cluster, each of which emulates a transaction system that is reliant on the \textsf{AmaaS} service.  Each $c$-node operates 25 concurrent threads to initiate and coordinate transactions, and a transaction $Tx$ involves a set $|Tx.dst| = 3,4,\ldots,10$ $c$-nodes; where $|Tx.dst|$ includes $Tx.c$. A thread coordinating a transaction starts its next transaction, $Tx'$, as soon as it executes a commit/abort decision for the currently active $Tx$. Thus, at any moment, $250$ transactions are in different stages of execution.  All emulated transactions consist purely of key/value write operations and hence require \emph{amcast} messages for coordination.  Infinispan's read requests ($get(k)$) are not emulated, as the retrieval of key/values occurs before $Tx.c$ \emph{amcast}s its $prepare(k)$ message, and therefore has no baring on \emph{amcast} performance.  
	
	A modified version of the \textsf{SCast} protocol is utilised by the $s$-nodes and $c$-nodes to provide \emph{amcast}s for the emulated transactions.  The workflow of a transaction in this system is as follows: A coordinator thread submits its \emph{amcast} request for $Tx$, denoted as $req(Tx)$, with some $s$-node; the latter stores such requests in the ARP in the arrival order. The \emph{Send} thread bundles some or all of these requests in the ARP in their arrival order into a message bundle $mb$, which can have a maximum payload of $1kB$ \footnote{In the experiments that utilise the \textsf{ABcast} protocol, we \emph{pad} the contents of the message bundle to ensure that it is always equal to $1kB$.  This ensures that all messages \emph{abcast} by the protocol are approximately the same size, which increases the accuracy of the DMC's predictions at the expense of redundant bandwidth.}, then \emph{abcast}s $mb$ to all other $s$-nodes.  The \emph{Send} thread waits if the ARP is empty and resumes bundling once ARP becomes non-empty\footnote{Hence, the number of requests bundled in any $mb$ varies depending on the request arrival rate.}. Once $req(Tx)$ has been \emph{abcast} to all $s$-nodes, a response message, $Rsp(Tx)$ it is sent to $Tx.c$ who then disseminates this message to $Tx.dst$ as $mcast(Tx)$.  When all $d \in Tx.dst$ have received and delivered $mcast(Tx)$ as per the delivery conditions of the \textsf{SCast} protocol, the transaction is considered complete and the coordinator thread can start executing $Tx'$.  
	
    Stage 1 of the \textsf{SCast} protocol has been omitted from this implementation, this is because we only compare the performance of the two approaches in a crash-free scenario.  It is possible for fault-tolerance to be provided to an \textsf{AmaaS} \emph{amcast} protocol in multiple ways, therefore we do not include this additional step as our focus is on performance in normal conditions.  However, it is worth noting, that as stage 1 only consists of a single round trip delay, its inclusion in the implementation would only have a marginal effect on the results of our experiments.  	
	
    In our experiments that utilise \textsf{ABcast}, an additional phase is required before the experiment detailed above can begin.  Prior to accepting requests from $c$ nodes, $s$-nodes go through a \textquoteleft{}warm-up' phase lasting approximately 1-2 seconds.  During this phase, the clocks of the $s$-nodes are synchronized and each $s$-node broadcasts $10^3$ \emph{probe} messages, with a payload of $1kB$, to all other $s$-nodes.  The purpose of these probe messages is to record the $NT_P$ latencies required by the \textsf{ABcast}'s DMC to calculate the initial estimates of $x_{mx}$, $q$, $\rho$, $\eta$ and $\omega$ .  Without this period, $s$-nodes would be unable to send \emph{abcast}s as the DMC would not have calculated the variables required by the Aramis protocol.  
    
    In addition to this \textquoteleft{}warm-up' phase, another important aspect of our \textsf{ABcast} implementation is the broadcasting of redundant message copies.  In $\S$ \ref{ssec:rbcast} we  state that at least $\rho$ copies of a message $m$ are broadcast, such that $m.copy = 0,\ldots,\rho$, however in our implementation we only send $m.copy = 0,1$.  We have \textquoteleft{}capped' $m.copy$ to reduce the overall bandwidth required by the protocol, whilst maintaining a sufficient number of redundant broadcasts to ensure that fault-tolerance is preserved.  Note, that although $m.copy$ no longer respects $\rho$, $\rho$ is still utilised for calculating the DMC's variables and $\Delta_m$.  
    
    Finally, all of our experiments with \textsf{ABcast} utilise the the following constant values.  The DMC utilises $R=0.9999$ for calculating various parameters ($\S$ \ref{ssec:dmc}), and AFC utilises $\delta_{min}$ and $\delta_{max}$ values equal to $1ms$ and $10ms$ respectively, with $Ss = 10$ ($\S$ \ref{sec:afc_protocol}).  
	
	\subsubsection*{P2P}
	In order to test the performance of P2P total order transactions we repeated the experiments detailed above, however, as per the P2P model, all $c$-nodes coordinate transactions between themselves without utilising any $s$-nodes.  In these experiments, a transaction is considered complete when it has been successfully \emph{amcast} to all $d \in Tx.dst$ by the P2P protocol; where success is defined as all correct destinations delivering the \emph{amcast} message. 
	
	The same cluster of machines were used for both the P2P and \textsf{AmaaS} experiments to ensure a fair comparison between protocols.   
	
	\subsection{Results}\label{sec:AmaaS_results}
	Our performance evaluation focuses on the comparison of the TOA protocol, being utilised in a traditional P2P scenario (\emph{TOA-P2P}), with an \textsf{AmaaS} service that implements the \textsf{SCast} protocol.  The \textsf{AmaaS} experiments were conducted with two different \emph{abcast} protocols used between $s$-nodes.  First, we utilised the TOA protocol (\emph{TOA-Service}) in order to provide a like-for-like comparison between the P2P approach and the \textsf{AmaaS} model. We then repeated our experiments with the \textsf{ABcast} protocol utilised between $s$-nodes (\emph{ABService}), in order to compare the performance of \textsf{ABcast} with both TOA and TOA-Service.  
    
    The performance of all three approaches is measured based upon the average transaction latency and throughput rate. In both TOA-Service and ABService, latency is measured as the time elapsed between a $c$-node's initial transmission of $req(Tx)$ to some $s$-node, and all members of $Tx.dst$ delivering $mcast(Tx)$ to the experiment application. In TOA-P2P, latency is measured as the time taken for all $Tx.dst$ to deliver $Tx$ to the experiment application. For both approaches, throughput is measured as the average number of \emph{abcast}s delivered by the experiment application per second at each $c$-node.
	
	All of our experiments were conducted in isolation in order to prevent any side effects caused by simultaneous execution across the cluster, however we conducted all experiments over approximately the same time period to ensure that the network was under similar loads for all of our experiments. 
	
	Figures \ref{fig:LatencyGraph} and \ref{fig:ThroughputGraph} show the latency and throughput results for our experiments, with $2N$ and $3N$ representing an \textsf{AmaaS} service that consists of two and three, $s$-nodes respectively.  Each plot on the graph is an average of three \emph{crash-free} trials; where a trial consists of each $c$ node completing $10^4$ transactions for a specific value of $|Tx.dst|$. Thus, in all three trials the TOA-Service and ABService each receive a total of $10^5$ \emph{amcast} requests. In TOA-P2P, each $c$ node initiates $10^4$ TOA executions between its peers.  
	
	Concerning \textsf{AmaaS} performance, Table \ref{table:emulated_transaction_averages} shows the average number of client requests received, the average number of \emph{abcast} messages sent and the average number of requests bundled into each \emph{abcast}, for all of our experiments that utilised a \textsf{AmaaS} service.  All of the average values are calculated based upon the statistics recorded by each $s$-node during our experiments. 
	
    \begin{table}[h]
  \begin{center}
    \begin{tabular}{|l|c|c|c|}
    \hline
    Experiment & $\#$ Client Requests  & $\#$ \emph{abcast}s & Bundle Rate \\ \hline \hline
    ABService-2N     & 50000    &    12763.4    &    4 \\ \hline
    TOA-Service-2N  & 50000    &   16632    &   3 \\ \hline
    ABService-3N     & 33333.3 &    13416.4  &   2.5 \\ \hline
    TOA-Service-3N  & 33333.3 &    13507.8 & 2.5 \\ \hline
    \end{tabular}
    \caption{Average Node Statistics for Emulated Transaction Experiments}
    \label{table:emulated_transaction_averages}
  \end{center}
\end{table}	
	
    Table \ref{table:emulated_transcation_aramis_deliveries} shows the performance of the \textsf{ABcast} protocol in both the ABService-2N and 3N experiments.  It shows the average number of \emph{abcast}s sent per node and the average number of these messages that were delivered by the Aramis protocol, as well as providing the total percentage of \emph{abcast}s that were delivered via Aramis.  Furthermore, this table also details the ratio of $s$-nodes that delivered an \emph{abcast} via Aramis compared to the total number of $s$-nodes utilised.  For example, in the case of ABService-2N we performed $24$ experiments, therefore we have statistics for $48$ $s$-nodes and our records show that only $3$ of these $48$ nodes utilised Aramis to deliver one or more \emph{abcast} message.  
	
	\begin{table}[h]
	  \begin{center}
	    \begin{tabular}{|l|c|c|c|c|}
	    \hline
	    Experiment  & $\#$ \emph{abcast}s & Nodes Effected &  Avg Aramis Deliveries & $\%$ Aramis Deliveries \\ \hline \hline
	    ABService-2N & 12763.4 & 3:48   & 10.8  & $0.085\%$  \\ \hline
	    ABService-3N & 13416.4 & 30:72 & 15.3  & $0.114\%$ \\ \hline
	    \end{tabular}
	    \caption{Average ABcast Statistics per Node}
	    \label{table:emulated_transcation_aramis_deliveries}
	  \end{center}
	\end{table}	
	
	\begin{figure}[tp]
	 % \centering
	 \includegraphics[width=\textwidth,height=\textheight,keepaspectratio, clip, trim={2cm 3.5cm 2cm 3cm}]{Latency2}
	 \caption{AmaaS Latency Comparison}
	 \label{fig:LatencyGraph}
	\end{figure}
	
	\begin{figure}[bp]
	% \centering
	 \includegraphics[width=\textwidth,height=\textheight,keepaspectratio, clip, trim={2cm 3.5cm 2cm 3cm}]{Throughput2}
	 \caption{AmaaS Throughput Comparison}
	 \label{fig:ThroughputGraph}
	\end{figure}	
	
	\clearpage
    \subsection{Evaluation}
    This section is split into three distinct subsections.  First we directly compare the performance of the \textsf{AmaaS} service and the P2P approach with both experiments utilising the same TOA protocol.  We then evaluate the performance of the ABService in contrast with the previous two approaches, focusing on the differences between the performance of the ABcast and TOA based service.  Finally, we evaluate the performance of \textsf{ABcast}, focusing on how often the \textsf{Aramis} protocol was utilised to deliver messages and its ability to maintain \emph{abcast} guarantees G1-G4.  
    
    \subsubsection*{AmaaS vs P2P}
	In Figure \ref{fig:LatencyGraph} we can see that when $|Tx.dst| \geq 4$, TOA-P2P's \emph{abcast} latencies increase considerably when compared to the two TOA-Service experiments.  With TOA-P2P experiencing approximately a $25\%$ and $50\%$ increase in average latency when compared to TOA-Service-3N and TOA-Service-2N respectively.  Thus, indicating that \emph{amcast}ing is best provided as a service as the number of clients involved in a transaction increases. Comparing throughput in Figure \ref{fig:ThroughputGraph} leads to similar conclusions, with the steady throughput observed as $|Tx.dst| \rightarrow 10$ also suggesting an absence of node saturation.  
	
	TOA-P2P's superior performance when $Tx.dst < 4$ can be attributed to the additional stages involved when utilising the \emph{AmaaS} model.  For example when TOA-Service utilises two $s$-nodes ($2N$) the following stages are required: $Tx.c$ sends a request, the \emph{multicast service} \emph{abcast}s it with $|m.dst| = 2$ to all $s$-nodes and returns it to $Tx.c$, who must then multicast $mcast(Tx)$ to $Tx.dst$.  Ignoring the individual message cost of each stage the total number of stages is four, whereas in TOA-P2P the only step required is the \emph{amcast}ing of $Tx$.  So although $|m.dst|$ for each \emph{amcast} is less in TOA-Service ($|m.dst| = 2$) than TOA-P2P ($|m.dst| = 3$), the overhead of sending a request to the \emph{multicast service} and back is much greater than the savings offered by reducing $|m.dst|$ by one node.  However, as $|Tx.dst|$ increases, the overhead of TOA-P2P's increased $|m.dst|$ becomes significant, to the point where TOA-Service's additional communication stages becomes less of an overhead than the cost of TOA-P2P \emph{amcat}ing to a large $m.dst$.  
	
    \subsubsection*{ABService vs TOA-Service}
    In Figure \ref{fig:LatencyGraph} we can see that the latencies encountered by the ABService-2N and TOA-Service-2N experiments are very similar regardless of the number of clients involved in a transaction, with the maximum difference between any two plots being no greater than $0.3$ milliseconds.  Interestingly, our experiments show that in the majority of experiments ($5/8$), the ABService does not just match the performance of the TOA-Service, but actually outperforms it.  This superior performance can be attributed to a combination of two factors: the number of requests that are bundled on average per \emph{abcast} and the overall message cost associated with the underlying \emph{abcast} protocol.  
    
    The average number of client requests bundled into a single \emph{abcast} can play a decisive role in the latency and throughput of a \textsf{AmaaS} service as the higher the average bundle rate, the lower the total number of \emph{abcast}s required.  As the \emph{abcast}ing of requests between $s$-nodes is the most expensive operation, in terms of bandwidth and latency in the \textsf{AmaaS} model, it is self-evident that reducing their frequency will reduce the average latency encountered by client requests, therefore reducing the total duration of a transaction.  
    
    Table \ref{table:emulated_transaction_averages} shows that the average bundle rate for the ABService-2N was $4$ messages, whilst it was only $3$ for TOA-Service-2N.  Therefore, on average a node in TOA-Service-2N sends $\approx 3869$ more \emph{abcast}s then its counterpart ABService-2N, which partially explains the difference in performance between the two approaches.  
    
    The difference in overall message cost between the two \emph{abcast} protocols is a consequence of the two different approaches to solving \emph{abcast} and the optimisations present in the \textsf{ABcast} protocol ($\S$  \ref{ssec:atomic_broadcast} $\&$ \ref{ssec:base_ack_piggyback}).  The \textsf{ABcast} protocol piggybacks any outstanding message acknowledgements on subsequent message broadcasts, enabling \emph{abcast}s to be executed in a single phase when all nodes are frequently sending \emph{abcast}s.  Whereas, the JGroups implementation of the TOA protocol does not implement any optimisations, and thus, each broadcast always consists of two phases, therefore increasing the average latency encountered by transaction requests.  
	
	Correspondingly, it is possible to observe that the average and maximum difference between the latencies encountered in the ABService-3N and TOA-Service-3N experiments is greater than that observed when $N = 2$.  This increase in the performance gap can be attributed to the \textsf{ABcast} optimisations becoming more effective when the number of $s$-nodes increases due to the total number of messages no longer required by \textsf{ABcast} also increasing.  For example, if $N = 3$ and \textsf{ABcast} sends a broadcast, the total message cost for that single \emph{abcast} is only $2$ unicasts, whereas with TOA the total cost is $6$ unicasts due to the two phases required by the protocol.  Clearly, the potential for such reductions in message cost will have a positive effect on the performance of the ABService implementation, especially when service requests are evenly distributed amongst $s$-nodes and are arriving frequently.  
	
    Interestingly, in Table \ref{table:emulated_transaction_averages} we can see that the average bundle rate of ABService-3N and TOA-Service-3N are almost the same, yet the difference between the observed latencies in the two approaches has increased.  	Thus in these experiments the average bundle rate has no significant impact on the performance of the two approaches.  
    
    The large difference between the average bundle rate observed in ABService-2N and 3N, is a direct consequence of the DMC's calculations and how AFC ($\S$ \ref{sec:afc_protocol}) manages broadcast rates.  Recall that the delay imposed by AFC, for an \emph{abcast} message, increases when latencies start to exceed the previously calculated $x_{mx}$ value, and decreases to $\delta_{min}$ when no such latencies are observed.  When $2 s$-nodes are utilised, the observed $x_{mx}$ is typically lower than $3 s$-nodes, as the number of unicasts sent between $s$-nodes is less; hence the probability of large delays being observed is reduced.  The smaller the average $x_{mx}$ value, the more susceptible the system is to delays periodically exceeding $x_{mx}$.  Therefore, when $2 s$-nodes are utilised the probability of the calculated AFC delay regularly exceeding $\delta_{min}$ increases, which in turn reduces the node's broadcast rate.  Consequently, the number of requests which can accumulate between \emph{abcast}s will increase, and hence the average bundle rate also increases.  When $3 s$-nodes are utilised, the DMC's observations are typically more stable, resulting in less \emph{outlier} latencies being recorded and the broadcast rate being more stable; hence an average bundle rate that is approximately the same as the TOA-Service-3N.  
	
	The throughput of the ABService and TOA-Service for both $2N$ and $3N$ follows a very similar pattern to that observed when analysing their latencies.  This is not surprising as the average transaction latency has a direct impact on the average rate of throughput.  Combining the results shown in Figures \ref{fig:LatencyGraph} and \ref{fig:ThroughputGraph}, it is clear to see that the ABService provides comparable performance to that of the TOA-Service and that both of these \textsf{AmaaS} solutions consistently outperform TOA-P2P when $Tx.dst > 3$.  
	
	\subsubsection*{ABcast}
	In Figure \ref{table:emulated_transcation_aramis_deliveries} we can see that only $3$ of the $48$ nodes utilised by ABService-2N, delivered an \emph{abcast} via the \textsf{Aramis} protocol, with the average number of messages being $\approx 11$, only $0.085\%$ of all messages.  Hence, the $\Delta_m$ value calculated by the DMC was sufficient for  $99.915\%$ of \emph{abcast}s.  The results of the ABService-3N experiments shows that as the number of $s$-nodes increased,  the total number of \textsf{Aramis} deliveries also increased.  Almost $50\%$ of nodes delivered at least one message via \textsf{Aramis}, with an overall average of $\approx 16$ messages per node.  Although this is a large increase in the number of nodes requiring \textsf{Aramis}, the protocol still only accounts for $0.114\%$ of all \emph{abcast}s sent.  
	
	The increase in \textsf{Aramis} deliveries as the number of $s$-nodes increase can be attributed to the DMC recording each latency anomalously (without regard for source of the message) and calculating $\Delta_m$ based upon these latencies.  In the experiments where $N=2$, we know that all of the latencies recorded by node $n_1$ will be from messages originating at $n_2$.  Therefore, when node $n_1$ broadcasts message $m$, it is guaranteed that the calculated $\Delta_m$ has been calculated utilising latencies representative of $n_2$'s past performance.  Whereas, when $n=3$, $n_1$ will have calculated $\Delta_m$ based upon latencies recorded from both $n_2$ and $n_3$, therefore it is possible that if $n_3$ is slower than $n_2$, the latencies calculated from $n_2$ will dilute the larger latencies recorded from messages originating at $n_3$.  Thus, the calculated $\Delta_m$ could be smaller than the value required by the slower node $n_3$.  
	
	None of the experiments that delivered a message via \textsf{Aramis} were required to reject an \emph{abcast}, therefore guarantees G1-G4 were maintained and the state of the \textsf{AmaaS} service remained consistent throughout all of our experiments.  Furthermore, we repeated our experiments with deliver condition $D1_B$ of the \textsf{Base} protocol disabled, meaning that messages can only be delivered via \textsf{Aramos}, in order to evaluate the accuracy of $\Delta_m$ in this environment.  We found that, for both $2N$ and $3N$, the calculated $\Delta_m$ was sufficient for all nodes to deliver messages without a single rejection occurring.  As expected, latencies were large, and they were so large that a single experiment (involving $10^5$ transactions) took several minutes to complete.  Since these latencies make the \textsf{AmaaS} approach redundant, these experiments are simply an illustration $\delta_m$'s ability to preserve \emph{abcast} guarantees G1-G4.  
		
	\subsection{Summary}
	When deploying a large-scale distributed transaction system that executes transactions that span several nodes ($n > 3$), higher throughout and lower-latency can be achieved by utilising the \textsf{AmaaS} model for \emph{amcast}ing.  Furthermore, such a service can provide non-blocking \emph{amcast}s when \textsf{ABcast} is utilised between service nodes, whilst maintaining comparable levels of performance to GM-based protocols.  

\section{ABcast - Infinite Clients}
    In the previous section, we tested the performance of the \textsf{AmaaS} approach whilst utilising the \textsf{ABcast} protocol.  Our results showed, that the \textsf{Aramis} protocol was rarely required to deliver messages, accounting for only $0.015\%$ and $0.114\%$ of messages, when the number of $s$-nodes was two and three respectively.  However, in these experiments the total number of \emph{abcast} messages was, on average, relatively low for each node; typically less than $2 \times 10^4$.  Furthermore, each $s$-node's rate of \emph{abcast}s would vary depending on the restrictions of the AFC protocol and the rate at which requests were being received by $c$-nodes.  Due to the number of client nodes being relatively small, only ten were utilised, it is probable that at times an $s$-node's ARP would be empty.  Therefore, in order to test the performance of \textsf{ABcast} under heavy loads, it was necessary for a new experiment to be developed.  The purpose of these experiments are two fold.  First, they allow us to measure how often \textsf{Aramis} is required to deliver messages and how often such deliveries cause messages to be rejected.  Secondly, they allow us to monitor the values calculated by the DMC during high levels of network load and determine their effect on the resulting $\Delta_m$.  
    
    One solution would be to simply increase the number of client nodes in our previous experiment, however this would require far more resources and would be more cumbersome to orchestrate.  Furthermore, such an approach does not guarantee that the ARP of a given $s$-node will always have a request to process.  Therefore, we propose a new experiment that we call an \emph{infinite client system} as it represents the performance of \textsf{AmaaS} ordering service if each $s$-node always had a full ARP.  This experiment does not utilise client nodes at all, instead, it simply consists of $n$ nodes executing $x$ \emph{abcast}s between themselves as fast as possible.  This is the same as the steps required by \textsf{SCast}, however we do not have the overhead of maintaining a state at each node, therefore the delay between subsequent \emph{abcast}s will be less in this experiment, hence \textsf{ABcast} will be under a heavier load then is possible when utilised with \textsf{SCast}. 
    
    The rest of this section is structured as follows.  First we detail the infinite client experiment and how it is implemented.  We then present the results of our experiments, before analysing them and providing an evaluation of our findings. 
    
    \subsection{Experimentation}
    The infinite client experiment was implemented using the JGroups framework and all of our experiments utilised the same implementation of \textsf{ABcast} as those discussed in $\S$ \ref{ssec:emulated_transaction_experiments}.  Furthermore, our experiments utilised the same specification of machine as our previous experiments from within the same university cluster.  
    
    An individual experiment consists of $n$ nodes sending $10^6$ \emph{abcast}s between each other; with each individual node sending $\frac{10^6}{n}$ messages.  Each node sends these broadcasts as fast as possible using a single thread, which represents the \emph{sender} thread utilised in \textsf{SCast} to process the ARP and broadcast requests between $s$-nodes.  As soon as a message has been sent, another broadcast via \textsf{ABcast} is initiated; where the sending of a message consists of it being sent down the JGroups stack, processed and delayed by AFC, before being unicast to all $n$ nodes.  An experiment is considered complete when each node has delivered $10^6$ messages, or if messages have been rejected by a node, then $10^6 - \#rejections$.  
    
    All of our experiments record the $m.id$ of each \textsf{ABcast} message, in the order that they were delivered to the application, across several text files.  This allows us to ensure that our implementation is correct and that violations to the \emph{abcast} total order only occur if one or more messages are rejected by \textsf{Aramis}.  For all of our experiments, this was true, therefore we know that the number of total order violations is always equal to the number of \textsf{Aramis} rejections.  Henceforth, all references to a message being rejected infers that a violation of the \emph{abcast} total order has occurred.  
    
    \subsection{Results}

%\begin{table}[h]
%  \begin{center}
%  \renewcommand{\arraystretch}{1.3}
%   \begin{tabular}{|l|c|c|c|}
%    \hline
%    Experiment & $N_1$   & $N_2$   & $N_3$      \\ \hline \hline
%    1          & 3003, (0)  & 3385, (0)  & 705, (0)    \\ \hline
%    2          & 303, (0)    & 353, (0)    & 279, (0)   \\ \hline
%    3          & 4192, (0)  & 3734, (0)  & 1727, (0)  \\ \hline
%    4          & 2324, (0)  & 2478, (0)  & 1094, (0)    \\ \hline
%    5          & 2640, (0)  & 2121, (0)  & 1804, (0)   \\ \hline
%    6          & 1040, (0)  & 1408, (0)  & 1365, (0)   \\ \hline
%    7          & 403, (0)    & 2108, (0)  & 2135, (0)   \\ \hline
%    8          & 1714, (0)  & 1303, (0)  & 1871, (0)   \\ \hline
%    9          & 2282, (0)  & 1163, (0)  & 3122, (0)   \\ \hline
%    10        & 143, (0)    & 391, (0)    &  361, (0)   \\ \hline
%    Total    &18044, (0) &18419, (0) &14463, (0) \\ \hline
%    \end{tabular}
%    \caption{Aramis deliveries (failures) for infinite clients - $\rho_{min}$ = 3}
%    \label{table:infinite_clients_aramis}
%  \end{center}
%\end{table}
%    
%\begin{table}[h]
%  \begin{center}
%  \renewcommand{\arraystretch}{1.3}
%   \begin{tabular}{|l|c|c|c|}
%    \hline
%    Node      & Avg Delivery Latency (ms) & Avg $\Delta_m$ (ms) \\ \hline \hline
%    $N_1$   & $19.98$                           & $869.34$                   \\ \hline
%    $N_2$   & $20.43$                           & $823.32$                   \\ \hline
%    $N_2$   & $20.96$                           & $824.65$                   \\ \hline \hline
%    Overall   & $20.46$                           & $8391.14$                 \\ \hline
%    \end{tabular}
%    \caption{Average \textsf{ABcast} Latencies and Calculated $\Delta_m$ - $\rho_{min}$ = 3}
%    \label{table:infinite_clients_aramis}
%  \end{center}
%\end{table}    
    
\begin{table}[h]
  \begin{center}
  \renewcommand{\arraystretch}{1.3}
   \begin{tabular}{|l|c|c|c|}
    \hline
    Experiment & $N_1$   & $N_2$   & $N_3$      \\ \hline \hline
    1          & 9220, (0)  & 7929, (0)  & 6434, (0)    \\ \hline
    2          & 3348, (0)  & 4555, (0)  & 5008, (0)   \\ \hline
    3          & 4496, (0)  & 4920, (0)  & 1952, (0)  \\ \hline
    4          & 5832, (0)  & 6439, (0)  & 4801, (0)    \\ \hline
    5          & 5320, (0)  & 5757, (0)  & 4066, (0)   \\ \hline
    6          & 4181, (0)  & 3286, (0)  & 4157, (0)   \\ \hline
    7          & 1743, (0)  & 2237, (0)  & 2235, (0)   \\ \hline
    8          & 4188, (0)  & 1846, (0)  & 5421, (0)   \\ \hline
    9          & 5621, (0)  & 4242, (0)  & 5291, (0)   \\ \hline
    10        & 2953, (0)  & 5014, (0)  & 3192 , (0)   \\ \hline
    Total    &46902, (0) &46225, (0) &42557, (0) \\ \hline
    \end{tabular}
    \caption{Aramis deliveries (rejections) for infinite clients - $\rho_{min}$ = 1}
    \label{table:infinite_clients_aramis}
  \end{center}
\end{table}
    
\begin{table}[h]
  \begin{center}
  \renewcommand{\arraystretch}{1.3}
   \begin{tabular}{|l|c|c|c|}
    \hline
    Node      & Avg Delivery Latency (ms) & Avg $\Delta_m$ (ms) \\ \hline \hline
    $N_1$   & $21.48$                           & $710.34$                   \\ \hline
    $N_2$   & $23.47$                           & $687.29$                  \\ \hline
    $N_2$   & $25.45$                           & $767.74$                   \\ \hline \hline
    Overall   & $23.47$                           & $721.79$                 \\ \hline
    \end{tabular}
    \caption{Average \textsf{ABcast} Latencies and Calculated $\Delta_m$ - $\rho_{min}$ = 1}
    \label{table:infinite_clients_aramis}
  \end{center}
\end{table}       
    
    \subsection{Evaluation}
    
    \subsection{Summary}

\section{ABcast - Fault Tolerance}
    \subsection{Rational}
    
    \subsection{Experimentation}
    
    \subsection{Results}
    
    \subsection{Evaluation}
    
    \subsection{Summary}
    
\section{Summary}