\chapter{Performance Evaluation}

    \graphicspath{{Chapter6-PerformanceEvaluation/Figs/Vector/}{Chapter6-PerformanceEvaluation/Figs/}}
    

% Probing validation experiments    
\section{DMC Validation}

% Emulated Transactions
\section{AmaaS}
    \subsection{Rational}
	To test our hypothesis that the \textsf{AmaaS} model can improve the scalability of Infinispan's distributed transactions, we developed an experiment that emulates the workflow of these transactions by replicating the \emph{amcast} requests required by Infinispan when executing total order transactions ($\S$ \ref{sec:to_commit}).  This experiment does not utilise Infinispan, or implement a basic transaction manager, rather it focuses purely on replicating the underlying communication stages required by Infinispan transactions.  Existing research \citep{Ruivo:2011:ETO:2120967.2121604} has already shown the benefits of utilising a total order protocol instead of 2PC, therefore our experiments concentrate on the performance of the underlying \emph{amcast} protocol used to coordinate these transactions.  If the underlying protocol is enhanced, with regard to reduced latency and/or increased throughput, as the number of nodes increases, then we can infer that the scalability of Infinispan and other such system's can be improved by utilising said protocol.  Thus, if the \textsf{AmaaS} model consistently outperforms P2P, our hypothesis is true.  
	
   In order to compare and contrast the performance of the \textsf{AmaaS} and P2P approach, it was necessary for two experiments to be created.  The first experiment was designed to evaluate the latency and throughput of the \textsf{AmaaS} model when utilised with different \emph{abcast} protocols.  This experiment not only allows the performance of the \textsf{AmaaS} model to be evaluated, but also for the the performance of the underlying \emph{abcast} protocol utilised between service nodes to be compared using workloads typical of \textsf{AmaaS}.  These experiments utilised a simplified version of the \textsf{SCast} Protocol ($\S$ \ref{sec:scast_protocol}) to coordinate interactions between client nodes, $c$-nodes, and the service's nodes, $s$-nodes; with $c$-nodes representing Infinispan nodes executing transactions.  
   
   The second experiment was designed to measure the performance of \emph{amcast} requests when utilising the P2P approach.  This experiment utilises the same workloads and parameters as the first experiment, however, as per the P2P model, no $s$-nodes are present and consequently there is no need for the \textsf{SCast} protocol.  Instead, the TOA protocol is executed directly between $c$-nodes when emulating transactions.  Utilising the same experiment structure and workload between both experiments enables us to compare the performance of the two system models across a consistent environment.  Thus it is possible for the performance of the TOA protocol, when utilised in both the P2P and \textsf{AmaaS} models, to be contrasted with \textsf{ABcast}s performance in \textsf{AmaaS}.  
	
	The rest of this section is structured as follows.  First we detail the \textsf{AmaaS} experiment and the environment in which it was executed.  We then describe the modifications required by this experiment in order to measure the performance of the P2P model, before presenting the results both sets of experiments.  Finally, we analyse the results of these experiments and present an evaluation of our findings.  
		
	\subsection{Experimentation}
	\subsubsection*{AmaaS}
	We implemented an \textsf{AmaaS} service using the JGroups\citep{JGroups} framework with $n=2$ and $3$ $s$-nodes.  All nodes in the experiment utilised commodity PCs of \emph{3.4GHz Intel Core i7-3770} CPU and 8GB of RAM, running \emph{Fedora 20} and communicating over Gigabit Ethernet. The $s$-nodes are a part of a large university cluster, hence communication delays between them can be quite volatile as they are influenced by other network traffic and by jobs launched by other users.
	
	Our experiments are based upon a heavily modified version of an existing performance test available in the JGroups\citep{JGroups} framework, which mimics the partial replication of key/values in Infinispan\citep{Infinispan}.  In these experiments we utilise ten $c$-nodes in the same cluster, each of which emulates a transaction system that is reliant on the \textsf{AmaaS} service.  Each $c$-node operates 25 concurrent threads to initiate and coordinate transactions, and a transaction $Tx$ involves a set $|Tx.dst| = 3,4,\ldots,10$ $c$-nodes; where $|Tx.dst|$ includes $Tx.c$. A thread coordinating a transaction starts its next transaction, $Tx'$, as soon as it executes a commit/abort decision for the currently active $Tx$. Thus, at any moment, $250$ transactions are in different stages of execution.  All emulated transactions consist purely of key/value write operations and hence require \emph{amcast} messages for coordination.  Infinispan's read requests ($get(k)$) are not emulated, as the retrieval of key/values occurs before $Tx.c$ \emph{amcast}s its $prepare(k)$ message, and therefore has no baring on \emph{amcast} performance.  
	
	A modified version of the \textsf{SCast} protocol is utilised by the $s$-nodes and $c$-nodes to provide \emph{amcast}s for the emulated transactions.  The workflow of a transaction in this system is as follows: A coordinator thread submits its \emph{amcast} request for $Tx$, denoted as $req(Tx)$, with some $s$-node; the latter stores such requests in the ARP in the arrival order. The \emph{Send} thread bundles some or all of these requests in the ARP in their arrival order into a message bundle $mb$, which can have a maximum payload of $1kB$, then \emph{amcast}s $mb$ to all other $s$-nodes.  The \emph{Send} thread waits if the ARP is empty and resumes bundling once ARP becomes non-empty. Thus, the number of requests bundled in any $mb$ varies depending on the request arrival rate. Once $req(Tx)$ has been \emph{amcast} to all $s$-nodes, a response message, $Rsp(Tx)$ it is sent to $Tx.c$ who then disseminates this message to $Tx.dst$ as $mcast(Tx)$.  When all $d \in Tx.dst$ have received and delivered $mcast(Tx)$, the transaction is considered complete and the coordinator thread can start executing $Tx'$.  
	
    Stage 1 of the \textsf{SCast} protocol has been omitted from this implementation as we only compare the performance of the two approaches in a crash-free scenario.  As stage 1 only consists of a single round trip delay, its omission from this implementation will not have a significant impact on the results of our experiments.  
	
	\subsubsection*{P2P}
	In order to test the performance of P2P total order commits we repeated the experiments detailed above, however $c$-nodes coordinate transactions between themselves without utilising any $s$-nodes.  A transaction is considered complete when it has been successfully \emph{amcast} to all $d \in Tx.dst$ by the P2P protocol; where success is defined as all correct destinations delivering the \emph{amcast} message. 
	
	The same cluster of machines were used for both the P2P and \textsf{AmaaS} experiments to ensure a fair comparison between protocols.   
	
	\subsection{Results}\label{sec:AmaaS_results}
	Our performance evaluation focuses on the comparison of the TOA protocol, being utilised in a traditional P2P scenario (\emph{TOA-P2P}), with an \textsf{AmaaS} service that implements the \textsf{SCast} protocol.  The \textsf{AmaaS} experiments were conducted with two different \emph{abcast} protocols used between $s$-nodes.  First, we utilised the TOA protocol between $s$-nodes (\emph{TOA-Service}) in order to provide a like-for-like comparison between the P2P approach and the \textsf{AmaaS} model. We then repeated our experiments with the \textsf{ABcast} protocol utilised between $s$-nodes (\emph{ABvcService}), in order to compare the performance of \textsf{ABcast} with both TOA and TOA-Service.  
    
    Each protocol's performance is measured based upon its average transaction latency and throughput rate. In both TOA-Service and ABService, latency is measured as the time elapsed between a $c$-node's initial transmission of $req(Tx)$ to some $s$-node, and all members of $Tx.dst$ delivering $mcast(Tx)$ to the experiment application. In TOA-P2P, latency is measured as the time taken for all $Tx.dst$ to deliver $Tx$ to the experiment application. For both approaches, throughput is measured as the average number of \emph{abcast}s delivered by the experiment application per second at each $c$-node.
	
	All of our experiments were conducted in isolation in order to prevent any side effects caused by simultaneous execution across the cluster, however we conducted all experiments over approximately the same time period to ensure that the network was under similar loads for all of our experiments. 
	
	\begin{figure}[htbp!]
	 % \centering
	 \includegraphics[width=\textwidth,height=\textheight,keepaspectratio, clip, trim={2cm 3cm 2cm 4cm}]{Latency}
	 \caption{AmaaS Latency Comparison}
	 \label{fig:LatencyGraph}
	\end{figure}
	
	\begin{figure}[htbp!]
	% \centering
	 \includegraphics[width=\textwidth,height=\textheight,keepaspectratio, clip, trim={2cm 3cm 2cm 4cm}]{Throughput}
	 \caption{AmaaS Throughput Comparison}
	 \label{fig:ThroughputGraph}
	\end{figure}
	
	Figures \ref{fig:LatencyGraph} and \ref{fig:ThroughputGraph} show the latency and throughput results for our experiments, with $2N$ and $3N$ representing an \emph{ordering service} that consists of two and three, $s$-nodes respectively.  Each plot on the graph is an average of three \emph{crash-free} trials; where a trial consists of each $c$ node completing $10^4$ transactions for a specific value of $|Tx.dst|$. Thus, in all three trials the TOA-Service and ABService each receive a total of $10^5$ \emph{amcast} requests. In TOA-P2P, each $c$ node initiates $10^4$ TOA executions between its peers.  
	
    \subsection{Evaluation}
    This section is split into two distinct subsections.  First we directly compare the performance of the \textsf{AmaaS} service and the P2P approach, with both experiments utilising the same TOA protocol.  We then evaluate the performance of the ABService in contrast with the previous two approaches, focusing on the differences between the performance of the ABcast and TOA based service.  
    
    \subsubsection*{AmaaS vs P2P}
	In Figure \ref{fig:LatencyGraph} we can see that when $|Tx.dst| \geq 4$, TOA-P2P's \emph{abcast} latencies increase considerably when compared to the two TOA-Service experiments.  With TOA-P2P experiencing approximately a $25\%$ and $50\%$ increase in average latency when compared to TOA-Service-3N and TOA-Service-2N respectively.  Thus, indicating that \emph{amcast}ing is best provided as a service as the number of clients involved in a transaction increases. Comparing throughput in Figure \ref{fig:ThroughputGraph} leads to similar conclusions, with the steady throughput observed as $|Tx.dst| \rightarrow 10$ also suggesting an absence of node saturation.  
	
	TOA-P2P's superior performance when $Tx.dst < 4$ can be attributed to the additional stages involved when utilising the \emph{AmaaS} model.  For example when TOA-Service utilises two $s$-nodes ($2N$) the following stages are required: $Tx.c$ sends a request, the \emph{multicast service} \emph{abcast}s it with $|m.dst| = 2$ to all $s$-nodes and returns it to $Tx.c$, who must then multicast $mcast(Tx)$ to $Tx.dst$.  Ignoring the individual message cost of each stage the total number of stages is four, whereas in TOA-P2P the only step required is the \emph{amcast}ing of $Tx$.  So although $|m.dst|$ for each \emph{amcast} is less in TOA-Service ($|m.dst| = 2$) than TOA-P2P ($|m.dst| = 3$), the overhead of sending a request to the \emph{multicast service} and back is much greater than the savings offered by reducing $|m.dst|$ by one node.  However, as $|Tx.dst|$ increases, the overhead of TOA-P2P's increased $|m.dst|$ becomes significant, to the point where TOA-Service's additional communication stages becomes less of an overhead than the cost of TOA-P2P \emph{amcat}ing to a large $m.dst$.  
	
    \subsubsection*{ABService vs TOA-Service}
    
	
	\subsection{Summary}
	When deploying a large-scale distributed transaction system, higher throughout and lower-latency can be achieved by utilising a separate service to provide \emph{amcast} capabilities when the number of nodes participating in a transaction is greater than three.  

\section{ABcast - Infinite Clients}

\section{ABcast - Fault Tolerance}

\section{Summary}