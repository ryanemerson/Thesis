\chapter{Performance Evaluation}

    \graphicspath{{Chapter6-PerformanceEvaluation/Figs/Vector/}{Chapter6-PerformanceEvaluation/Figs/}}
    

% Probing validation experiments    
%\section{DMC Validation}

% Emulated Transactions
\section{AmaaS}
	To test our hypothesis that the \textsf{AmaaS} model can improve the scalability of Infinispan's distributed transactions, we developed an experiment that emulates the workflow of these transactions by replicating the \emph{amcast} messages sent by Infinispan when executing total order transactions ($\S$ \ref{sec:to_commit}).  This experiment does not utilise Infinispan, or implement a basic transaction manager, rather it focuses purely on replicating the underlying communication stages required by Infinispan transactions.  Existing research \citep{Ruivo:2011:ETO:2120967.2121604} has already shown the benefits of utilising a total order protocol instead of 2PC, therefore our experiments concentrate on the performance of the underlying \emph{amcast} protocol used to coordinate these transactions.  In our experiments, if a new \emph{amcast} protocol can demonstrably increase the throughput and reduce the latency of \emph{amcast} messages, as the number of destinations increase,  then we can infer that the scalability of the Infinispan system will be improved by adopting this protocol.  Therefore, if our experiments show that the \textsf{AmaaS} model consistently outperforms P2P, then we assume our hypothesis to be true. Furthermore, if the \textsf{AmaaS} service utilises the \textsf{ABcast} protocol and performs similarly to when a deterministic protocol is used, we can conclude that the \textsf{ABcast} protocol provides low-latency message delivery in the absence of node failures and within the context of \textsf{AmaaS}.  
	
   In order to compare and contrast the performance of the \textsf{AmaaS} and P2P approach, it was necessary for two experiments to be created.  The first experiment was designed to evaluate the latency and throughput of the \textsf{AmaaS} model when utilised with different \emph{abcast} protocols.  This experiment not only allows the performance of the \textsf{AmaaS} model to be evaluated, but also for the the performance of the underlying \emph{abcast} protocol utilised between service nodes to be compared using workloads typical of \textsf{AmaaS}.  These experiments utilised a simplified version of the \textsf{SCast} Protocol ($\S$ \ref{sec:scast_protocol}) to coordinate interactions between client nodes, $c$-nodes, and the service's nodes, $s$-nodes; with $c$-nodes representing Infinispan nodes executing transactions.  
   
   The second experiment was designed to measure the performance of \emph{amcast} requests when utilising the P2P approach.  This experiment utilises the same workloads and parameters as the first experiment, however, as per the P2P model, no $s$-nodes are present and consequently there is no need for the \textsf{SCast} protocol.  Instead, the TOA protocol is executed directly between $c$-nodes when emulating transactions.  Utilising the same experiment structure and workload between both experiments enables us to compare the performance of the two system models across a consistent environment.  Thus it is possible for the performance of the TOA protocol, when utilised in both the P2P and \textsf{AmaaS} models, to be contrasted with \textsf{ABcast}s performance in \textsf{AmaaS}.  
	
	The rest of this section is structured as follows.  First we detail the \textsf{AmaaS} experiment and the environment in which it was executed.  We then describe the modifications required by this experiment in order to measure the performance of the P2P model, before presenting the results both sets of experiments.  Finally, we analyse the results of these experiments and present an evaluation of our findings.  
		
	\subsection{Experimentation}\label{ssec:emulated_transaction_experiments}
	\subsubsection*{AmaaS}
	We implemented an \textsf{AmaaS} service using the JGroups\citep{JGroups} framework with $n=2$ and $3$ $s$-nodes.  All nodes in the experiment utilised commodity PCs of \emph{3.4GHz Intel Core i7-3770} CPU and 8GB of RAM, running \emph{Fedora 20} and communicating over Gigabit Ethernet. The $s$-nodes and $c$-nodes utilised in our experiments are a part of a large university cluster, hence communication delays between them can be quite volatile as they are influenced by other network traffic and by jobs launched by other users.
	
	Our experiments are based upon a heavily modified version of an existing performance test available in the JGroups\citep{JGroups} framework, which mimics the partial replication of key/values in Infinispan\citep{Infinispan}.  In these experiments we utilise ten $c$-nodes in the same cluster, each of which emulates a transaction system that is reliant on the \textsf{AmaaS} service.  Each $c$-node operates 25 concurrent threads to initiate and coordinate transactions, and a transaction $Tx$ involves a set $|Tx.dst| = 3,4,\ldots,10$ $c$-nodes; where $|Tx.dst|$ includes $Tx.c$. A thread coordinating a transaction starts its next transaction, $Tx'$, as soon as it executes a commit/abort decision for the currently active $Tx$. Thus, at any moment, $250$ transactions are in different stages of execution.  All emulated transactions consist purely of key/value write operations and hence require \emph{amcast} messages for coordination.  Infinispan's read requests ($get(k)$) are not emulated, as the retrieval of key/values occurs before $Tx.c$ \emph{amcast}s its $prepare(k)$ message, and therefore has no baring on \emph{amcast} performance.  
	
	A modified version of the \textsf{SCast} protocol is utilised by the $s$-nodes and $c$-nodes to provide \emph{amcast}s for the emulated transactions.  The workflow of a transaction in this system is as follows: A coordinator thread submits its \emph{amcast} request for $Tx$, denoted as $req(Tx)$, with some $s$-node; the latter stores such requests in the ARP in the arrival order. The \emph{Send} thread bundles some or all of these requests in the ARP in their arrival order into a message bundle $mb$, which can have a maximum payload of $1kB$ \footnote{In the experiments that utilise the \textsf{ABcast} protocol, we \emph{pad} the contents of the message bundle to ensure that it is always equal to $1kB$.  This ensures that all messages \emph{abcast} by the protocol are approximately the same size, which increases the accuracy of the DMC's predictions at the expense of redundant bandwidth.}, then \emph{abcast}s $mb$ to all other $s$-nodes.  The \emph{Send} thread waits if the ARP is empty and resumes bundling once ARP becomes non-empty\footnote{Hence, the number of requests bundled in any $mb$ varies depending on the request arrival rate.}. Once $req(Tx)$ has been \emph{abcast} to all $s$-nodes, a response message, $Rsp(Tx)$ it is sent to $Tx.c$ who then disseminates this message to $Tx.dst$ as $mcast(Tx)$.  When all $d \in Tx.dst$ have received and delivered $mcast(Tx)$ as per the delivery conditions of the \textsf{SCast} protocol, the transaction is considered complete and the coordinator thread can start executing $Tx'$.  
	
    Stage 1 of the \textsf{SCast} protocol has been omitted from this implementation, this is because we only compare the performance of the two approaches in a crash-free scenario.  It is possible for fault-tolerance to be provided to an \textsf{AmaaS} \emph{amcast} protocol in multiple ways, therefore we do not include this additional step as our focus is on performance in normal conditions.  However, it is worth noting, that as stage 1 only consists of a single round trip delay, its inclusion in the implementation would only have a marginal effect on the results of our experiments.  	
	
    In our experiments that utilise \textsf{ABcast}, an additional phase is required before the experiment detailed above can begin.  Prior to accepting requests from $c$ nodes, $s$-nodes go through a \textquoteleft{}warm-up' phase lasting approximately 1-2 seconds.  During this phase, the clocks of the $s$-nodes are synchronized and each $s$-node broadcasts $10^3$ \emph{probe} messages, with a payload of $1kB$, to all other $s$-nodes.  The purpose of these probe messages is to record the $NT_P$ latencies required by the \textsf{ABcast}'s DMC to calculate the initial estimates of $x_{mx}$, $q$, $\rho$, $\eta$ and $\omega$ .  Without this period, $s$-nodes would be unable to send \emph{abcast}s as the DMC would not have calculated the variables required by the Aramis protocol.  
    
    In addition to this \textquoteleft{}warm-up' phase, another important aspect of our \textsf{ABcast} implementation is the broadcasting of redundant message copies.  In $\S$ \ref{ssec:rbcast} we  state that at least $\rho$ copies of a message $m$ are broadcast, such that $m.copy = 0,\ldots,\rho$, however in our implementation we only send $m.copy = 0,1$.  We have \textquoteleft{}capped' $m.copy$ to reduce the overall bandwidth required by the protocol, whilst maintaining a sufficient number of redundant broadcasts to ensure that fault-tolerance is preserved.  Note, that although $m.copy$ no longer respects $\rho$, $\rho$ is still utilised for calculating the DMC's variables and $\Delta_m$.  
    
    Finally, all of our experiments with \textsf{ABcast} utilise the the following constant values.  The DMC utilises $R=0.9999$ for calculating various parameters ($\S$ \ref{ssec:dmc}), and AFC utilises $\delta_{min}$ and $\delta_{max}$ values equal to $1ms$ and $10ms$ respectively, with $Ss = 10$ ($\S$ \ref{sec:afc_protocol}).  
	
	\subsubsection*{P2P}
	In order to test the performance of P2P total order transactions we repeated the experiments detailed above, however, as per the P2P model, all $c$-nodes coordinate transactions between themselves without utilising any $s$-nodes.  In these experiments, a transaction is considered complete when it has been successfully \emph{amcast} to all $d \in Tx.dst$ by the P2P protocol; where success is defined as all correct destinations delivering the \emph{amcast} message. 
	
	The same cluster of machines were used for both the P2P and \textsf{AmaaS} experiments to ensure a fair comparison between protocols.   
	
	\subsection{Results}\label{sec:AmaaS_results}
	Our performance evaluation focuses on the comparison of the TOA protocol, being utilised in a traditional P2P scenario (\emph{TOA-P2P}), with an \textsf{AmaaS} service that implements the \textsf{SCast} protocol.  The \textsf{AmaaS} experiments were conducted with two different \emph{abcast} protocols used between $s$-nodes.  First, we utilised the TOA protocol (\emph{TOA-Service}) in order to provide a like-for-like comparison between the P2P approach and the \textsf{AmaaS} model. We then repeated our experiments with the \textsf{ABcast} protocol utilised between $s$-nodes (\emph{ABService}), in order to compare the performance of \textsf{ABcast} with both TOA and TOA-Service.  
    
    The performance of all three approaches is measured based upon the average transaction latency and throughput rate. In both TOA-Service and ABService, latency is measured as the time elapsed between a $c$-node's initial transmission of $req(Tx)$ to some $s$-node, and all members of $Tx.dst$ delivering $mcast(Tx)$ to the experiment application. In TOA-P2P, latency is measured as the time taken for all $Tx.dst$ to deliver $Tx$ to the experiment application. For both approaches, throughput is measured as the average number of \emph{abcast}s delivered by the experiment application per second at each $c$-node.
	
	All of our experiments were conducted in isolation in order to prevent any side effects caused by simultaneous execution across the cluster, however we conducted all experiments over approximately the same time period to ensure that the network was under similar loads for all of our experiments. 
	
	Figures \ref{fig:LatencyGraph} and \ref{fig:ThroughputGraph} show the latency and throughput results for our experiments, with $2N$ and $3N$ representing an \textsf{AmaaS} service that consists of two and three, $s$-nodes respectively.  Each plot on the graph is an average of three \emph{crash-free} trials; where a trial consists of each $c$-node completing $10^4$ transactions for a specific value of $|Tx.dst|$. Thus, in all three trials the TOA-Service and ABService each receive a total of $10^5$ \emph{amcast} requests. In TOA-P2P, each $c$-node initiates $10^4$ TOA executions between its peers ($10$ $c$-nodes $\times 10^4 = 10^5$).  
	
	Concerning \textsf{AmaaS} performance, Table \ref{table:emulated_transaction_averages} shows the average number of client requests received, the average number of \emph{abcast} messages sent and the average number of requests bundled into each \emph{abcast}, for all of our experiments that utilised a \textsf{AmaaS} service.  All of the average values are calculated based upon the statistics recorded by each $s$-node during our experiments. 
	
    \begin{table}[h]
  \begin{center}
    \begin{tabular}{|l|c|c|c|}
    \hline
    Experiment & $\#$ Client Requests  & $\#$ \emph{abcast}s & Bundle Rate \\ \hline \hline
    ABService-2N     & 50000    &    12763.4    &    4 \\ \hline
    TOA-Service-2N  & 50000    &   16632    &   3 \\ \hline
    ABService-3N     & 33333.3 &    13416.4  &   2.5 \\ \hline
    TOA-Service-3N  & 33333.3 &    13507.8 & 2.5 \\ \hline
    \end{tabular}
    \caption{Average Node Statistics for Emulated Transaction Experiments}
    \label{table:emulated_transaction_averages}
  \end{center}
\end{table}	
	
    Table \ref{table:emulated_transcation_aramis_deliveries} shows the performance of the \textsf{ABcast} protocol in both the ABService-2N and 3N experiments.  It shows the average number of \emph{abcast}s sent per node and the average number of these messages that were delivered by the Aramis protocol, as well as providing the total percentage of \emph{abcast}s that were delivered via Aramis.  Furthermore, this table also details the ratio of $s$-nodes that delivered an \emph{abcast} via Aramis compared to the total number of $s$-nodes utilised.  For example, in the case of ABService-2N we performed $24$ experiments, therefore we have statistics for $48$ $s$-nodes and our records show that only $3$ of these $48$ nodes utilised Aramis to deliver one or more \emph{abcast} message.  
	
	\begin{table}[h]
	  \begin{center}
	    \begin{tabular}{|l|c|c|c|c|}
	    \hline
	    Experiment  & $\#$ \emph{abcast}s & Nodes Effected &  Avg Aramis Deliveries & $\%$ Aramis Deliveries \\ \hline \hline
	    ABService-2N & 12763.4 & 3:48   & 10.8  & $0.085\%$  \\ \hline
	    ABService-3N & 13416.4 & 30:72 & 15.3  & $0.114\%$ \\ \hline
	    \end{tabular}
	    \caption{Average ABcast Statistics per Node}
	    \label{table:emulated_transcation_aramis_deliveries}
	  \end{center}
	\end{table}	
	
	\begin{figure}[tp]
	 % \centering
	 \includegraphics[width=\textwidth,height=\textheight,keepaspectratio, clip, trim={2cm 3.5cm 2cm 3cm}]{Latency2}
	 \caption{AmaaS Latency Comparison}
	 \label{fig:LatencyGraph}
	\end{figure}
	
	\begin{figure}[bp]
	% \centering
	 \includegraphics[width=\textwidth,height=\textheight,keepaspectratio, clip, trim={2cm 3.5cm 2cm 3cm}]{Throughput2}
	 \caption{AmaaS Throughput Comparison}
	 \label{fig:ThroughputGraph}
	\end{figure}	
	
	\clearpage
    \subsection{Evaluation}
    This section is split into three distinct subsections.  First we directly compare the performance of the \textsf{AmaaS} service and the P2P approach with both experiments utilising the same TOA protocol.  We then evaluate the performance of the ABService in contrast with the previous two approaches, focusing on the differences between the performance of the ABcast and TOA based service.  Finally, we evaluate the performance of \textsf{ABcast}, focusing on how often the \textsf{Aramis} protocol was utilised to deliver messages and its ability to maintain \emph{abcast} guarantees G1-G4.  
    
    \subsubsection*{AmaaS vs P2P}
	In Figure \ref{fig:LatencyGraph} we can see that when $|Tx.dst| \geq 4$, TOA-P2P's \emph{abcast} latencies increase considerably when compared to the two TOA-Service experiments.  With TOA-P2P experiencing approximately a $25\%$ and $50\%$ increase in average latency when compared to TOA-Service-3N and TOA-Service-2N respectively.  Thus, indicating that \emph{amcast}ing is best provided as a service as the number of clients involved in a transaction increases. Comparing throughput in Figure \ref{fig:ThroughputGraph} leads to similar conclusions, with the steady throughput observed as $|Tx.dst| \rightarrow 10$ also suggesting an absence of node saturation.  
	
	TOA-P2P's superior performance when $Tx.dst < 4$ can be attributed to the additional stages involved when utilising the \emph{AmaaS} model.  For example when TOA-Service utilises two $s$-nodes ($2N$) the following stages are required: $Tx.c$ sends a request, the \emph{multicast service} \emph{abcast}s it with $|m.dst| = 2$ to all $s$-nodes and returns it to $Tx.c$, who must then multicast $mcast(Tx)$ to $Tx.dst$.  Ignoring the individual message cost of each stage the total number of stages is four, whereas in TOA-P2P the only step required is the \emph{amcast}ing of $Tx$.  So although $|m.dst|$ for each \emph{amcast} is less in TOA-Service ($|m.dst| = 2$) than TOA-P2P ($|m.dst| = 3$), the overhead of sending a request to the \emph{multicast service} and back is much greater than the savings offered by reducing $|m.dst|$ by one node.  However, as $|Tx.dst|$ increases, the overhead of TOA-P2P's increased $|m.dst|$ becomes significant, to the point where TOA-Service's additional communication stages becomes less of an overhead than the cost of TOA-P2P \emph{amcat}ing to a large $m.dst$.  
	
    \subsubsection*{ABService vs TOA-Service}
    In Figure \ref{fig:LatencyGraph} we can see that the latencies encountered by the ABService-2N and TOA-Service-2N experiments are very similar regardless of the number of clients involved in a transaction, with the maximum difference between any two plots being no greater than $0.3$ milliseconds.  Interestingly, our experiments show that in the majority of experiments ($5/8$), the ABService does not just match the performance of the TOA-Service, but actually outperforms it.  This superior performance can be attributed to a combination of two factors: the number of requests that are bundled on average per \emph{abcast} and the overall message cost associated with the underlying \emph{abcast} protocol.  
    
    The average number of client requests bundled into a single \emph{abcast} can play a decisive role in the latency and throughput of a \textsf{AmaaS} service as the higher the average bundle rate, the lower the total number of \emph{abcast}s required.  As the \emph{abcast}ing of requests between $s$-nodes is the most expensive operation, in terms of bandwidth and latency in the \textsf{AmaaS} model, it is self-evident that reducing their frequency will reduce the average latency encountered by client requests, therefore reducing the total duration of a transaction.  
    
    Table \ref{table:emulated_transaction_averages} shows that the average bundle rate for the ABService-2N was $4$ messages, whilst it was only $3$ for TOA-Service-2N.  Therefore, on average a node in TOA-Service-2N sends $\approx 3869$ more \emph{abcast}s then its counterpart ABService-2N, which partially explains the difference in performance between the two approaches.  
    
    The difference in overall message cost between the two \emph{abcast} protocols is a consequence of the two different approaches to solving \emph{abcast} and the optimisations present in the \textsf{ABcast} protocol ($\S$  \ref{ssec:atomic_broadcast} $\&$ \ref{ssec:base_ack_piggyback}).  The \textsf{ABcast} protocol piggybacks any outstanding message acknowledgements on subsequent message broadcasts, enabling \emph{abcast}s to be executed in a single phase when all nodes are frequently sending \emph{abcast}s.  Whereas, the JGroups implementation of the TOA protocol does not implement any optimisations, and thus, each broadcast always consists of two phases, therefore increasing the average latency encountered by transaction requests.  
	
	Correspondingly, it is possible to observe that the average and maximum difference between the latencies encountered in the ABService-3N and TOA-Service-3N experiments is greater than that observed when $N = 2$.  This increase in the performance gap can be attributed to the \textsf{ABcast} optimisations becoming more effective when the number of $s$-nodes increases due to the total number of messages no longer required by \textsf{ABcast} also increasing.  For example, if $N = 3$ and \textsf{ABcast} sends a broadcast, the total message cost for that single \emph{abcast} is only $2$ unicasts, whereas with TOA the total cost is $6$ unicasts due to the two phases required by the protocol.  Clearly, the potential for such reductions in message cost will have a positive effect on the performance of the ABService implementation, especially when service requests are evenly distributed amongst $s$-nodes and are arriving frequently.  
	
    Interestingly, in Table \ref{table:emulated_transaction_averages} we can see that the average bundle rate of ABService-3N and TOA-Service-3N are almost the same, yet the difference between the observed latencies in the two approaches has increased.  	Thus in these experiments the average bundle rate has no significant impact on the performance of the two approaches.  
    
    The large difference between the average bundle rate observed in ABService-2N and 3N, is a direct consequence of the DMC's calculations and how AFC ($\S$ \ref{sec:afc_protocol}) manages broadcast rates.  Recall that the delay imposed by AFC, for an \emph{abcast} message, increases when latencies start to exceed the previously calculated $x_{mx}$ value, and decreases to $\delta_{min}$ when no such latencies are observed.  When $2 s$-nodes are utilised, the observed $x_{mx}$ is typically lower than $3 s$-nodes, as the number of unicasts sent between $s$-nodes is less; hence the probability of large delays being observed is reduced.  The smaller the average $x_{mx}$ value, the more susceptible the system is to delays periodically exceeding $x_{mx}$.  Therefore, when $2 s$-nodes are utilised the probability of the calculated AFC delay regularly exceeding $\delta_{min}$ increases, which in turn reduces the node's broadcast rate.  Consequently, the number of requests which can accumulate between \emph{abcast}s will increase, and hence the average bundle rate also increases.  When $3 s$-nodes are utilised, the DMC's observations are typically more stable, resulting in less \emph{outlier} latencies being recorded and the broadcast rate being more stable; hence an average bundle rate that is approximately the same as the TOA-Service-3N.  
	
	The throughput of the ABService and TOA-Service for both $2N$ and $3N$ follows a very similar pattern to that observed when analysing their latencies.  This is not surprising as the average transaction latency has a direct impact on the average rate of throughput.  Combining the results shown in Figures \ref{fig:LatencyGraph} and \ref{fig:ThroughputGraph}, it is clear to see that the ABService provides comparable performance to that of the TOA-Service and that both of these \textsf{AmaaS} solutions consistently outperform TOA-P2P when $Tx.dst > 3$.  
	
	\subsubsection*{ABcast}
	In Figure \ref{table:emulated_transcation_aramis_deliveries} we can see that only $3$ of the $48$ nodes utilised by ABService-2N, delivered an \emph{abcast} via the \textsf{Aramis} protocol, with the average number of messages being $\approx 11$, only $0.085\%$ of all messages.  Hence, the $\Delta_m$ value calculated by the DMC was sufficient for  $99.915\%$ of \emph{abcast}s.  The results of the ABService-3N experiments shows that as the number of $s$-nodes increased,  the total number of \textsf{Aramis} deliveries also increased.  Almost $50\%$ of nodes delivered at least one message via \textsf{Aramis}, with an overall average of $\approx 16$ messages per node.  Although this is a large increase in the number of nodes requiring \textsf{Aramis}, the protocol still only accounts for $0.114\%$ of all \emph{abcast}s sent.  
	
	The increase in \textsf{Aramis} deliveries as the number of $s$-nodes increase can be attributed to the DMC recording each latency anomalously (without regard for source of the message) and calculating $\Delta_m$ based upon these latencies.  In the experiments where $N=2$, we know that all of the latencies recorded by node $n_1$ will be from messages originating at $n_2$.  Therefore, when node $n_1$ broadcasts message $m$, it is guaranteed that the calculated $\Delta_m$ has been calculated utilising latencies representative of $n_2$'s past performance.  Whereas, when $n=3$, $n_1$ will have calculated $\Delta_m$ based upon latencies recorded from both $n_2$ and $n_3$, therefore it is possible that if $n_3$ is slower than $n_2$, the latencies calculated from $n_2$ will dilute the larger latencies recorded from messages originating at $n_3$.  Thus, the calculated $\Delta_m$ could be smaller than the value required by the slower node $n_3$.  
	
	None of the experiments that delivered a message via \textsf{Aramis} were required to reject an \emph{abcast}, therefore guarantees G1-G4 were maintained and the state of the \textsf{AmaaS} service remained consistent throughout all of our experiments.  Furthermore, we repeated our experiments with deliver condition $D1_B$ of the \textsf{Base} protocol disabled, meaning that messages can only be delivered via \textsf{Aramos}, in order to evaluate the accuracy of $\Delta_m$ in this environment.  We found that, for both $2N$ and $3N$, the calculated $\Delta_m$ was sufficient for all nodes to deliver messages without a single rejection occurring.  As expected, latencies were large, and they were so large that a single experiment (involving $10^5$ transactions) took several minutes to complete.  Since these latencies make the \textsf{AmaaS} approach redundant, these experiments are simply an illustration $\delta_m$'s ability to preserve \emph{abcast} guarantees G1-G4.  
		
	\subsection{Summary}
	When deploying a large-scale distributed transaction system that executes transactions that span several nodes ($n > 3$), higher throughout and lower-latency can be achieved by utilising the \textsf{AmaaS} model for \emph{amcast}ing.  Furthermore, such a service can provide non-blocking \emph{amcast}s when \textsf{ABcast} is utilised between service nodes, whilst maintaining comparable levels of performance to GM-based protocols.  

\section{ABcast - Infinite Clients}
    In the previous section, we tested the performance of the \textsf{AmaaS} approach whilst utilising the \textsf{ABcast} protocol.  Our results showed, that the \textsf{Aramis} protocol was rarely required to deliver messages, accounting for only $0.015\%$ and $0.114\%$ of messages, when the number of $s$-nodes was two and three respectively.  However, in these experiments the total number of \emph{abcast} messages was, on average, relatively low for each node; typically less than $2 \times 10^4$.  Furthermore, each $s$-node's rate of \emph{abcast}s would vary depending on the restrictions of the AFC protocol and the rate at which requests were being received by $c$-nodes.  Due to the number of client nodes being relatively small, only ten were utilised, it is probable that at times an $s$-node's ARP would be empty.  Therefore, in order to test the performance of \textsf{ABcast} under heavy loads, it was necessary for a new experiment to be developed.  The purpose of these experiments are two fold.  First, they allow us to measure how often \textsf{Aramis} is required to deliver messages and how often such deliveries cause messages to be rejected.  Secondly, they allow us to monitor the values calculated by the DMC during high levels of network load and determine their effect on the resulting $\Delta_m$.  
    
    In order to test the performance of \textsf{ABcast} under heavy loads, we could simply increase the number of client nodes that were used in our previous experiment, however this would require a large amount of resources and would be cumbersome to orchestrate.  Furthermore, such an approach does not guarantee that the ARP of a given $s$-node will always have a request to process.  Therefore, we propose a new experiment that we call an \emph{infinite client system} as it represents the performance of \textsf{AmaaS} ordering service if each $s$-node always had a full ARP.  This experiment does not utilise client nodes at all, instead, it simply consists of $n$ nodes executing $x$ \emph{abcast}s between themselves as fast as possible.  This is the same as the steps required by \textsf{SCast}, however we do not have the overhead of maintaining a state at each node, therefore the delay between subsequent \emph{abcast}s will be less in this experiment, hence \textsf{ABcast} will be under a heavier load then is possible when utilised with \textsf{SCast}. 
    
    The rest of this section is structured as follows.  First we detail the infinite client experiment and how it is implemented.  We then present the results of our experiments, before analysing them and providing an evaluation of our findings. 
    
    \subsection{Experimentation}\label{ssec: infinite_experimentation}
    The infinite client experiment was implemented using the JGroups framework and all of our experiments utilised the same implementation of \textsf{ABcast} as those discussed in $\S$ \ref{ssec:emulated_transaction_experiments}.  Furthermore, our experiments utilised the same specification of machine as our previous experiments from within the same university cluster.  
    
    An individual experiment consists of $3$ nodes sending $10^6$ \emph{abcast}s between each other; with each individual node sending $\frac{10^6}{n}$ messages with a payload of $1kB$.  Each node broadcasts s request as fast as possible using a single thread, which represents the \emph{sender} thread utilised in \textsf{SCast} to process the ARP and broadcast requests between $s$-nodes.  As soon as a message has been sent, another broadcast via \textsf{ABcast} is initiated; where the sending of a message $m$ consists of $m$ being sent down the JGroups stack, processed and delayed by AFC, before being unicast to all $n$ nodes.  An experiment is considered complete when each node has delivered $10^6$ messages, or if one or more messages are rejected by a node, then $10^6 - \#rejections$.  For all of our experiments the \textsf{ABcast} protocol used the following constant values: $R = 0.9999$, $\delta_{min} = 1ms$, $\delta_{max} = 10ms$ and $Ss = 10$.  
    
    All of our experiments record the $m.id$ of each \textsf{ABcast} message, in the order that they were delivered to the application, across several text files.  This allows us to ensure that our implementation is correct and that violations to the \emph{abcast} total order only occur if one or more messages are rejected by \textsf{Aramis}.  For all of our experiments, this was true, therefore we know that the number of total order violations is always equal to the number of \textsf{Aramis} rejections.  Henceforth, all references to a message being rejected infers that a violation of the \emph{abcast} total order has occurred.  
    
    \subsection{Results}
    % Latency recorded as the difference between a messages initial broadcast and delivery to application
We executed the experiment detailed in $\S$ \ref{ssec: infinite_experimentation} a total of ten times, utilising the same machines for each experiment.  Table \ref{table:emulated_transcation_aramis_deliveries} presents the results of each of these experiments based upon each node's individual performance as well as the performance of the cluster as a whole; where $N_1, N_2, N_3$ correspond to the values recorded by an individual node and we define the cluster as being the combined performance of $\{N_1,N_2,N_3\}$.  For each node in an experiment, we show the total number of \emph{abcast}s that were delivered by \textsf{Aramis} and in brackets the number of messages rejected by the protocol.  We also show the total number of \emph{abcast}s delivered by \textsf{Aramis} across the cluster, and the percentage of all \emph{abcast}s that are delivered by \textsf{Aramis}.  
    
\begin{table}[p]
  \begin{center}
  \renewcommand{\arraystretch}{1.3}
   \begin{tabular}{|l|c|c|c|c|c|}
    \hline
    Experiment & $N_1$ & $N_2$       & $N_3$      & Total    & $\%$ of all \emph{abcast}s \\ \hline \hline
    1          & 9220, (0)  & 7929, (0)  & 6434, (0)  & 23538 & 2.36 \\ \hline
    2          & 3348, (0)  & 4555, (0)  & 5008, (0)  & 12911 & 1.29 \\ \hline
    3          & 4496, (0)  & 4920, (0)  & 1952, (0)  & 11368 & 1.14 \\ \hline
    4          & 5832, (0)  & 6439, (0)  & 4801, (0)  & 17072 & 1.71 \\ \hline
    5          & 5320, (0)  & 5757, (0)  & 4066, (0)  & 15143 & 1.51 \\ \hline
    6          & 4181, (0)  & 3286, (0)  & 4157, (0)  & 11624 & 1.16 \\ \hline
    7          & 1743, (0)  & 2237, (0)  & 2235, (0)  & 6215   & 0.62 \\ \hline
    8          & 4188, (0)  & 1846, (0)  & 5421, (0)  & 11455 & 1.15 \\ \hline
    9          & 5621, (0)  & 4242, (0)  & 5291, (0)  & 15154 & 1.52 \\ \hline
    10        & 2953, (0)  & 5014, (0)  & 3192 , (0) & 11159 & 1.12 \\ \hline \hline
    Total    &46902, (0) &46225, (0) &42557, (0) &135684 & 1.36\\ \hline
    \end{tabular}
    \caption{Aramis deliveries (rejections) for infinite clients - $\rho_{min}$ = 1}
    \label{table:infinite_clients_rejections}
  \end{center}
\end{table}
    
    Table \ref{table:emulated_transcation_aramis_deliveries} presents the average delivery latency encountered by all \emph{abcast}s sent via \textsf{ABcast} (including those delivered by \textsf{Aramis}), as well as the average $\Delta_m$ value calculated by each node.  Each node records its delivery delay as $Dt_m - m.ts$, where $m.ts$ is the timestamp allocated to an \emph{abcast} message $m$ when an \emph{abcast} is initiated and $Dt_m$ is the time at which $m$ is passed upto the application.  The average $\Delta_m$ value is recorded using a given node's calculations of $\Delta$ not those recorded by others, thus $N_1$'s average is calculated using only $\Delta$ values calculated by $N_1$'s DMC.  Hence, the \textquoteleft{}overall' entry in the table provides the average $\Delta$ value calculated by the entire cluster.  
    
\begin{table}[p]
  \begin{center}
  \renewcommand{\arraystretch}{1.3}
   \begin{tabular}{|l|c|c|c|}
    \hline
    Node      & Avg Delivery Latency (ms) & Avg $\Delta_m$ (ms) \\ \hline \hline
    $N_1$   & $21.48$                           & $710.34$                   \\ \hline
    $N_2$   & $23.47$                           & $687.29$                  \\ \hline
    $N_2$   & $25.45$                           & $767.74$                   \\ \hline \hline
    Overall   & $23.47$                           & $721.79$                 \\ \hline
    \end{tabular}
    \caption{Average \textsf{ABcast} Latencies and Calculated $\Delta_m$ - $\rho_{min}$ = 1}
    \label{table:infinite_clients_aramis_latencies}
  \end{center}
\end{table}       

    %\begin{table}[h]
%  \begin{center}
%  \renewcommand{\arraystretch}{1.3}
%   \begin{tabular}{|l|c|c|c|}
%    \hline
%    Experiment & $N_1$   & $N_2$   & $N_3$      \\ \hline \hline
%    1          & 3003, (0)  & 3385, (0)  & 705, (0)    \\ \hline
%    2          & 303, (0)    & 353, (0)    & 279, (0)   \\ \hline
%    3          & 4192, (0)  & 3734, (0)  & 1727, (0)  \\ \hline
%    4          & 2324, (0)  & 2478, (0)  & 1094, (0)    \\ \hline
%    5          & 2640, (0)  & 2121, (0)  & 1804, (0)   \\ \hline
%    6          & 1040, (0)  & 1408, (0)  & 1365, (0)   \\ \hline
%    7          & 403, (0)    & 2108, (0)  & 2135, (0)   \\ \hline
%    8          & 1714, (0)  & 1303, (0)  & 1871, (0)   \\ \hline
%    9          & 2282, (0)  & 1163, (0)  & 3122, (0)   \\ \hline
%    10        & 143, (0)    & 391, (0)    &  361, (0)   \\ \hline
%    Total    &18044, (0) &18419, (0) &14463, (0) \\ \hline
%    \end{tabular}
%    \caption{Aramis deliveries (failures) for infinite clients - $\rho_{min}$ = 3}
%    \label{table:infinite_clients_aramis}
%  \end{center}
%\end{table}
%    
%\begin{table}[h]
%  \begin{center}
%  \renewcommand{\arraystretch}{1.3}
%   \begin{tabular}{|l|c|c|c|}
%    \hline
%    Node      & Avg Delivery Latency (ms) & Avg $\Delta_m$ (ms) \\ \hline \hline
%    $N_1$   & $19.98$                           & $869.34$                   \\ \hline
%    $N_2$   & $20.43$                           & $823.32$                   \\ \hline
%    $N_2$   & $20.96$                           & $824.65$                   \\ \hline \hline
%    Overall   & $20.46$                           & $8391.14$                 \\ \hline
%    \end{tabular}
%    \caption{Average \textsf{ABcast} Latencies and Calculated $\Delta_m$ - $\rho_{min}$ = 3}
%    \label{table:infinite_clients_aramis}
%  \end{center}
%\end{table}   

    \subsection{Evaluation}
    In Table \ref{table:emulated_transaction_averages} we can see that out of all $10^7$ messages, only $1.36\%$ of \emph{abcast}s were delivered by \textsf{Aramis}.  Furthermore, out of these $135684$ \textsf{Aramis} deliveries not a single \emph{abcast} was rejected, therefore \emph{abcast} guarantees G1-G4 were maintained by \textsf{ABcast} even when the rate of requests was very high.  This lack of rejections implies that the calculated $\Delta_m$ is sufficiently large to prevent messages being missed, whilst still being small enough for some \emph{abcast}s ($1.36\%$) to be delivered before all acknowledgements required by the deterministic \textsf{Base} protocol have been received.  Thus, for $1.36\%$ of \emph{abcast}s the \textsf{ABcast} protocol reduces latency and prevents message blocking even in the absence of node failures, when compared to traditional GM based protocols.  Finally, the lack of rejections indicates that the protocol is able to handle large number of \emph{abcast} requests over an extended period of time without compromising guarantees G1-G4.  This is vital for utilising \textsf{ABcast} in an \textsf{AmaaS} service, as the underlying \emph{abcast} protocol must be able to handle a large number of requests as the number of clients scale up.
       
    Correspondingly, Table \ref{table:emulated_transcation_aramis_deliveries} shows that the average delivery latency encountered by \emph{abcast} messages remains low even when the network is heavily loaded and the average $\Delta_m$ value remains below $800ms$ for each node.  \textsf{ABcast}s ability to provide to low-latency message delivery in such conditions is crucial, as the speed of the \emph{abcast} protocol utilised in an \textsf{AmaaS} service ultimately determines the response time for each client request.  More significantly, the low average $\Delta_m$ value shows that, even under the heaviest of loads, the DMC is able to calculate an average $\Delta_m$ that is sub $1$ second and still deliver all messages without a single rejection.  This is a vital result as, if the $\Delta_m$ value became increasingly large as the load increased, it would start to exceed the typical delay required by the GM service to detect a node crash, therefore rendering the deterministic/probabilistic hybrid approach redundant.  
    
    \subsection{Summary}
    The \textsf{ABcast} protocol is capable of providing low-latency \emph{abcast}s over a sustained period of time in conditions representative of those found in an \textsf{AmaaS} service.  In such conditions, the DMC consistently calculates a $\Delta_m$ value that is small enough to outperform GM services, whilst being sufficiently large to ensure that no violations of \emph{abcast} guarantees occur when messages are delivered by \textsf{Aramis}.  
    
\section{ABcast - Fault Tolerance}
    In our prior experiments with \textsf{ABcast} we have evaluated the performance of the protocol in the context of an \textsf{AmaaS} service where no node failures occur.  However, as \textsf{ABcast} has been designed to provide the low-latency performance of GM protocols, whilst allowing for non-blocking message delivery when node crashes occur, it is necessary to ensure that $\Delta_m$ is sufficiently small for message delivery to continue before the GM service detects the crash.  Ultimately, if the GM service is able to detect a node crash before any messages are delivered via the \textsf{Aramis} protocol, then the increased complexity of a hybrid approach is unnecessary.  In such a case, a traditional GM based protocol would be more suitable as there is no risk of the \emph{abcast} total order being violated.  Therefore it was necessary to create an experiment that monitors the number of messages, if any, that are delivered by \textsf{ABcast} in the interim period between a node crashing and the GM service detecting it.  Furthermore, our experiments explore the impact of utilising different \textsf{ABcast}'s constant values, such as $\rho_{min}$ and $R$, on the number of messages delivered in this interim period.  More specifically, exploring their impact on the average $\Delta_m$ value calculated by a node and how this impacts the number of messages rejected by \textsf{Aramis}.  
      
   The rest of this section is structured as follows.  First we detail the experiment we designed and all of the parameters utilised by \textsf{ABcast}.  We then present the result of our experiments, before providing an evaluation of our findings.  
    
    \subsection{Experimentation}\label{ssec:crash_experiment}
    In order to test the performance of \textsf{ABcast} when a node crashes, we reuse the experiments detailed in $\S$ \ref{ssec: infinite_experimentation}.  However, in these experiments, instead of all $3$ nodes sending a total of $10^6$ \emph{abcast}s, only $2$ of the nodes complete their broadcasts.  The third node, $N_3$, is crashed after sending $50000$ \emph{abcast}s.  As JGroups is implemented in the Java programming language, we crash $N_3$, by crashing the underlying Java Virtual Machine (JVM) not the physical machine.  In order to understand our experiment and why this is necessary, it is important to consider the design of the GM service and associated protocols that are provided by the JGroups framework.  The remainder of this section details these protocols, how we crash the JVM and the different constant values utilised by \textsf{ABcast}.  
    
    The GM service offered by JGroups, GMS, maintains the current view of network members by adding or removing nodes from the view.  This protocol relies on three \emph{failure detection} protocols located below it in the stack to issue \texttt{SUSPECT} messages to the stack when a node is suspected of crashing.  From the bottom of the stack up, these protocols are called \emph{\texttt{FD\_SOCK}}, \emph{\texttt{FD\_ALL}} and \emph{\texttt{VERIFY\_SUSPECT}} \citep{JGroups}.  
    
    \texttt{FD\_SOCK} is the lowest of the three protocols in the stack, and it utilises a \textquoteleft{}ring' of TCP sockets that is established between each node in the current view to detect if one or more nodes become inoperative \footnote{All of the experiments detailed in this thesis utilise UDP packets for sending unicasts, however the TCP sockets are still open as part of the \texttt{FD\_SOCK} protocol and are present purely for failure detection.}.  If a node's TCP socket is abruptly closed, then \texttt{FD\_SOCK} suspects that the node has crashed and issues a \texttt{SUSPECT} message.  Conversely, if a node wishes to leave the view gracefully, i.e it has not crashed, then a leaving message is sent around the ring of TCP sockets before the node closes its socket.    This leaving message is sent when the JGroups shutdown hook is activated during the normal shutdown process of a Java program (calling System.exit() or requesting the process is terminated at the OS level).  
    
    \texttt{FD\_ALL} is a failure detector protocol that utilises a simple heartbeat protocol \citep{AW98} to issue \texttt{SUSPECT} messages.  Each node periodically sends a heartbeat message to all other nodes in the current view, and suspects another member of crashing if a heartbeat message has not been received after a specified timeout.  By default, \texttt{FD\_ALL} utilises a timeout value equal to $40$ seconds with each heartbeat message sent every $8$ seconds.  
    
    Finally, the highest failure detection protocol in the stack, is the \texttt{VERIFY\_SUSPECT} protocol.  This protocol aims to reduce the chances of a node being falsely suspected of crashing by intercepting \texttt{SUSPECT} messages, sent from lower in the stack, and attempting to contact the suspected node for a final time.  If no response is received within $1.5$ seconds, then the \texttt{SUSPECT} message is sent upto the GMS protocol and the node will be excluded from the current view.  Otherwise, the original \texttt{SUSPECT} message is discarded as we know that the suspected node must be alive if it is able to respond to this protocol.  
    
    The three protocols described above, when used as a triple, provide an effective method for detecting node crashes, with initial experiments showing that the \texttt{FD\_SOCK} protocol was particular effective at detecting crashed nodes as it does not rely on large timeout values.  Therefore, in order for \textsf{ABcast} to deliver messages before GMS detected a crash, the calculated $\Delta_m$ would need to remain relatively small ($\approx < 2$ $seconds$) throughout the experiment.  Furthermore, because of \texttt{FD\_SOCK}'s use of Java shutdown hooks, it was not possible for the crashed node in our experiments to be exited in the normal way as this would result in the node sending a leaving message to all members in the view and alerting GMS almost instantly that the node was leaving the current view.  Obviously it would not be possible for such a leaving message to be sent if a node actually crashed.  Therefore, it was necessary for us to terminate the JVM in the most disruptive manner possible, in order to replicate the unexpected nature of a real node crash.  We achieved this by using reflection to access the \emph{sun.misc.Unsafe} api and crash the JVM.  The code used to crash the JVM is shown below:
    
    \begin{lstlisting}
            Field theUnsafe = Unsafe.class.getDeclaredField("theUnsafe");
            theUnsafe.setAccessible(true);
            ((Unsafe) theUnsafe.get(null)).getByte(0);
    \end{lstlisting}

    As previously stated, we crash the node $N_3$ after it has initiated $50000$ \emph{abcast} requests.  Therefore, we consider each experiment to be complete when both $N_1$ and $N_2$ have delivered $(666666 + 50000 - \#rejection)$ \emph{abcast}s.  For all of our experiments, we utilise the following constant values $\delta_{min} = 1ms$, $\delta_{max} = 10ms$ and $Ss = 10$.  However, we execute our experiments utilising $\rho_{min} = 1,2,3$ and $R=0.9999$ in order to determine the effect of increasing $\Delta_m$ on the number of messages delivered before GM detects a crash.  As well as their impact on \textsf{Aramis}'s likelihood of rejecting a message.  Similarly, we also execute our experiments utilising $\rho_{min} = 1$ and $R=0.99999$, to see the effect of increasing $R$ on $\Delta_m$ and \textsf{Aramis}'s reliability.  
    
    \subsection{Results}
    Tables \ref{table:crashed_node_rho1}, \ref{table:crashed_node_rho2} and \ref{table:crashed_node_rho3} show the performance of the \textsf{ABcast} protocol in the experiments described in \ref{ssec:crash_experiment}, when $\rho_{min}$ is equal to $1, 2$ and $3$, respectively.  With each table showing the results of ten experiments that were executed with the specified $\rho_{min}$ value.  Each of these tables, show the average $\Delta_m$ value calculated for messages originating at both $N_1$ and $N_2$, as well as the total number of \emph{abcast}s, $\#abcast$ delivered by \textsf{Aramis} in the interim period between node $N_3$ crashing and GMS detecting the crash \footnote{If a column contains $-$ it indicates that no \textsf{Aramis} deliveries occurred before GMS detected $N_3$'s crash.}.  Furthermore, the value in brackets next to this total represents the number of \emph{abcast}s rejected by \textsf{Aramis} in the interim period.  Ultimately, $\#abcast$ shows the throughput gain provided by utilising the probabilistic \textsf{Aramis} protocol, as these \emph{abcast}s would not have been delivered until GMS detected $N_3$'s crash if a GM based protocol had been used for \emph{abcast}ing.   

    Similarly, Table \ref{table:crashed_node_R.99999} shows the performance of \textsf{ABcast} in the same experiments, but with $\rho_{min} = 1$ and the constant $R = 0.99999$.  The fields and columns presented in this table are equivalent to those describe above.  
    
    \begin{table}[h]
  \begin{center}
  \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{$\rho_{min}$} & \multirow{2}{*}{$R$}    & \multirow{2}{1.75cm}{$\#$ of Fail Free Runs} & \multicolumn{2}{|c|}{Failure / Delivery} \\ \cline{4-5}
                                 &              &                   & $N_1$   & $N_2$             \\ \hline \hline
    3                           & 0.9999  & $10/10$    & $-$        & $-$                 \\ \hline
    2                           & 0.9999  & $9/10$      & $-$        & $\frac{1}{19485}$ \\ \hline
    \multirow{2}{*}{1} & \multirow{2}{*}{0.9999} & \multirow{2}{*}{$8/10$} & $-$ & $\frac{1}{12483}$ \\ 
                                 &              &                   & $-$        & $\frac{1}{10544}$ \\ \hline
    1                           &0.99999 & $9/10$      & $-$        & $\frac{1}{17016}$ \\ \hline
    \end{tabular}
    \caption{Summary of $\rho_{min}$ and $R$ when Node Crashes Occur}
    \label{table:crashed_node_summary}
  \end{center}
\end{table}    
    
    Table \ref{table:crashed_node_summary} provides a summary of all of these previous tables, with each set of experiments represented as a single row and being uniquely identified by the combination of $R$ and $\rho_{min}$ values used in the experiments.  For each experiment, we show the number of experiments that encountered no \textsf{Aramis} failures as a ratio of all experiments, and in experiments where failures did occur we present the number of failures over the number of successful \textsf{Aramis} deliveries that occurred before GMS detected $N_3$'s crash\footnote{In Table \ref{table:crashed_node_summary}, $-$ indicates that not \textsf{Aramis} failures occurred}.  
     
\begin{table}[p]
    \begin{center}
        \renewcommand{\arraystretch}{1.25}
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            \multirow{2}{*}{Experiment} & \multicolumn{2}{|c|}{$N_1$} & \multicolumn{2}{|c|}{$N_2$} \\ \cline{2-5}
                                                       & $\Delta_m$&\textsf{Aramis} & $\Delta_m$&\textsf{Aramis} \\ \hline \hline
            1 & 240 & 10544, (0) & 212 & 10544, (1) \\ \hline
            2 & 553 & 6874, (0) & 527 & 6874, (0) \\ \hline
            3 & 517 & 17452, (0) & 402 & 17452, (0) \\ \hline
            4 & 334 & 18487, (0) & 274 & 18483, (0) \\ \hline
            5 & 426 & 12483, (0) & 322 & 12483, (1) \\ \hline
            6 & 717 & 4723, (0) & 429 & 4723, (0) \\ \hline
            7 & 491 & 8936, (0) & 816 & 8936, (0) \\ \hline
            8 & 510 & 393, (0) & 475 & 392, (0) \\ \hline
            9 & 478 & 3798, (0) & 931 & 3798, (0) \\ \hline
            10 & 234 & 17341, (0) & 290 & 17805, (0) \\  \hline
        \end{tabular}
        \caption{\textsf{Aramis} deliveries (rejections) before GMS detects $N_3$ has crashed \\ $R=0.9999$, $\rho_{min}=1$}
        \label{table:crashed_node_rho1}
    \end{center}
\end{table}

\begin{table}[p]
    \begin{center}
        \renewcommand{\arraystretch}{1.25}
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            \multirow{2}{*}{Experiment} & \multicolumn{2}{|c|}{$N_1$} & \multicolumn{2}{|c|}{$N_2$} \\ \cline{2-5}
                                                       & $\Delta_m$&\textsf{Aramis} & $\Delta_m$&\textsf{Aramis} \\ \hline \hline
            1 & 664 & 5509, (0) & 580 & 5509, (0) \\ \hline
            2 & 636 & 13697, (0) & 555 & 13697, (0) \\ \hline
            3 & 1020 & 2688, (0) & 496 & 2688, (0) \\ \hline
            4 & 320 & 19481, (0) & 279 & 19485, (1) \\ \hline
            5 & 331 & 19012, (0) & 400 & 19106, (0) \\ \hline
            6 & 456 & 2669, (0) & 466 & 2669, (0) \\ \hline
            7 & 432 & 10823, (0) & 939 & 10823, (0) \\ \hline
            8 & 271 & 18412, (0) & 272 & 18414, (0) \\ \hline
            9 & 498 & 5440, (0) & 362 & 5449, (0) \\ \hline
            10 & 716 & 3611, (0) & 376 & 3611, (0) \\ \hline
        \end{tabular}
        \caption{\textsf{Aramis} deliveries (rejections) before GMS detects $N_3$ has crashed \\ $R=0.9999$, $\rho_{min}=2$}
        \label{table:crashed_node_rho2}
    \end{center}
\end{table}

\begin{table}[p]
    \begin{center}
        \renewcommand{\arraystretch}{1.25}
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            \multirow{2}{*}{Experiment} & \multicolumn{2}{|c|}{$N_1$} & \multicolumn{2}{|c|}{$N_2$} \\ \cline{2-5}
                                                       & $\Delta_m$&\textsf{Aramis} & $\Delta_m$&\textsf{Aramis} \\ \hline \hline
            1 & 452 & 17651, (0) & 451 & 21064, (0) \\ \hline
            2 & 475 & $-$ & 679 & - \\ \hline
            3 & 754 & 3911, (0) & 515 & 3911, (0)  \\ \hline
            4 & 355 & 16516, (0) & 515 & 3911, (0)  \\ \hline
            5 & 214 & 17620, (0) & 503 & 17619, (0)  \\ \hline
            6 & 386 & 12968, (0) & 694 & 12968, (0)  \\ \hline
            7 & 453 & 7311, (0) & 345 & 7311, (0)  \\ \hline
            8 & 632 & 12613, (0) & 546 & 12613, (0)  \\ \hline
            9 & 356 & 18030, (0) & 569 & 18034, (0)  \\ \hline
            10 & 695 & 13907, (0) & 511 & 13907, (0)  \\ \hline
        \end{tabular}
        \caption{\textsf{Aramis} deliveries (rejections) before GMS detects $N_3$ has crashed \\ $R=0.9999$, $\rho_{min}=3$}
        \label{table:crashed_node_rho3}
    \end{center}
\end{table}

\begin{table}[p]
    \begin{center}
        \renewcommand{\arraystretch}{1.25}
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            \multirow{2}{*}{Experiment} & \multicolumn{2}{|c|}{$N_1$} & \multicolumn{2}{|c|}{$N_2$} \\ \cline{2-5}
                                                       & $\Delta_m$&\textsf{Aramis} & $\Delta_m$&\textsf{Aramis} \\ \hline \hline
            1 & 387 & 17982, (0) & 453 & 17016, (1)  \\ \hline
            2 & 7507 & 255, (0) & 3804 & 742, (0)  \\ \hline
            3 & 2019 & 8117, (0) & 1676 & 8117, (0)  \\ \hline
            4 & 3094 & - & 1899 & -  \\ \hline
            5 & 264 & 10876, (0) & 416 & 10880, (0)  \\ \hline
            6 & 683 & 9262, (0) & 605 & 9262, (0)  \\ \hline
            7 & 244 & 18224, (0) & 301 & 18222, (0)  \\ \hline
            8 & 1160 & 2207, (0) & 830 & 2207, (0)  \\ \hline
            9 & 334 & 19058, (0) & 278 & 19060, (0)  \\ \hline
            10 & 233 & 17588, (0) & 421 & 17586, (0)  \\ \hline
        \end{tabular}
        \caption{\textsf{Aramis} deliveries (rejections) before GMS detects $N_3$ has crashed \\ $R=0.99999$, $\rho_{min}=1$}
        \label{table:crashed_node_R.99999}
    \end{center}
\end{table}

    \subsection{Evaluation}
    From Tables \ref{table:crashed_node_rho1}, \ref{table:crashed_node_rho2},  \ref{table:crashed_node_rho3} and \ref{table:crashed_node_R.99999} we can clearly see that the \textsf{ABcast} protocol allows for a large number of \emph{abcast}s to be delivered in the interim period between a node crashing and the GMS protocol detecting it.  With an individual node delivering, on average, greater than $10^4$ \emph{abcast}s and in one case more than double that amount.  Furthermore, out of $40$ experiments there was only two instances when there was no benefit to using the \textsf{ABcast} protocol, and this was when the protocol utilised more conservative constants values of $R=0.99999$ and $\rho_{min}=3$ respectively.  
    
    In \ref{table:crashed_node_summary}, we can clearly see that increasing the size of $\rho_{min}$ has a direct impact on the reliability of \textsf{Aramis}.  Because the number of \emph{abcast} rejections reaches zero when $\rho_{min}$ is at its largest.  This can be explained by a larger $\rho_{min}$, increasing the calculated $\Delta_m$ value for each \emph{abcast} (as seen in Tables \ref{table:crashed_node_rho1}, \ref{table:crashed_node_rho2},  \ref{table:crashed_node_rho3}) \footnote{The difference in calculated $\Delta_m$ values is not significant between $\rho_{min}=1,2,3$ in our results, however this can be attributed to the varying state of the underlying network.  Our experiments were conducted in sets based upon their constant values, e.g. all ten experiments that utilised $\rho_{min}=1$ and $R=0.9999$ were performed one after the other.  Therefore, as these experiments take several minutes each, the time required to conduct all of the experiments was significant, and as a consequence these experiments were conducted over several days.  Consequently, the load on the underlying network will have varied for each set of experiments.  However, we can still attribute the reduced number of \textsf{Aramis rejections} to an increase in $\Delta_m$, as this variable is calculated based upon latencies that represent the networks current state.  Therefore, if a smaller $\rho_{min}$ value was utilised under the exact same network conditions as the $\rho_{min}=3$ experiments, we know that the calculated $\Delta_m$ value would have been significantly smaller.}.  Conversely, when we increase $R$ from $0.9999$ to $0.99999$, the number of rejections is reduced to one, at the expense of a greatly enlarged $\Delta_m$ (compared to $\rho_{min}=1,2,3$ when $R=0.9999$).  One would expect that this increase in $\Delta_m$ would result in the number of rejections being zero, as is the case in $\rho_{min}=3$ where the average $\Delta_m$ value is much smaller.  However, this is not the case as increasing $R$ not only enlarges $\Delta_m$, but it also inflates $\eta$.  Consequently, the delay between redundant broadcasts of message copies and the maximum $\mathcal{A}_d$ value is also increased.  The increased delay between message broadcasts means that it will take longer for $m.copy > 0$ to reach each destination, which if the original packets of $m.copy = 0$ have been lost, then it is more likely that a copy $m$ will arrive at a destination after $\Delta_m$.  More significant is the increased  $\mathcal{A}_d$ value, as this means that if no \emph{abcast}s are ready to send, then a node will wait longer to send an explicit acknowledgement message.  Resulting in the time between a node being ready to acknowledge a message $m$, and another node receiving the acknowledgement for $m$, increasing, which increases the chances of a subsequent message $m'$ being delivered ahead of $m$ in the total order. Hence, it is more likely for the message $m$ to be rejected by \textsf{Aramis}, which could have been a significant factor in why one of our experiments rejected a message when $R=0.99999$.  
    
    Finally, while our experiments show that a larger number of \emph{abcast}s are delivered in the interim period between node failures and detection, we believe that in the event of a \textquoteleft{}real} crash this value could be much higher.  In our experiments we crash the JVM instantly, which results in the TCP sockets utilised by the \texttt{FD_SOCK} protocol being closed immediately.  This means that it is almost certainly the \texttt{FD_SOCK} protocol that detects the failure of $N_3$ each time.  If a crash was preceeded by a slowing down period where node responses become more staggered and the node was unresponsive, but still running and maintaining an open TCP socket, it is highly probable that the total number of \emph{abcast}s sent in the interim period would be much larger, as the alternative failure detection protocol \texttt{FD_ALL} has a default timeout period of $40$ seconds.  
        
    \subsection{Summary}
        We have found that utilising the \textsf{ABcast} protocol for \emph{abcast}s, allows for a significant number of messages ($> 10^4$) to be delivered in the interim period between a node crash and a GM protocol detecting it.  Furthermore, we have found that increasing both $\rho_{min}$ and $R$ reduces the chances of \textsf{Aramis} messages being rejected when node crashes occur.  However, increasing $R$ can potentially undermine the \textsf{ABcast} protocol whilst creating a large $\Delta_m$.  Therefore, if a more conservative \textsf{ABcast} protocol is required, it is more effective to increase $\rho_{min}$ then $R$.  
        
\section{Summary}