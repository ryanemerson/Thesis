\chapter{Probabilistic SCast}

% **************************** Define Graphics Path **************************
    \graphicspath{{Chapter5-PSCast/Figs/Vector/}{Chapter5-PSCast/Figs/}}

This chapter explores the consequences of utilising the \textsf{ABcast} protocol for state machine replication between $s$-nodes in the \textsf{AmaaS} model.  Throughout our explanations we assume that the \textsf{SCast} protocol and its system model are utilised for all interactions between $c$-nodes and the ordering service.  We refer to this approach as Probabilistic \textsf{SCast}, or \textsf{PSCast} for short. 

The remainder of this chapter is structured as follows: First we present the new \emph{amcast} guarantees provided by \textsf{PSCast}.  We then explore the ramifications of these new guarantees and discuss how they can cause \emph{amcast} messages to be delivered out of the total order at one or more destinations.  Finally, we describe the consequences of such missorderings, within the context of Infinispan, and explore potential solutions that can be employed by Infinispan to tolerate these missorderings.  

\newpage
\section{PSCast Guarantees}
Below, we state the \emph{amcast} guarantees provided by \textsf{PSCast}.  
   
    \begin{description}
        \item [\textbf{G1}] - \emph{Validity}: If the source of $m_i$ does not crash until it \emph{abcast}s $m_i$, then all operative destinations of $m_i$ deliver $m_i$.
       
        \item [\textbf{G2}] - \emph{Uniform Agreement}: If the source of $m_i$ crashes while \emph{abcast}ing $m_i$, and if any destination delivers $m_i$, then all operative
destinations of $m_i$ must deliver $m_i$.
        
        \item [\textbf{G3}] - \emph{Uniform Integrity}: If $m_i$ has already been delivered by a destination $d$, then $d$ cannot deliver $m_i$ again.  
       
        \item [\textbf{G4-PSCast}] - \emph{Probabilistic Total Order}: If two \emph{\emph{amcast}s}, $m_i$ and $m_j$, have common destinations, then all such destinations that deliver both $m_i$ and $m_j$, will deliver them in an identical order with a probability $> R$.  Typically $R \rightarrow 1$.
\end{description}

\textbf{Note: } The guarantees of \textsf{PSCast} are identical to those of the underlying \emph{abcast} protocol, \textsf{ABcast}.  This is because the guarantees of the \emph{abcast} protocol, and any violations of these guarantees, directly impacts how each $s$-node maintains its $order\_history[]$ and generates $m.history[]$; with $m.history[]$ dictating the order in which a $c$-node must deliver \emph{amcast}s ($\S$ \ref{sec:scast_protocol}).  

\section{G4-PSCast Violations}
Recall that the \textsf{SCast} protocol utilises the final timestamp of the \emph{abcast} message, which contains a clients ordering request, to determine the total order of \emph{amcast} associated with the request.  

The \textsf{SCast} protocol, described in chapter \ref{ch:amaas}, assumes that the underlying \emph{abcast} protocol provides deterministic guarantees for all G1-G4 ($\S$ \ref{sec:atomic_guarantees}).  Hence, the \textsf{SCast} protocol assumes that all $s$-nodes will eventually deliver an \emph{abcast} and that all \emph{abcast}s are delivered in the same order at each destination.  This assumption means that the \textsf{SCast} protocol is able to guarantee that all \emph{amcast}s sent via the ordering service will respect G4 as it is guaranteed that all $s$-nodes will always maintain a consistent $order\_history[]$.  Therefore, all ordering responses, $rsp(Tx)$, from the service will contain the correct $m.history[]$ data.  

Conversely, the \textsf{PSCast} utilises a probabilistic \emph{abcast} protocol, \textsf{ABcast}, that only guarantees G4 with probability $R$ (G4-P).  Therefore, for an \emph{abcast} $m$ sent between $s$-nodes, there is a small probability ($1-R$) that a destination, $N_s$, will not receive or \emph{know} of $m$ after $\Delta_m$ time.  In this case, it is possible for another \emph{abcast}, $m'$, $m.ts < m'.ts$, sent from a different $s$-node, $m.o \neq m'.o$, to have been delivered at $N_s$ when $\Delta_m'$ expired.  Resulting in a violation of G4-P, as $m'$ has preceded $m$, which causes $N_s$ to \emph{reject} $m$ from the total order and deliver it to the application (\textsf{SCast}), via an exception, when it eventually arrives.  

This G4-P violation, and hence the \emph{late} delivery of $m$, can be abstracted as follows: For every message, $m$, that is about to be added to the $ordering\_history[]$, a random value, $RV$, is assigned, which is uniformly distributed between $0$ and $1$.  $RV$ is then compared with the probability that G4-P will hold, $R$, before $m$ is added to the $ordering\_history[]$. Two cases are then possible:

    \begin{description}
        \item[$\bm{RV \leq R}$]\hfill \\
        This represents that G4-P was met and therefore $m$ is processed the same as in the deterministic \textsf{SCast} protocol.  Hence, $m$ is added to a node's $order\_history[]$ instantly as described in \textsf{SCast}.  
        
        \item[$\bm{RV > R}$]\hfill \\
        This represents that G4-P was not met, as G4-P is guaranteed with probability $R$.  In this case, $m$ is not inserted in $order\_history[]$ before $x$ time; with $x$ being a random time in the future, which represents the unknown time at which $m$ would be delivered via an exception at the node that did not maintain G4-P.  Consequently, subsequent \emph{abcast} messages, $m'$, $m'.ts > m.ts$, delivered by this node before $x$, are inserted into $order\_history[]$ ahead of $m$.  
    \end{description}
    
    By the way of comparison, we now explore the consequences of a message being assigned $RV > R$, with respect to the $order\_history[]$ and $m.history[]$ generated by an $s$-node.  Consider a scenario where an $s$-node receives four \emph{abcast}s, $\{m_1, m_2, m_3, m_4\}$; where each $m$ corresponds to a transaction with the same value, \emph{e.g.} $m_1 \rightarrow Tx_1$, and $m_1.dst = \{C_1, C_2, C_3\}; m_2.dst = \{C_1, C_3\}; m_3.dst = \{C_2, C_3\}; m_4 = \{C_2, C_3\}$.  Furthermore, each $mcast(Tx_1)$ message sent as per the \textsf{SCast} protocol, is denoted as $mc_1$ for short, \emph{e.g.} $mc_1$ is associated with the same \emph{amcast} as $Tx_1$ and $m_1$.  
    
    If all four messages are allocated an $RV$ value, $RV \leq R$, then all messages are inserted into the $order\_history[]$ in the correct order.  Therefore, a correct $m_4.history[]$ is produced by this node; the resulting $order\_history[]$ and $m_4.history[]$ generated upon delivery of $m_4$ are shown in Figure \ref{fig:correct_ohistory_pscast}.  
    
    Alternatively, consider that a single message, $m_3$, is allocated a value $RV > R$.  This causes $m_3$ to be delayed for $x$ time.  Assuming that $m_4$ is delivered before $x$ time, $m_4$ will take $m_3$'s place in the total order, resulting in an erroneous $order\_history[]$ and $m_4.history[]$.  This is shown in Figure \ref{fig:incorrect_ohistory_pscast}.  
    
    \begin{figure}[h] 
        \centering
         \makebox[\textwidth][c]{\includegraphics[width=1.25\textwidth]{correct_ohistory}}
         \caption[Order History with deterministic ordering (G4)]{$order\_history[]$ with deterministic ordering (G4)}
         \label{fig:correct_ohistory_pscast}
    \end{figure}    

    \begin{figure}[h] 
        \centering
         \makebox[\textwidth][c]{\includegraphics[width=1.25\textwidth]{wrong_ohistory}}
         \caption[Order History with non-deterministic ordering (G4-P is not met)]{$order\_history[]$ with non-deterministic ordering (G4-P is not met)}
         \label{fig:incorrect_ohistory_pscast}
    \end{figure}  
    
    The consequences of the incorrect $m_4.history[]$, as shown in Figure \ref{fig:incorrect_ohistory_pscast}, are that it is now possible for the $c$-node, $N_c2$, to miss $mc_3$ in its total order, even though ${mc_3 \ \llcurly_{N_c2} \ mc_4}$.  In order for such a miss-ordering to occur, both of the following must be true:
    
    \begin{enumerate}[label=\roman*]
        \item $N_c2$ \textbf{\emph{receives}} $mc_4$ before $mc_3$.     
        
        \item $N_c2$ \textbf{\emph{delivers}} the message stated in $mc_4.history[N_c2]$ before receiving $mc_3$.  
    \end{enumerate}
    
    If condition $(i)$ is true, then $N_c2$ will not know of $mc_3$ as it is not stated in $mc_4.history[]$ and if $(ii)$ is also true, then $mc_4$ will be delivered immediately.  Note, that if $N_c2$ was to receive $mc_3$, before delivering $mc_4.history[Nc_2]$, then $mc_3$ will be \emph{known} by $Nc_2$ and the correct delivery order of ${mc_1 \ \llcurly_{Nc_2} \ mc_3 \ \llcurly_{Nc_2} \ mc_4}$ will be preserved.  

    \textbf{Note:} The probability of a G4-PSCast violation is the product of two probabilities; the probability that a G4-P violation occurs, \emph{i.e.} $R$; and the probability that both conditions $(i)$ and $(ii)$ are met.  Therefore, in practice the probability of a G4-PScast violation occurring is much smaller than $(1 - R)$.  

\section{Service Node - Mitigating G4-P Violations}
% Updating history. Same as random wait.

\section{Infinispan (Client Node) - Mitigating G4-PSCast Violations}
% Transaction manager, state all assumptions - exceptions etc.

    \subsection{Repeatable Read and Read Committed}
    
    \subsection{Repeatable Read with WSC}

\section{Summary}


\newpage
\section{Handling ABcast Rejections}
 Unlike traditional \emph{abcast} protocols, such as TOA, it is possible for \textsf{ABcast} to reject messages, thereby preventing them from being delivered to the application outside of the total order ($\S$ \ref{ssec:aramis}).  Performance evaluation in chapter \ref{ch:perf_eval} shows that the likelihood of rejections occurring is very small even when the number of \emph{abcast}s is large, however as such events are possible it is necessary for applications to implement additional logic to accommodate such occurrences.  
 
Consider the \textsf{SCast} protocol defined in $\S$ \ref{sec:scast_protocol}.  The current protocol assumes that no message misorderings can occur and that \emph{abcast} guarantees G1-G4 always remain true.  When \textsf{ABcast} is utilised for \emph{abcast}ing client requests between two $s$-nodes, $\{N_i, N_j\}$, it is possible for an \emph{abcast} message, $m_i$, sent by $N_i$ to be rejected by $N_j$, resulting in the predecessor data stored at $N_j$ being inconsistent with $N_i$'s data.  This inconsistency within the \textsf{AmaaS} service, can lead to conflicts between data replicas at the client level.  For example, Imagine that two clients, $C_1$ and $C_2$ have each started a transaction, $Tx_i$ and $Tx_j$, both of which have the same destination set $dst=\{C_1, C_2\}$.  Client $C_1$ issues a \emph{amcast} request to $N_i$ and $C_2$ does likewise to $N_j$, therefore $N_i$ \emph{abcast}s $Tx_i$ as $m_i$ and $N_j$ \emph{abcast}s $Tx_j$ as $m_j$.  If both \emph{abcast}s are successfully delivered by service nodes, in the order $<m_i, m_j>$, then the predecessor data attached to the response message, $rsp(Tx)$, for $Tx_i$ will be the id of the preceding transactions $Tx_h$, and for $Tx_j$'s response message it will be $Tx_i$.  Thus, if clients $C_1$ or $C_2$ receive $rsp(Tx_j)$ before $rsp(Tx_i)$, then the \textsf{SCast} protocol at each client is able to deduce that $Tx_j$ cannot be delivered until $Tx_i$ has been delivered based upon $Tx_i$ being specified in $Tx_j$'s predecessor data.  

Now consider a scenario where $m_i$ is rejected by $N_j$.  This would result in the predecessor data stored at nodes $N_i$ and $N_j$ being inconsistent; as $N_j$ has not delivered $m_i$, therefore $N_j$'s predecessor data will not include $m_i$.  Thus, when $N_j$ sends the response message of $Tx_j$, $rsp(Tx_j)$ to the clients, $Tx_j$'s predecessor data will state that $Tx_h$ preceded $Tx_j$ not $Tx_i$. Consequently, clients $C_2$ or $C_1$ can now deliver $Tx_j$ to their transaction manager before $Tx_i$, which could result in the two clients processing $Tx_i$ and $Tx_j$ in different orders, resulting in the clients' replicas being inconsistent.  We refer to an instance where a client delivers a message ahead of its place in the total order, for example $Tx_j$ being passed to the transaction manager before $Tx_i$, as an \emph{amcast} violation, as it is the guarantees of the \emph{amcast} protocol \textsf{SCast} that have been violated.  

Violations of the \emph{amcast} ordering, as described above, can only occur if the service's response message, $rsp(Tx_j)$, containing the incomplete predecessor data, is received by a client node before the response message containing the correct data, $rsp(Tx_i)$.  If $rsp(Tx_i)$ is received first by a client, $C_2$, then $Tx_i$ will be delivered to the application \footnote{Assuming $rsp(Tx_h)$ has already been received and $Tx_h$ delivered.}.  When $rsp(Tx_j)$ is received, $C_2$ is able to deduce that the predecessor of $Tx_j$ specified in $rsp(Tx_j)$, $Tx_h$, has already been delivered and that another transaction, $Tx_i$, exists with a smaller timestamp than $Tx_j$, therefore $Tx_j$ must come after $Tx_i$ in the total order and $Tx_j$ can safely be delivered. Conversely, if $rsp(Tx_j)$ is received first by $C_2$ it will delivered to the application as $Tx_h$ has already been delivered and $Tx_j$ is unaware of $Tx_i$.  However, when $C_2$ eventually receives $rsp(Tx_i)$, it becomes aware that $Tx_i$'s specified predecessor was not the last message to be delivered and that $Tx_i$ should have preceded the last delivered message $Tx_j$.  In such a scenario It is not possible for $C_2$ to undeliver $Tx_j$, however $C_2$ can throw an exception containing the missed transaction, $Tx_i$, in order to alert the transaction manager that a \emph{amcast} violation has occurred.  Therefore allowing steps to be taken to mitigate the consequences of an \emph{amcast} violation.  

In the previous paragraphs we have presented the conditions required for a \emph{amcast} violation to occur when utilising the \textsf{SCast} protocol.  We now present a solution for handling rejections between $s$-nodes, and for handling \emph{amcast} violations that occur as a result of these rejections.  In our explanations, all client nodes are assumed to be running the Infinispan application.  As in the previous examples, we consider a service consisting of only two nodes for simplicity and these nodes are called $N_i$ and $N_j$.  Furthermore, we assume that the $Tx.dst$ sets of all transactions have overlapping nodes, if no overlaps existed, then it would not be possible for a \emph{amcast} violation to occur.  More specifically, we assume that $Tx.dst = {C_1,C_2}$ for all transactions, unless stated otherwise.  

    \subsection{SCast - Handling Rejections in a Ordering Service}
    Assume that $s$-nodes, $N_i$ and $N_j$ \emph{abcast} messages $m_i$ and $m_j$ respectively, and the total order is $<m_i, m_j>$.  If $N_j$ rejects $m_i$ because it has delivered $m_j$ in its place, then $N_j$'s predecessor data would record $m_j$'s id as the last transaction that interacted with clients $C_1$ and $C_2$.  By definition $N_i$ is guaranteed to deliver its own message $m_i$.  Now assume that $N_i$ also delivers $m_j$, in this case $N_i$'s predecessor data will also state that the last transaction involving clients $C_1$ and $C_2$ was $m_j.id$.  Therefore, we state that when a message $m$ is rejected by \textsf{ABcast} at a node $N$, no updates are required to $N$'s stored predecessor data if the existing transaction $id$ associated with a client is greater than the $m.id$ \footnote{This rule is applicable to all clients in $N$'s predecessor data, however in reality we only need to check the $id$'s associated with clients specified in the destination set of the transaction contained within the rejected $m$}.  

    Alternatively, consider a scenario where $m_i$ contains a transaction $Tx_i$ with $Tx.dst = {C_1, C_2}$ and $m_j$ contains a transaction $Tx_j$ with $Tx.dst = {C_2, C_3}$.  $N_i$ delivers $m_i$ and $m_j$, but $N_j$ rejects $m_i$, with the correct total order of messages being $<m_i, m_j>$.  In such a scenario, $N_i$'s predecessor data would associate $Tx_i$ with $C_1$, and $Tx_j$ with $C_2$ and $C_3$.  Whereas, $N_j$ would associate $C_2$ and $C_3$ with $Tx_j$, but $C_1$'s value would be null or the previously associated transaction.  Therefore, when $N_j$ is made aware of $m_i$ via the rejection exception, it is necessary for its predecessor data to be updated so that $C_1$ is associated with $Tx_i$, as the previous transactions timestamp will be less than $Tx_i.ts$.  If $N$'s predecessor is not updated, then it is possible for the predecessor data associated with a future client response to be incorrect as $m_i.id$ would be missing from $N$'s records.  Hence, it would be possible for another \emph{amcast} violation to occur if the predecessor data was not updated in this way.  
    
    \subsection{Infinispan - Recovering from \emph{amcast} Violations}
    In the event that one or more \emph{amcast} violations occur, it is necessary for the client application to be able to recover from such events.  In this subsection, we present a recovery mechanism that can be employed by the Infinispan transaction manager at each client node in the event of a violation occurring.  The exact methods of the recovery mechanism are determined by the isolation level chosen by the Infinispan clients before runtime; with one method required for \emph{Repeatable Read}, $RR$ and \emph{Read Committed}, $RC$ transactions, whilst an alternative method is required for $RR$ with WSC due to its dependence on a voting phase to commit/abort transactions.  Therefore, we first explored the method required for $RR$ and $RC$ before detailing the provisions for $RR$ with WSC.  
    
    \subsubsection*{Repeatable Read and Read Committed}
    When $RR$ or $RC$ isolation is utilised the outcome of a transaction is determined in a single phase, as each $d \in Tx.dst$ decides deterministically whether to commit or abort a transaction without requiring any additional communication between other destinations.  Consequently, when a transaction manager receives a $prepare(Tx)$ message via \emph{amcast} it is able to commit the transaction immediately.  However, if an \emph{amcast} violation has occurred and a client $C$ has received an \emph{amcast} message, $m_j$, the transaction manager will be unaware that a preceding transaction sent in $m_i$ has been missed and $m_j$ will be committed.  Therefore, when the \emph{amcast violation} is discovered the only course of action is for a roll-back procedure, similar to that utilised in compensating transactions \citep{Korth:1990:FAR:645916.671971}, to be initiated, with all operations that should have occurred after $m_i$'s transaction being reversed.  Once these operations have been reversed it is possible for $m_i$'s transaction to be executed and the previously rolled back transactions reapplied in the correct total order.  For example, if the original value of a key $k$ was $v=5$, the transaction associated with $m_i$ was $put(k, 10)$ and $m_j$'s transaction was $put(k, v+1)$.  When $m_i$ is missing, the outcome of $m_j$'s transaction would be $v = 6$, however after $m_i$ is discovered and the transactions are reapplied, the result of $m_j$ will be $v = 11$.  A consequence of this approach is that \textquoteleft{}stale' reads can occur as it is possible for an Infinispan application, or subsequent transaction, to read $v = 6$ when requesting the value of $k$, ultimately leading to \emph{write-skew}.  We consider this an acceptable risk for two reasons: firstly, the window of opportunity for such an occurrence is very small \footnote{We anticipate that the time between an \emph{amcast} violation occurring and being detected to be in the order of milliseconds.  Furthermore, \emph{amcast} violations are only possible when \textsf{ABcast} rejects a message and a client node receives the message with inconsistent predecessor data first.  Therefore, the probability of a \emph{write-skew} occurring is the product of several small probabilities.}.  Secondly, the Infinispan store is already susceptible to \emph{write-skew}s when the WSC is not utilised, therefore the business logic of the application utilising Infinispan should already be tolerant of such phenomena.  Ultimately, this solution results in the state of all $d \in Tx.dst$ eventually becoming consistent if one or more \emph{amcast} violations occur at any $d$.  
    
    \subsubsection*{Repeatable Read with WSC}
    When utilising transactions with $RR$ and WSC it is not possible to utilise the previous solution, as the outcome of each transaction affects the second voting phase that is required to avoid \emph{write-skews}.  However, the additional voting stage required by the WSC can be used to our advantage to prevent \emph{amcast} violations from affecting the consistency of Infinispan's key/values.  Recall that the WSC requires that a transaction coordinator, $Tx.c$, receives at least one commit vote for each of the distinct keys involved in a $Tx$ in order for it to be able to send a $commit(Tx)$ decision to all $Tx.dst$.  Whereas, $Tx.c$ only requires a single abort vote from any of $Tx.dst$ members in order to disseminate an $abort(Tx)$ decision.  Furthermore, for every key stored in the infinispan system there exists a single \emph{primary} replica and at least one \emph{backup} replica.  The solutions presented in this section require a slight modification to this existing WSC behaviour.  Our solutions require that: 

\begin{quotation}    
    For all transactions, votes to commit or abort a transaction can only be sent by the transaction manager of the client node which contains the \emph{primary} replica of a key.  
\end{quotation}

     In the previous section, our solution was applicable to all transactions executing with $RR$ or $RC$ isolation, however the behaviour of our WSC solution is determined by the number $d \in Tx.dst$ that suffer from \emph{amcast} violations, the order in which messages are delivered to the transaction manager and the operations involved in the conflicting transactions.  Consider the following scenario: two nodes, $C_1$ and $C_2$ are participating in transactions $Tx_i$ and $Tx_j$ which are associated with \emph{amcast}s $m_i$ and $m_j$ respectively, with the destination set of both transactions being equal to $Tx.dst = \{C_1, C_2, C_3\}$ with $C_3$ being the transaction coordinator for both transactions.  Client $C_1$ hosts the \emph{primary} replica for all keys \footnote{Unless stated otherwise.} and each transaction consists of the following operations:
    
    \begin{lstlisting}
		    	Tx.begin();
		    	    put(k1, x);
		    	    put(k2, y);
		    	Tx.commit();
    \end{lstlisting}
    
     In the above scenario \emph{amcast} violations can occur in three different ways:
    
    \begin{enumerate}
        \item All nodes involved in a transaction suffer an \emph{amcast} violation, with $m_j$ being delivered before $m_i$ at all nodes.      
        
        \item At least one node storing a backup replica of a key suffers an \emph{amcast} violation, but the \emph{primary} replica does not.  
        
        \item The node containing the \emph{primary} replica for a key suffers an \emph{amcast} violation.  
        
        \item A node containing a \emph{primary} replica of a key, suffers an \emph{amcast} violation, however another \emph{primary} replica involved in a transaction does not.
    \end{enumerate}
    
    The recovery mechanism for all three of these scenarios are detailed below.
    
    \begin{description}
         \item[Scenario 1 - $m_i$ is missed by all nodes] \hfill \\
         For example when both $C_1$ and $C_2$'s deliver $m_j$ ahead of $m_i$.  In this case the active transaction in both nodes' transaction manager is $Tx_j$.  The workflow for such a scenario is as follows: The \emph{primary} replica of $k1$ and $k2$, unicasts a $vote(Tx_j)$ to the transaction coordinator, $Tx_j.c$, for both keys.  Upon receiving a $vote(Tx_j)$ from $C_1$, $Tx_j.c$ deduces that it has received a $vote(Tx_j)$ from the \emph{primary} replica of each key in the transaction and sends a $commit(Tx_j)$ or $abort(Tx_j)$ decision to all $Tx_j.dst$.   When a transaction manager receives a decision from $Tx_j.c$, the decision is executed locally on $Tx_j$ and a subsequent transaction is processed.  
         
         During, or after, the processing of $Tx_j$, $m_i$ will be received by the lower level \textsf{SCast} protocol and an exception thrown upto the transaction manager.  In which case, the processing of the current transaction will be suspended and the exception will be handled by the transaction manager.  When an exception is caught, the transaction manager determines whether this node hosts a \emph{primary} replica of any of the keys contained within the transaction.  If a node does not host a \emph{primary} replica, then it must wait to receive an $abort(Tx_i)$ from $Tx_i.c$ before aborting $Tx_i$ and resuming the current transaction.  However if a node contains a \emph{primary} replica, then it sends an $abort(Tx_i)$ vote to $Tx_i.c$ and aborts the transaction locally, before resuming the current transaction $Tx_j$.  A final decision will be sent by $Tx_i.c$ to all $Tx_i.dst$, however this node can ignore this message as $Tx_i$ has already been aborted locally. 
         

         \item[Scenario 2 - $m_i$ is missed by a backup replica $C_2$] \hfill \\
         For example when $C_1$ delivers $m_i$ in the correct total order, but $C_2$ delivers $m_j$, resulting in the currently active transactions for $C_1$ and $C_2$ being $Tx_i$ and $Tx_j$ respectively.  The workflow for such a scenario is as follows: $C_1$ is the \emph{primary} replica for all keys in $Tx_i$, therefore it will send a $vote(Tx_i)$ to the transaction coordinator.  $Tx_i.c$ receives this vote, realises that a vote has been received from all primaries and issues a $commit(Tx_i)$ or $abort(Tx_j)$ message to all $Tx_i.dst$.  Upon executing the local commit or abort of $Tx_i$, $C_1$ starts processing $Tx_j$.  
         
         As $C_2$'s active transaction is $Tx_j$, $C_2$ only becomes aware of the missed transaction $Tx_i$ when it receives a decision from $Tx_i.c$ or when it catches the exception thrown by \textsf{SCast} for $m_i$ \footnote{It is guaranteed that a node participating in $Tx.dst$ will eventually receive an exception from \textsf{SCast} and a final decision from $Tx_i.c$ for $Tx_i$, if $Tx_i.c$ does not crash. In the event that $Tx_i.c$ does crash, $Tx_i$ will eventually abort due to the use of transaction timeouts detailed in scenario 4}.  In the event that the \textsf{SCast} exception is caught first, the active transaction $Tx_j$ is suspended and $Tx_i$ becomes the active transaction until a decision for $Tx_i$ is received from $Tx_i.c$.  Upon receiving a decision from $Tx_i.c$, $C_2$'s transaction manager commits or aborts $Tx_i$ depending on $Tx_i.c$'s instructions.  
         
         Conversely, if the decision for $Tx_i$ is received before the \emph{SCast} exception, then one of two outcomes are possible.  If the decision is to abort $Tx_i$, then $C_2$ aborts $Tx_i$ locally and continues to execute $Tx_j$; ignoring $Tx_i$'s exception when it is caught as $Tx_i$ has already been aborted.  However, if the decision is to commit $Tx_i$, it is necessary to temporarily suspend $Tx_j$ and set $Tx_i$ as the active transaction.  The transaction manager then waits until the \textsf{SCast} exception is caught, before committing $Tx_i$.  This wait is necessary, as $C_2$'s transaction manager does not receive the actual transaction $Tx_i$ until this exception is caught, as the decision sent by $Tx_i.c$ only contains the $id$ of a transaction and the decision to commit or abort.  
         
        Finally, it is possible for $Tx_i$'s decision or exception to be received after $Tx_j$ has already been delivered if $Tx_i$ and $Tx_j$ have distinct transaction coordinators.  In the event of such an occurrence, it is necessary for $C_2$ to execute the rollback mechanism \footnote{As detailed in our $RR$ and $RC$ solutions.} in order to restore the state of the backup replicas $k_1$ and $k_2$.  For example, if $Tx_j$ has already been committed when $C_2$ receives $Tx_i$'s exception, then it is possible for $Tx_j$ to be rolled back so that the values of $k_1$ and $k_2$ are returned to their pre $Tx_j$ values, at which point $Tx_i$ and $Tx_j$ are executed sequentially so that the state of both keys is the same as at the \emph{primary} replica.  Note, utilising such a mechanism on a node hosting backup replicas does not change the outcome of the WSC on any transaction, as all decisions to commit or abort a transaction are made by the \emph{primary} replica.  
        
        
		\item[Scenario 3 - $m_i$ is missed by the \emph{primary} replica $C_1$] \hfill \\
        For example when $C_1$ suffers and \emph{amcast} violation and delivers $m_j$ before $m_i$, but $C_2$ delivers $m_i$ in the correct total order, resulting in $C_1$ and $C_2$'s active transactions being $Tx_j$ and $Tx_i$ respectively.  The workflow for such a scenario is as follows: $C_1$ is the primary for all keys in both $Tx_i$ and $Tx_j$, therefore when $Tx_j$ is $C_1$'s active transaction, $C_1$ sends a $vote(Tx_j)$ to $Tx_j.c$.  $Tx_j.c$ receives $C_1$'s vote, realises that it has received a vote from all \emph{primary} replicas and sends a decision to all $Tx_j.dst$.  Upon receiving the decision for $Tx_j$, $C_1$ starts executing subsequent transactions.  
        
        As $C_2$'s active transaction is $Tx_i$ and $C_2$ is a backup replica, $C_2$ will wait to receive a decision from $Tx_i.c$ before processing subsequent transactions.  No progress can be made by $C_2$'s transaction manager until $C_1$ discovers that it has missed $Tx_i$ in the total order.  When $C_1$ catches an exception from \textsf{SCast} containing $Tx_i$, it realises that it has missed $Tx_i$ in the total order because $Tx_i$'s timestamp is less than the previously processed transaction $Tx_j$.  Therefore, $C_1$ aborts $Tx_i$ locally and sends an abort vote to $Tx_i.c$.  Upon receiving this abort vote, $Tx_i.c$ sends an abort decision to all $Tx_i.dst$.  At which point $C_2$ will abort $Tx_i$ and start executing $Tx_j$.  
         
		\item[Scenario 4 - Deadlock between two \emph{primary} replicas when $m_i$ is missed by $C_2$] \hfill \\
		Unlike in the previous scenarios, this scenario requires that the two keys, $k_1$ and $k_2$ involved in our transactions $Tx_i$ and $Tx_j$ have distinct \emph{primary} replicas; with $C_1$ and $C_2$ being the \emph{primary} for $k_1$ and $k_2$ respectively.  Now consider that $C_1$ and $C_3$'s active transaction is $Tx_i$ as no \emph{amcast} violations have occurred at these nodes, whereas $C_2$'s is $Tx_j$ as $m_i$ was missed by the \textsf{SCast} protocol at this node.  The workflow for such a scenario is as follows: $C_1$ is the primary for $k_1$, therefore it sends a $vote(Tx_i)$ to $Tx_i.c$.  Likewise, $C_2$ is the primary for $k_2$, therefore it sends a $vote(Tx_j)$ to $Tx_j.c$.  Recall that the transaction coordinator for both $Tx_i$ and $Tx_j$ is $C_3$, hence the votes sent by both transactions are sent to $C_3$.  As $C_3$'s active transaction is $Tx_i$, $C_3$ will receive $vote(Tx_i)$ from $C_1$, however it will then wait indefinitely for a $vote(Tx_i)$ from $C_2$, as $C_2$'s active transaction is $Tx_j$.  Hence, a deadlock has occurred between the two \emph{primary} replicas of $k_1$ and $k_2$.  
		
		In order to resolve such a deadlock, we propose that all transactions should utilise an abort timeout as per the Two-phase commit protocol ($\S$ \ref{ssec:2PC}).  Therefore, in the case described above, $Tx_i$ will eventually abort as no progress can be made until $Tx_i$ times out at $C_3$.  Furthermore, it is also probable that $Tx_j$ will be aborted as the duration of both transactions will be similar due to both transactions being ordered via \textsf{SCast}'s service.  
		
		It is worth noting that, although we have reintroduced deadlock into the total order commit protocol, such occurrences are very rare.  The probability of a transaction being aborted is equal to the product of several small probabilities, with deadlock only occurring if the \textsf{ABcast} protocol rejects a message and an \emph{amcast} violation occurs which happens to result in scenario 4.  
    \end{description}

    \subsection{Summary}
        In this section we have presented several strategies that can be utilised by \textsf{SCast} nodes, both service and clients, to reduce the chances of \emph{amcast} violations occurring.  We have also presented a solution that allows Infinispan transaction managers to tolerate such violations regardless of the isolation level utilised, but at the expense of aborting transactions.  Our proposed solution for reducing \emph{amcast} violations can add an additional delay to the delivery of \textsf{SCast} \emph{amcast}s, when the \textsf{ABcast} protocol delivers messages via \textsf{Aramis}.  Therefore, such a solution should only be adopted when the potential increase in latency is more desirable then an increase in the total number of aborted transactions.      