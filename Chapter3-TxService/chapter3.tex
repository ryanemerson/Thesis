\chapter{AmaaS - Atomic Multicast as a Service}

% **************************** Define Graphics Path **************************
    \graphicspath{{Chapter3-TxService/Figs/Vector/}{Chapter3-TxService/Figs/}}

This chapter introduces the concept of providing \emph{abcast} messaging as a service to members of a cluster.

First we describe the rational behind \textsf{Abaas}, then we explore the requirements of such a service and the challenges involved in meeting them.  This is followed by the introduction of the \textsf{ABService} protocol that is used to implement \textsf{AbaaS}.  We then explain the methodology used to evaluate \textsf{ABService}, before presenting the results of our performance evaluation.  Finally, we discuss the limitations of existing \emph{abcast} protocols in the context of \textsf{AbaaS}, and propose the need for a new non-blocking \emph{abcast} solution.  

\section{Rational}
Total order commit protocols can be utilised by distributed systems to coordinate transactions without the use of locks.  Reducing the abort rate of transactions when contention is high, as system deadlocks cannot occur when distributed locks are not present.  Therefore, total order commit protocols can aid scalability as they improve transaction throughput \citep{Ruivo:2011:ETO:2120967.2121604}.  

The limiting factor of total order commit protocols is the underlying protocol used to provide atomic guarantees on message delivery.  The \emph{amcast} protocol, TOA, currently utilised by Infinispan, does not scale well as the number of destinations $N$ increase, as $N->1$ communication is expensive ($\S$ \ref{ssec:TOA_limations}).  Similarly, other GM protocols such as Newtop \citep{Ezhilchelvan:1995:NFG:876885.880005}, exasperate the problem, as the number of messages required to perform an \emph{amcast} increases dramatically as $N$ increases.  Finally, Quorum based protocols provide even less scalability, then GM based protocols, as their inability to \emph{amcast} messages to disjoint sets of nodes typically requires all nodes in the cluster to participate in an \emph{abcast}.  

Regardless of the protocol used, $N->1$ communication is inherently unscalable.  Therefore, we propose that reaching a consensus on ordering should not be conducted between a transaction coordinator $Tx.c$ and its participating Infinispan nodes $Tx.dst$.  Instead consensus should be restricted to an independent coordination service, consisting of a dedicated set of nodes, that agree upon a global ordering for a transaction, before multicasting the $Tx$ to all $d \in Tx.dst$.  We call this system model Atomic Multicast as a Service (\textsf{AmaaS}), and refer to the existing Infinispan approach as \emph{peer-to-peer} (P2P).  

Utilising \textsf{AmaaS} restricts the number of nodes required to reach consensus on a transaction's ordering to the nodes providing the \emph{amcast} service; known as \emph{service} nodes, or simply $s$-nodes, and denoted as $N_s$; Infinispan nodes are known as \emph{client} nodes, or simply $c$-nodes, and denoted as $N_c$.  All $N_s$ in the multicast service utilise the state machine approach, with each $s$-node forwarding their received $c$-node requests to all other $s$-nodes in the service.  An \emph{abcast} protocol is utilised by the $s$-nodes to forward $c$-node requests to all $s$-nodes, ensuring that all of the state machines ($s$-nodes) process $c$-node requests in the same order, therefore guaranteeing that the distributed state of $s$-nodes remains consistent.  

Figure \ref{fig:abaas_concept} shows the two key stages in the \textsf{AmaaS} model from the perspective of a $c$-node.  Stage 1 involves the $Tx.c$ requesting a multicast for $Tx$, $req(Tx)$,  from the multicast service, and occurs once the $Tx.c$ has been executed locally and is ready for broadcasting to each destination $d \in Tx.dst$; $req(Tx)$ contains the $Tx$ and all associated data, such as $Tx.dst$.  Upon receiving $req(Tx)$, the multicast service updates the records of all $s$-nodes and multicasts $Tx$, with a total order value determined by the service, to all $d \in Tx.dst$ (Stage 2).  

    \begin{figure}[htbp!] 
        \centering    
         \includegraphics[width=1.0\textwidth]{amaas_concept}
         \caption[Atomic Multicast as a Service Concept Diagram]{Atomic Multicast as a Service Concept Diagram}
         \label{fig:abaas_concept}
    \end{figure}	 

Decoupling message broadcasting and ordering means that regardless of $\left\vert Tx.dst \right\vert$, the number of nodes that need to reach a consensus in order to obtain a total order for $Tx$ will always be equal to $\left\vert s\text{-nodes}\right\vert$.  Restricting consensus to $s$-nodes means that, for all transactions the nodes involved in the consensus process will always be the same, therefore various optimisations can be made to the \emph{abcast} protocol used for consensus ($\S$ \ref{ssec:atomic_broadcast}), in addition to several other optimisations that can be made to the service itself ($\S$ \ref{ssec:abaas_optimisations}).  

Figure \ref{fig:ordering_service_concept} shows how an ordering request is handled by the $s$-node that receives an ordering request.  Stage 1 is simply $N_s1$ receiving the request $req(Tx)$ from $Tx.c$.  Upon receiving $req(Tx)$, $N_s1$ \emph{abcast}s the request to all $s$-nodes including itself (Stage 2).  Once $N_s1$ has received its own \emph{abcast} message, it is possible for $req(Tx)$ to be processed, a total order assigned to $Tx$, before $req(Tx)$ and its total order is multicast to all $d \in Tx.dst$ (Stage 3).  Note, that regardless of the number of destinations involved in $Tx$, the number of $s$-nodes participating in the \emph{abcast} is constant.  

    \begin{figure}[htbp!] 
        \centering    
         \includegraphics[width=1.0\textwidth]{ordering_service_concept}
         \caption[Multicast Service Concept Diagram]{Multicast Service Concept Diagram}
         \label{fig:ordering_service_concept}
    \end{figure}	 

	Lastly, implementing transaction ordering as a dedicated service, provides distinct advantages for system administrators.  When utilising the P2P \emph{amcast} approach, administrators would typically want to run Infinispan over a cluster of nodes utilising a homogeneous hardware specification to ensure that Infinispan's performance would not be hindered by a lesser machine.  In an environment where low-latency and high-capacity in-memory database is desired, this would require a large number of expensive machines. However, when \textsf{AmaaS} is utilised, it is possible to improve the performance of the entire cluster, simply by upgrading the $s$-nodes used to provide the \textsf{AmaaS} service.  For example, consider a cluster consisting of $50$ $c$-nodes, and $3$ $s$-nodes, instead of upgrading all $50$ $c$-nodes it is possible to improve transaction latency and throughput by upgrading the hardware of the $3$ $s$-nodes.  
	
	\subsection{Optimisations: A New Hope}\label{ssec:abaas_optimisations}
	The \textsf{Amaas} model allows for two key optimisations that are not possible when \emph{amcast}ing occurs directly between $c$-nodes in the P2P approach: \emph{Message Bundling} and \emph{Acknowledgement Piggybacking}.  
	
%		\subsubsection{Message Bundling} \label{ssec:bundling} \hspace{0pt} \\
		\paragraph{Message Bundling} \hspace{0pt} \\
		As the number of concurrent transactions between $c$-nodes increases, the total number of \emph{amcast}s required also increases.  When utilising \emph{amcast}s between $c$-nodes to coordinate the transactions, typically it is not possible to bundle multiple \emph{amcast} messages $<m_i, m_j>$, into a single \emph{amcast}, $m$, due to the high probability of $m_i.dst \neq m_j.dst$.  Of course it is possible to implement a bundling strategy that is utilised only when $m_i.dst = m_j.dst$, however in a system such as Infinispan the performance improvements provided by such a strategy are negligible; as the wide distribution of key/value pairs significantly reduces the probability of two \emph{amcast}s having the same destination set.  
		
		When utilising \textsf{AmaaS} it is possible for all \emph{amcast} requests received from $c$-nodes to be bundled into a single \emph{abcast} (between $s$-nodes) at a receiving $s$-node, regardless of their destination set.  This is because $s$-nodes are only required to send \emph{ambast}s to other $s$-nodes in order for a consensus on transaction ordering to be reached, therefore the destination set for each \emph{abcast} is the same for all $c$-node requests.   The ability to bundle multiple \emph{amcast} requests into a single \emph{abcast} reduces the number of times that consensus needs to be reached between all $s$-nodes.  Thus further reducing the number of $N->1$ communication steps required, with the total number of \emph{abcast}s reduced by $\left\vert bundle \right\vert$; where $bundle$ is the number of  \emph{amcast} requests from $c$-nodes that are sent as a single \emph{abcast}.  As a result of this optimisation network traffic is significantly reduced when requests are frequent, resulting in the capacity and scalability of an \emph{AmaaS} service increasing. Conversely, message bundling does not compromise performance when the number of service requests is low, as bundling does not require any intensive computation or additional communication steps.  
		
%		\subsubsection{Acknowledgement Piggybacking} \hspace{0pt} \\
		\paragraph{Acknowledgement Piggybacking} \hspace{0pt} \\
		In the \textsf{AmaaS} model, all executions of the \emph{abcast} protocol used for consensus, occur between the same set of $s$-nodes in the \emph{multicast service}, and hence, every \emph{abcast} has the same destination set.  As each \emph{abcast} is guaranteed to be received by all $s$-nodes it is possible for message acknowledgements to be piggybacked, therefore enabling \emph{abcast}s to be satisfied with a single dedicated broadcast ($\S$ \ref{ssec:newtop}).  This optimisation is ideal for \emph{abcast}s between $s$-nodes, as all nodes in the service must handle $c$-node requests, therefore each $s$-node will be sending \emph{abcast}s frequently, allowing message acknowledgements to be piggybacked, and reducing the average number of messages required per \emph{abcast}.  	
	
\section{Limitations of Existing Coordination Services}\label{sec:limitations_existing_coordination}
It is possible to utilise existing coordination solutions ($\S$ \ref{sec:coordination}), such as Zookeeper\citep{Hunt:2010:ZWC:1855840.1855851} and Chubby\citep{Burrows:2006:CLS:1298455.1298487}, as the basis of a \textsf{AmaaS} service.  However, both of these solutions are intended for high levels of read requests, not workloads that consist predominantly of write operations; where a write operation is classified as any operation that changes the state of a single $s$-node, which requires all other $s$-nodes to update their state in order to maintain the consistency of the replicated state.  

Write operations are vital to the \textsf{AmaaS} model, as every client request needs to be forwarded to all $s$-nodes within a service to ensure consistency (S1, S2).  Both of these existing services rely on establishing a quorum amongst $s$-nodes for each write operation, which is coordinated by a single master node, and as a result performance deteriorates rapidly as the number of write requests increase.  Therefore, if Zookeeper or Chubby were utilised as the basis of a \textsf{AmaaS} service, the service's latency and throughput would be poor as neither solution is intended for write-dominated workloads.  The observed limitations of these existing solutions is the motivation for requirement S4 in $\S$ \ref{sec:absaas_requirements}.  	
	
\section{System Requirements}\label{sec:absaas_requirements}
The \textsf{AbaaS} model consists of two distinct entities: $s$-nodes and $c$-nodes.  This section will explore the requirements that need to be met in order for the \textsf{AmaaS} model to be effective.  We consider requirements from the perspective of both clients, $c$-nodes, and the multicast service, $s$-nodes.

	\paragraph{Client Requirements} \hspace{0pt}
	\begin{itemize}
		\item [\textbf{CR1}] Clients must be able to send \emph{amcast}s to multiple destination sets that may overlap.
		
		\item [\textbf{CR2}] The service must provide a consistent total order $m.ts$ on messages irrespective of the $s$-node handling a clients request, or the $c$-node from which the clients request originated.  
		
		\item [\textbf{CR3}] Client nodes must be informed of $m_i$ when handling $m_j$, if $m_i.dst \cap m_j.dst$ to ensure that $m_i$ is not missed by a $c$-node in $m_i.dst$.  
	\end{itemize}
	
	\paragraph{Service Requirements} \hspace{0pt}
	\begin{itemize}
		\item [\textbf{S1}] The service must provide fault-tolerance ($s\text{-nodes} > 1$).
		
		\item [\textbf{S2}] The service must be highly available and non-blocking in the event of $s$-node failures.  Necessary to prevent the entire cluster becoming blocked if a $s$-node fails.   
		
		\item [\textbf{S3}] All service nodes must process client requests in the exact same order to maintain a consistent state between all $s$-nodes.  This prevents a $c$-node from receiving conflicting ordering data, for example two distinct $m$ being allocated the same global timestamp.  
		
		\item [\textbf{S4}] All $s$-nodes should be able to handle client requests, to allow for high availability and to prevent a single $s$-node becoming a performance bottleneck.
	\end{itemize}

\section{Protocol Details}\label{sec:decoupled_protocol}
% Change protocol name from Dcup
We have developed a protocol for the \textsf{AmaaS} model, which we call \textsf{SCast}; as the protocol offers multicast as a service, hence Service Multicast - \textsf{SCast}.  \textsf{SCast} satisfies all of the requirements specified in $\S$ \ref{sec:absaas_requirements} with the exception of S2.  S2 cannot be satisfied by our protocol directly, rather \textsf{SCast} relies on a \emph{abcast} protocol to guarantee S1, S3 and S4, therefore in order for S2 to be satisfied, the underlying \emph{abcast} protocol must not block in the presence of node failures.  

\textsf{SCast} consists of five distinct phases, each of which are explored below.  In the explanation below we assume that Infinispan is executing a 1-Phase Total Order transaction, without a second WSC phase, and that the transaction has already been successfully executed locally.  Furthermore, we assume that a reliable network protocol is being utilised as the underlying communication mechanism, for example TCP\citep{Cerf:2005:PPN:1064413.1064423} or Reliable UDP\citep{ReliableUDP}.  Finally, we refer to a collection of $s$-nodes providing the \emph{amcast} service as the \emph{multicast service}.  

	\begin{description}
		\item[1. Client Request - Client] \hfill \\
		Once a transaction coordinator, $Tx_i.c$, has completed its local execution of $Tx_i$ it is ready to \emph{amcast} a $prepare(Tx_i)$ message to $Tx_i.dst$ as required by the total order commit protocol.  In \textsf{SCast} \emph{amcasts} are initiated by the $Tx_i.c$ unicasting a \emph{amcast} request, $req(Tx_i)$, to all  $s$-nodes in the \emph{ordering service}.  The request, $req(Tx_i)$, contains the contents of a transaction's $prepare(Tx_i)$ message and the addresses of $Tx_i.dst$.  Each client request is associated with a unique id that consists of the $c$-node's address and a sequence number that is incremented after each request from this client.    
		
		\item[2. Receive Request - Multicast Service] \hfill \\
		Upon receiving $req(Tx_i)$, each $s$-node places the request in its \emph{Abcast Request Pool} (ARP), which is a bounded queue for storing requests before they are \emph{abcast} to all $s$-nodes.  If an $s$-node's ARP becomes full, subsequent requests from $c$-nodes are rejected until space becomes available in the ARP.  When a $c$-node request is rejected a \emph{reject} response is sent to $Tx_i.c$.  If $Tx_i.c$ receives a \emph{reject} response from all $s$-nodes, then it can either abort $Tx_i$ or resend the \emph{amcast} request after a configurable amount of time.    
		
		The ARP is necessary to ensure that if the \emph{ordering service} starts to become overloaded by client requests their is a 'feedback' mechanism that makes clients aware of the services current limitations, allowing clients to restrict user operations if necessary.  Utilising an ARP is also essential for providing message bundling, which as described in \ref{ssec:abaas_optimisations} is an effective optimisation for improving the throughput of the \emph{multicast service}.  
		
		\item[3. Process ARP - Multicast Service] \hfill \\
		A single thread, called the \emph{send} thread, is utilised for retrieving requests from the ARP and \emph{abcast}ing them to all $s$-nodes for ordering.  The \emph{send} thread retrieves ordering requests from the ARP in their arrival order, and bundles them into a single message bundle $mb$, before \emph{abcast}ing $mb$ to all $s$-nodes.  If the ARP is empty, then the \emph{send} thread waits for the ARP to become non-empty before resuming \emph{abcast}ing.  
		
		A configurable upper limit is placed on the maximum size (number of messages or \emph{bytes}) of a bundle message.  If this upper limit is reached and the ARP still has available requests, then the \emph{send} thread will \emph{abcast} the next message bundle $mb'$ once $mb$ has been \emph{abcast}.  If message bundling is not enabled, then a upper limit of one message is set for all bundles.    
		
		All \emph{abcast} $mb$ sent by an $s$-node have an originator field that is set to equal the sending node's address $N_s$, $mb.o$ = $N_s$, this is necessary for the next phase of the protocol.  
		
		\item[4. Process Requests and Multicast - Multicast Service] \hfill \\
		When an $s$-node, $N_s$, receives a request bundle $mb$, it 'un-bundles' $mb$ and processes each ordering request $req(Tx)$ in the order that they arrived in the ARP at $mb.o$.  If $N_s$ has already received $req(Tx)$ in a previous \emph{abcast} message, we discard this request and take no further action.  It is possible to discard a repeat request, as we know that all other $s$-nodes have, or will eventually, handle(d) the same copy of the request as $N_s$ due to the guarantees provided by \emph{abcast}.  Each accepted $req(Tx)$ is associated with a global timestamp $ts$: $req(Tx_i).ts = m.ts\oplus m.o \oplus$\emph{sequence number} of $req(Tx_i)$ within the bundle; where $\oplus$ is the append operator and $m.ts$ is the final timestamp provided by the underlying \emph{abcast} protocol utilised between $s$-nodes.  
		
		The $s$-node who's copy of $req(Tx_i)$ was first received by the $s$-nodes, and thus accepted, is responsible for multicasting a response message, $Rsp(Tx_i)$, containing the transaction and associated ordering data to all $Tx.dst$.  Delegating the multicasting of requests in this manner prevents $Rsp(Tx_i)$ being multicast by all $s$-nodes.  
		
		In addition to  the actual transaction, a multicast response $Rsp(Tx_i)$ consists of two types of ordering data: $ts$ agreed by $s$-nodes for $req(Tx_i)$, and $req(Tx_i)$'s \emph{immediate} predecessor data.  The latter is the identity of $Rsp(Tx_j)$ whose delivery at the specified $c$-node must \emph{precede} \emph{immediately} before delivery of $Rsp(Tx_i)$.  More precisely, all $d \in Rsp(Tx_i).dst$ must not deliver $Rsp(Tx_i)$ until they have delivered $Rsp(Tx_j)$, and only $Rsp(Tx_i)$ can be delivered immediately after $Rsp(Tx_j)$.
		
		The storage of \emph{immediate} predecessor data works as follows: All $s$-nodes maintain a map that stores a transaction history by mapping a $c$-node address with the id of the last transaction they were associated with, hence its \emph{immediate} predecessor.  So for each $req(Tx)$ the associated $req(Tx).ts$ is stored in the map for each $d \in Tx.dst$. When a $s$-node receives an \emph{abcast} bundle $mb$, it knows that all other $s$-nodes have received, or will receive, $mb$ in the same order.  Therefore, when $mb$ is processed by an $s$-node, it is guaranteed that all other $s$-nodes will have processed $mb$ in the exact same order, hence we know that the transaction history will be consistent across all $s$-nodes.  
		
		Note that the immediate predecessor of $Rsp(Tx_i)$ is applicable to \emph{all} \emph{amcast}s directed at a given $d$ - not just those that originate from $Tx_i.c$ nor just those that are handled by one $s$-node. Thus, it is specific to each $d \in Tx_i.dst$ and ensures that delivery at every $d$ is per the finalized $Rsp(Tx).ts$. To illustrate this, let $Tx_i.c$ send $req(Tx_i)$ to the \emph{multicast service}, with $N_s$'s \emph{abcast} copy being accepted , $Tx_j.c$ then sends $req(Tx_j)$ and $N_{s'}$'s copy is accepted by the service.  Assume $d \in Tx_i.dst \cap Tx_j.dst$ and the \emph{multicast service} orders $req(Tx_j)$ before $req(Tx_i)$, if $d$ receives $Rsp(Tx_i)$ before $Rsp(Tx_j)$ it will not deliver $Rsp(Tx_i)$ until it has delivered $Rsp(Tx_j)$.
    
    \item[5. Receive Multicast - Client] \hfill \\
    Upon receiving $Rsp(Tx_i)$, a $c$-node, $c$, will check the \emph{immediate} predecessor data that is applicable to $c$, in this case $Rsp(Tx_j)$.  If $Rsp(Tx_j)$ has been received by $c$ then $Rsp(Tx_i)$ can be delivered by $c$ and the $prepare(Tx_i)$ operation is executed.  However, if $Rsp(Tx_j)$ has not yet been received by $c$ then $Rsp(Tx_i)$ cannot be delivered locally, and $c$ must wait to receive $Rsp(Tx_j)$ before delivering $Rsp(Tx_i)$.  
    
    A single $ts$ is provided for each $d \in Tx.dst$ in the predecessor data, opposed to a list of past timestamps for each $d$, in order to reduce the size of each $Rsp(Tx)$.  This results in a cascading wait occurring if multiple messages have not yet been received by $c$.  For example, if $c$ has received $Rsp(Tx_i)$ but has not received its predecessors $Rsp(Tx_j)$ and $Rsp(Tx_k)$, $c$ is only aware of $m.j$, however when $Rsp(Tx_j)$ arrives, it reads $Rsp(Tx_j)$'s predecessor data and becomes aware that it has not yet received $Rsp(Tx_k)$ and must therefore wait for $Rsp(Tx_k)$ before delivering $Rsp(Tx_j)$ and $Rsp(Tx_i)$.  
    
   	\end{description}

	\subsection{Fault-Tolerance}
	Fault-tolerance in \textsf{SCast} must consider the consequences of both crashed $c$-nodes and $s$-nodes.  Here we explore the consequences of both $c$-node and $s$-node crashes during various stages of a \textsf{SCast} \emph{amcast}.  For the sake of simplicity, we only consider node crashes from the perspective of a single transaction, however it is worth noting that each $c$-node would typically have multiple transactions executing concurrently.  
	
	\paragraph{Client Node Crash}
	\begin{description}
         \item[\emph{Local Tx Execution}]  \hfill \\
         If a $c$-node, $Tx.c$, crashes during or directly after the local execution of $Tx_j$, then no action needs to be taken for this $Tx$, as no interactions with other nodes has occurred.  
		
	    \item[\emph{Phase 1}]  \hfill \\
	    If $Tx.c$ crashes during the unicasting of a request to the \emph{multicast service}, then three scenarios are possible: 
		    \begin{itemize}
			    \item    No $s$-nodes receive the request, in which case the multicast will never complete and no further actions are required.
			    \item    Not all of the $s$-nodes receive the original request, in which case the nodes who do receive the request will execute as normal.  All $s$-nodes will eventually receive details of the transaction, as the other $s$-nodes \emph{abcast} their copy of the request between service members, and the transaction will be multicast as normal.  
			    \item    Only one $s$-node receives a copy of the request and that $s$-node also crashes, in which case no further action is required as no other $s$-node is aware of the request.  
		    \end{itemize}        	
		
	    \item[\emph{Phase 2-5}]  \hfill \\
	    Finally, its possible for $Tx.c$ or any other $d \in Tx.dst$ to crash after the \emph{multicast service} has received the original request, in which case the service will continue to process the request as normal and multicast the message to all of the operative destinations.  
    \end{description}
    
	\paragraph{Service Node Crash}
	\begin{description}
       \item[\emph{Phase 1}] \hfill \\
       If a $s$-node crashes while a $Tx.c$ is issuing a service request, then the \emph{amcast} can still succeed as the client request is unicast to all $s$-nodes, therefore one of the remaining $s$-nodes will handle the request.  
	
       \item[\emph{Phase 2-3}] \hfill \\
       If a $s$-node crashes after receiving a request $req(Tx_i)$, then another $s$-node will eventually receive $req(Tx_i)$, as $Tx.c$ unicasts the request to all $s$-nodes.  
    
        \item[\emph{Phase 4}] \hfill \\
	    Its possible for a $s$-node, $N_s$, to crash just after it has been designated as the multicasting $s$-node for $Rsp(Tx_i)$.  In which case, it is necessary for the remaining $s$-nodes to take responsibility for multicasting $Rsp(Tx_i)$ to ensure that all $Tx_i.dst$ receive the transaction.  
	    
	    The remaining $s$-nodes can determine which requests still require multicasting if meta data is piggybacked onto each \emph{abcast} sent between $s$-nodes.  For example, if each operative $s$-node: 
	    \begin{enumerate}
	    \item    Piggybacks the timestamp of the latest request it has successfully responded to; where success is defined as the $Rsp$ message being multicast to all destinations.
	    \item    Maintains a recent history of client requests that have been accepted; storing the transaction as well as the address of the $s$-node whose \emph{abcast} request was accepted by the service.  
	    \end{enumerate}. 
	    
	    When an $s$-node crash is detected the remaining $s$-nodes iterate through their recent history of client requests, starting at the timestamp of the last confirmed multicast to be completed by the crashed $s-$node.  Each subsequent client request that was the responsibility of the crashed $s$-node to respond to, which has not been completed, is then handled by the operative $s$-node.  This will potentially cause multiple $s$-nodes to multicast the same $Rsp(Tx_i)$ message, however $c$-nodes can simply discard any duplicate transmissions that are received from the \emph{multicast service}.  
	    
	    \item[\emph{Stage 5}] \hfill \\
	    A $s$-node crash at this stage of the protocol has no effect on the outcome of the \emph{amcast}, as the $Rsp$ message has already been multicast to all destinations.  
    \end{description}

\section{Experimentation}
% Explain - 10 clients, x box members etc.  Utilise Aramis paper
% Flow control used in the box and in traditional TOA.  

To test our assumption that the \textsf{AmaaS} model would improve transaction performance, we developed an experiment that utilises the \textsf{AmaaS} Protocol (\ref{sec:decoupled_protocol}).  The details of these experiments are outlined below. 

\paragraph{AmaaS Experiment} \hspace{0pt} \\
We implemented an \textsf{AmaaS} service using the JGroups\citep{JGroups} framework with $n=2$ and $3$ $s$-nodes.  All nodes in the experiment utilised commodity PCs of \emph{3.4GHz Intel Core i7-3770} CPU and 8GB of RAM, running \emph{Fedora 20} and communicating over Gigabit Ethernet. The $s$-nodes are a part of a large university cluster, hence communication delays between them can be quite volatile as they are influenced by other network traffic and by jobs launched on $s$-nodes by other users.

Our experiments are based upon a highly modified version of an existing performance test available in the JGroups\citep{JGroups} framework, that mimics the partial replication of key/values in Infinispan\citep{Infinispan}.  In these experiments we utilise ten $c$-nodes in the same cluster, each of which emulates a transaction system that is reliant on the \textsf{AmaaS} service.  Each $c$-node operates 25 concurrent threads to initiate and coordinate transactions, and a transaction $Tx$ involves a set $Tx.dst$ of $3,4,\ldots,10$ $c$ nodes (including its coordinator). Each $Tx$ consists purely of key/value write operations and hence requires \emph{amcast} for completion. Read requests ($get(k)$) are not emulated, as the retrieval of key/values occurs before $Tx.c$ \emph{amcast}s its $prepare(k)$ message, and therefore has not baring on \emph{amcast} performance.  A thread coordinating a transaction starts the next $Tx'$ as soon as it dispatches commit/abort decision for the current $Tx$. Thus, at any moment, $250$ transactions are in different stages of execution.

A coordinator thread submits its \emph{amcast} request for $Tx$, denoted as $r(Tx)$, with some $s$-node; the latter stores such requests in the ARP in the arrival order. The \emph{Send} thread bundles some or all of these requests in the ARP in their arrival order into a message bundle $mb$, which can have a maximum payload of $1kB$, then \emph{amcast}s $mb$ to all other $s$-nodes.  The \emph{Send} thread waits if the ARP is empty and resumes bundling once ARP becomes non-empty. Thus, the number of requests bundled in any $mb$ varies depending on the request arrival rate. Once $r(Tx)$ has been \emph{amcast} to all $s$-nodes, it is returned to $Tx.c$ whom disseminates the message to $Tx.d$.  When all $d \in Tx.dst$ have received $r(Tx)$, the transaction is considered complete and the coordinator thread can start executing $Tx'$.  

\paragraph{P2P Experiment} \hspace{0pt} \\
In order to test the performance of P2P total order commits we repeated the  experiments detailed above, however $c$-nodes coordinate transactions between themselves without utilising any $s$-nodes.  Furthermore, the same cluster, and commodity machines were used for all experiments to ensure a fair comparison.   

\section{Performance Evaluation and Comparison}\label{sec:AmaaS_results}
Performance evaluation focuses on the comparison of a \textsf{Base} protocol (TOA), being utilised in a traditional P2P scenario (\emph{BaseP2P}), with an \textsf{AmaaS} service that implements the \textsf{SCast} protocol and utilises the same \textsf{Base} protocol between $s$-nodes (\emph{BaseService}).

Our performance comparison focuses on comparing transaction latency and throughput between \emph{BaseP2P} and \emph{BaseService}.  In \emph{BaseService}, latency is measured as the time elapsed between a $c$-node's initial transmission of $r(Tx)$ to some $s$-node, and all members of $Tx.dst$ delivering $r(Tx)$ to the experiment application. In BaseP2P, latency is measured as the time taken for all $Tx.dst$ to deliver $Tx$ to the experiment application. For both approaches, throughput is measured as the average number of \emph{abcast}s delivered by the experiment application per second at each $c$-node.

All of our experiments were conducted in isolation in order to prevent any side effects caused by simultaneous execution across the cluster, however we conducted all experiments during the same time period to ensure that the network was under similar loads for all of our experiments. 

\begin{figure}[htbp!]
 % \centering
 \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Latency}
 \caption{AmaaS Latency Comparison}
 \label{fig:LatencyGraph}
\end{figure}

\begin{figure}[htbp!]
% \centering
 \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Throughput}
 \caption{AmaaS Throughput Comparison}
 \label{fig:ThroughputGraph}
\end{figure}

Figures \ref{fig:LatencyGraph} and \ref{fig:ThroughputGraph} show latency and throughput, with $2N$ and $3N$ representing an \emph{ordering service} that consists of two and three, $s$-nodes respectively.  Each plot on the graph is an average of three \emph{crash-free} trials; a trial consists of each $c$ node completing $10^4$ transactions for a specific value of $|Tx.dst|$. Thus, BaseService receives a total of $10^5$ \emph{amcast} requests in each trial. In BaseP2P, each $c$ node initiates $10^4$ Base executions with its peers and the steady throughput in Figure \ref{fig:ThroughputGraph} as $|Tx.dst| \rightarrow 10$ suggests an absence of node saturation.

Referring to Figure \ref{fig:LatencyGraph}, as $|Tx.dst| \geq 4$, BaseP2P's \emph{abcast} latencies increase considerably, indicating that \emph{amcast}ing is best provided as a service for scalable performance. Comparing throughput in Figure \ref{fig:ThroughputGraph} leads to similar conclusions.  

BaseP2P's superior performance when $Tx.dst < 4$ can be attributed to the additional stages involved when utilising the \emph{AmaaS} model.  For example when BaseService utilises two $s$-nodes ($2N$) the following stages are required: $Tx.c$ sends a request, the \emph{multicast service} \emph{amcast}s it with $|m.dst| = 2$ and returns it to $Tx.c$, who must then broadcast $r(Tx)$ to $Tx.dst$.  Ignoring the individual message cost of each stage the total number of stages is four, whereas in BaseP2P the only step required is the \emph{abcast}ing of $Tx$.  So although $|m.dst|$ for each \emph{abcast} is less in BaseService ($|m.dst| = 2$) than BaseP2P ($|m.dst| = 3$), the overhead of sending a request to the \emph{multicast service} and back is much greater than the savings offered by reducing $|m.dst|$ by one node.  However, as $|Tx.dst|$ increases, the overhead of BaseP2P's increased $|m.dst|$ becomes significant, to the point where BaseService's additional communication stages becomes less of an overhead than the cost BaseP2P \emph{amcat}ing to a large $m.dst$.  

\paragraph{Summary} \hspace{0pt} \\
When deploying a large-scale distributed transaction system, higher throughout and lower-latency can be achieved by utilising a separate service to provide \emph{amcast} capabilities when the the number of nodes participating in a transaction is greater than three.  

\section{A New Atomic Broadcast Solution is Required}
Section \ref{sec:AmaaS_results} clearly shows that transaction throughput can be improved by utilising the \textsf{AmaaS} model for \emph{amcast}ing transactions.  However, the existing \emph{amcast} protocol, TOA,  used by $s$-nodes in our experiments, is a GM based protocol that blocks severely when node crashes occur.  This blocking behaviour is acceptable when TOA is utilised between client nodes, as it is presumed that blocking will only occur at a small subset of nodes in the cluster, thus system \emph{liveness} is still preserved at the majority of nodes.  However, if all $s$-nodes utilise TOA for \emph{abcast}ing and a single $s$-node crashes, all $s$-nodes will block, resulting in no client requests being satisfied. Therefore, not only are the $s$-nodes blocked, but as a consequence of this blocking, so to are all of the $c$-nodes.  Thus the entire system's \emph{liveness} is lost until the GM protocol is able to detect the $s$-node crash and unblock the ordering service.  GM based protocols, such as TOA, cannot be used as the basis of a \textsf{AmaaS} service as system-wide blocking would be catastrophic for any in-memory database.  

As detailed in sections \ref{sec:limitations_existing_coordination} and \ref{sec:coordination}, quorum based \emph{abcast} protocols are not suitable for use in \textsf{AmaaS} due to their tendency to block mildly when the master is falsely/validly suspected of crashing, and because of the performance limitations associated with master based protocols.  

Therefore, in order for \textsf{AmaaS} to be valid system model, a new \emph{abcast} protocol is required.  This protocol must provide non-blocking characteristics in the presence of node failures, whilst allowing for low-latency, high-throughput \emph{abcast}s in their absence.  

\section{Summary}
This chapter presented \textsf{AmaaS} - a new model for \emph{amcast} protocols that utilises a dedicated set of nodes to provide \emph{amcast} as a service to distributed transactional systems.  We presented a new protocol \textsf{SCast} for use in such an environment, and detail an experiment that allows for the performance of \textsf{AmaaS} and traditional P2P protocols to be evaluated.  Our performance evaluation showed that the \textsf{AmaaS} approach provides superior performance to the existing P2P approach as the number of nodes in a transaction increases.  Lastly, we outlined the shortcomings of existing \emph{abcast} solutions and the need for a new protocol in order for the \textsf{AmaaS} approach to be fully realised. 