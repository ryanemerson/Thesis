\chapter{ABcast}\label{ch:abcast}

% **************************** Define Graphics Path **************************
    \graphicspath{{Chapter4-ABcast/Figs/Vector/}{Chapter4-ABcast/Figs/}}

In this chapter we  introduce a hybrid \emph{abcast} protocol, called \textsf{ABcast}, which provides non-blocking message delivery in the presence of node failures and low-latency message delivery in their absence.  This protocol was designed for use amongst $N_s1 \ldots N_sn$ nodes within the \textsf{AmaaS} system model.  

The remainder of this chapter is structured as follows:  First we introduce the rationale behind utilising a Hybrid protocol and our design approach for \textsf{ABcast}, before detailing the protocol's requirements and assumptions.  This is followed by an in-depth look at the components required by \textsf{ABcast}, and how they have been implemented.  We then explore the two protocols used to create the hybrid solution in detail, outlining each protocol's delivery and rejection criteria for \emph{abcast} messages. Finally we discuss the need for additional application logic and a bespoke flow control algorithm when utilising \textsf{ABcast}.  

\section{Rationale}
    In the previous chapter we introduce \textsf{AmaaS}, a new system model that aims to increase the transactional throughput of distributed in-memory transactional systems.  This model depends on an \emph{abcast} protocol to maintain the replicated state between the service nodes which provide multicast ordering to client nodes; with each multicast request requiring a state change between service nodes.  For an \textsf{AmaaS} service to be viable it is vital that it provides low-latency responses to the requesting client nodes, as well as being able to handle an increasing number of client requests as the transactional system scales.  Furthermore, it is essential that such a service maintains high-availability, even in the presence of node failures, as an entire cluster of client nodes are dependent on the service.  Thus, it is essential that the underlying \emph{abcast} protocol utilised by the service can provide both non-blocking and low-latency message delivery in order to satisfy the clients requirements of highly-available and low-latency requests respectively.  
    
    \subsection{Existing Atomic Broadcast Solutions}
    The FLP impossibility \citep{Fischer:1985:IDC:3149.214121} dictates that in an asynchronous environment \emph{abcast} protocols must either admit blocking to meet its atomic guarantees or permit a likelihood of its guarantees not being met.  As  previously stated, known blocking protocols are of two types: GM dependent and Quorum based, both of which admit blocking in order to remain atomic.  The quorum based protocols block mildly due to false/valid suspicions of the leader node and GM protocols block severely but only in the presence of node failures.  Quorum based protocols provide non-blocking message delivery, however they only provide low levels of throughput as they are typically leader based, which ultimately limits the scalability of the transactional system.  Furthermore, there is also a non-zero probability that such protocols get stuck indefinitely in a cycle of leader elections after the previous leader node is falsely suspected of crashing\footnote{This is unlikely to occur in practice with adaptive or sufficiently long timeouts used for crash-suspicion.}.  On the contrary, GM based protocols provide very low latency \emph{abcast}'s that can handle high levels of throughput, however the severe blocking inherent in GM protocols would critically undermine an \textsf{AmaaS} service's availability in the event of a service node crash.  
    
    From the disadvantages stated above, it is clear that protocols belonging to the blocking category of protocols are not ideal when utilised within \textsf{AmaaS}.  Therefore it is necessary for a non-blocking approach to be utilised, that allows for the possibility that guarantees G1-G4 ($\S$ \ref{ssec:atomic_broadcast}) will not always be met in order to overcome the limitations of the FLP impossibility.  Utilising probabilistic guarantees on message delivery is an established technique for increasing the scalability of network multicasting systems\citep{Kermarrec:2003:PRD:766617.766623}, which has also been applied to \emph{abcast} protocols.  
    
    Felber \emph{et al.} \citep{Felber01probabilisticatomic} propose an \emph{abcast} protocol, \textsf{PABCast}, that provides probabilistic guarantees on both message \emph{safety} and \emph{liveness}.  If these probabilistic guarantees are not met, then it is possible for only a subset of the destination set to receive a broadcast, or for all destinations to deliver the broadcast but in an inconsistent ordering.  The aim of the  \textsf{PABCast} protocol is to provide increased scalability for atomic broadcasts across large numbers of destinations, not a small subset of nodes as required by the \textsf{AmaaS}.  As such the protocol does not consider throughput a primary concern.  The protocol uses \emph{rounds} to regulate when a node can initiate a broadcast.  A node cannot initiate a new broadcast until all broadcasts of the current round have been delivered locally.  Ultimately this protocol structure limits a sending node to a single broadcast, which clearly limits the protocol's throughput capabilities.  In the literature, the performance of \textsf{PABCast} is evaluated using a simulation that focuses on the scalability of the system in terms of message cost as well as the likelihood of a broadcast's \emph{safety} and \emph{liveness} being violated due to the probabilistic guarantees not being met. The performance evaluation presented in the paper does not consider the throughput or latency of the \textsf{PABCast} protocol, and the protocol is only evaluated using a simulation so it is not possible to ascertain how such a protocol will function in a live asynchronous system.  In conclusion, \textsf{PABCast} is not suitable for use in the \emph{AmaaS} system model.  
    
    \subsection{Our Approach}
    Our approach is to create a hybrid protocol that combines the leaderless GM-based protocol described in section \ref{ssec:newtop}, with a custom designed probabilistic \emph{abcast} protocol that is also leaderless.  As discussed earlier, the GM-based protocol provides the best possible performance in normal conditions, but blocks when nodes crash.  In these circumstances, probabilistic \emph{abcast} will be used to deliver \emph{abcast}s. We refer to the probabilistic protocol as \textsf{Aramis}, and the deterministic protocol as \textsf{Base}, which when combined creates the hybrid Atomic Broadcast protocol - \textsf{ABcast}.  
    
    It should be noted that both \textsf{Aramis} and \textsf{Base} work in parallel and additional overhead is minimal as both protocols are leaderless in nature.  Consequently, there is no protocol switch over in response to actual crashes; rather, both protocols attempt to deliver each \emph{abcast} in parallel; \textsf{Base} succeeds when there are no crashes and \textsf{Aramis} succeeds when \textsf{Base} is blocked.  
    
    \textsf{Aramis} is a non-blocking \emph{abcast} protocol that guarantees G1and G2 with a probability close to 1.  \textsf{Aramis} utilises the probabilistic synchronous model ($\S$ \ref{ssec:probabilistically_synchronous}), in conjunction with closely synchronised clocks, to calculate a probabilistic upper bound on \emph{abcast} delivery times; we refer to this upper bound as a message's delivery delay, $\Delta_m$.  
    
    \textbf{\textsf{Aramis: } An Informal Description.}  Upon receiving an \emph{abcast} message, a destination node waits for the calculated delivery delay to expire before delivering the message to the application.  If a message $m$ does not reach one of its destination, say $N_si$, before $\Delta_m$, then it is possible for $N_si$ to deliver a subsequent message $m'$ if $\Delta_{m'}$ expires before $m$ is received by $N_si$.  When such a scenario occurs the \emph{abcast} guarantees G1 will not be met and therefore the broadcast cannot be considered to be atomic.  Furthermore, $N_si$ will reject $m$ when $m$ arrives after $m'$ has been delivered so that G4 is met.  
    
    A key advantage of the \textsf{Aramis} approach is that no message acknowledgements are required for a message to be delivered, instead it depends entirely on the calculated delivery delay $\Delta_m$.  Relying solely on $\Delta_m$ ensures that faulty nodes have no effect on the delivery of a message at correct nodes and it is therefore impossible for a message's delivery to become blocked.  Furthermore, as no quorums or acknowledgements are required, it is possible for  \textsf{Aramis} to tolerate at most $(n - 1)$ destination crashes when $n$ nodes are involved in an \emph{abcast}.  
    
    The \textsf{Aramis} protocol was developed to be risk adverse, with all probabilistic calculations carried out pessimistically in order to ensure that $\Delta_m$ is rarely exceeded.  Furthermore, $\Delta_m$ always assumes the worst case scenario will happen when the protocol is executing (e.g. the originator node crashing during broadcast) to ensure that such situations are catered for.  A consequence of this pessimism, is that the latency of a \emph{abcast} message can be very large, typically 100-1000ms.  Note that these potentially large latencies, though not desirable, do not undermine \textsf{Aramis} from offering high throughput.  
    
    \textbf{\textsf{Aramis} and \textsf{Base}: An Informal Description.} To counteract the large delivery latencies of \textsf{Aramis} it is necessary to operate a low-latency \emph{abcast} protocol, \textsf{Base}, alongside \textsf{Aramis}.  The \textsf{Base} protocol is a GM based deterministic protocol, similar to NewTop\citep{Ezhilchelvan:1995:NFG:876885.880005}, that provides low-latency high throughput \emph{abcast}s at the expense of blocking when node failures occur.  In the context of an \textsf{AmaaS} ordering service, \textsf{Base} works as follows: A message's orginator, say $m.o = N_si$, broadcasts $m$ to every $N_sj$, which in turn broadcasts an $ack_j(m)$ to every node in the service.  Once a $s$-node has received $ack_j(m)$ from all $N_sj$, $N_sj \neq m.o$, $m$ becomes deliverable.  Note that if one $N_sj$ crashes during the \emph{abcast}ing of $m$, the delivery of $m$ will be blocked if the crashed node had not sent $ack_j(m)$ before crashing and the protocol must wait for the GM service to detect the crash so that it can unblock $m$.   

    
    In order to hone the advantages of both protocols it was necessary to create the hybrid protocol \textsf{ABcast}, where an \textsf{abcast} $m$ becomes deliverable either when $\Delta_m$ has elapsed (\textsf{Aramis}) or when $ack_j(m)$ is received from every $N_sj$ (\textsf{Base}).  This approach provides the application with the low-latency of \textsf{Base} for the majority of message deliveries, whilst ensuring that a missing acknowledgement is not waited upon for more than $\Delta_m$ time.  In the event of a node failure the \textsf{Base} protocol does not have to wait for the GM service to detect a crash before message delivery becomes unblocked.  Instead, messages will be delivered by \textsf{Aramis} after $\Delta_m$ expires.  Therefore, when node failures are present the \textsf{ABcast} protocol will always allow for a greater throughput of delivered messages than a traditional GM based protocol, assuming that $\Delta_m$ remains smaller than the time it takes the GM to detect a node failure.  In the worse case, if the GM delay is smaller than $\Delta_m$, then the \textsf{Base} protocol can simply unblock its message buffer and continue to deliver messages without the use of \textsf{Aramis}.  Finally, in normal working conditions, the \textsf{ABcast} protocol should have similar performance to a traditional GM based protocol as, in the majority of cases, \textsf{Aramis} is not used for message delivery.  

As the \textsf{ABcast} protocol utilises the probabilistic protocol \textsf{Aramis}, it is possible for a node not to have received an \emph{abcast} $m$ at all when that node uses \textsf{Aramis} for delivery.  However, as previously stated, \textsf{Aramis} is carefully designed to keep the probability of meeting G1 and G2 close to 1.  Furthermore, as \textsf{Aramis} is only used when \textsf{Base} is slow or a node failures occurs, the probability of an \emph{abcast} message $m$ being missed in the total order is the product of two very small probabilities; \textsf{Base} not being able to deliver $m$ and \textsf{Aramis} failing $m$.  Therefore, in reality the occurrence of a node not delivering an \emph{abcast} $m$ is rare.   

In this section we have described the inspiration and rationale behind the \textsf{ABcast} protocol, as well as presenting some high-level details of how the hybrid protocol functions.  In the next section we present the underlying assumptions made when designing \textsf{ABcast}, before detailing the specific components required to implement the protocol.  We then take in-depth look at both the \textsf{Aramis} and \textsf{Base} protocols, detailing the specifics of each protocol's delivery conditions as well as the maths behind \textsf{Aramis}'s probabilistic guarantees.  This is followed by an exploration of potential recovery strategies that can be utilised by an \textsf{AmaaS} service in the event of an \textsf{ABcast} message not meeting guarantees G1 and G4.  Lastly, we motivate the need for a new flow control protocol in order to maximise the effectiveness of the \textsf{ABcast} protocol.  

\newpage
\section{Assumptions}
    This section first defines the four key assumptions made when designing the \textsf{Aramis} protocol. 

    \subsection*{Assumptions:}  
    \begin{description} 
    % ******** Is this the case? Why?
        \item [\textbf{A1 - Fault Tolerance}] \hfill \\
        At most ($n-1$) of $n$ nodes involved in a broadcast can crash. However, 2 or more nodes cannot crash within an interval of some finite duration $\Delta_m$ that is smaller than a few seconds.
        
        \item [\textbf{A2 - Synchronised Clocks}] \hfill \\
        At any moment, clocks of any two operative nodes utilising \textsf{ABcast} are synchronised within $2\epsilon$ with a probability at least as large as $(1-10^{-5})$.
        
        We meet \textbf{A2} by implementing the well known probabilistic clock synchronisation algorithm \citep{Cristian:1996:SA:227210.227231}.  The details of our implementation and the parameters used are explored in $\S$ \ref{ssec:clocksynch}.       
        
        \item [\textbf{A3 - Reliable Communication}] \hfill \\
        When an operative node broadcasts $m$ to all $m.dst$, all operative destinations $d \in m.dst$ will eventually receive $m$.  
        
        We use reliable UDP protocol to guarantee that all operative nodes receive $m$ in crash-free scenarios.  However, when a broadcasting node crashes, the use of reliable UDP alone is not enough to ensure that all of the operative destinations receive $m$.  Therefore, a reliable broadcast, \emph{rbcast}, protocol will be required.  The Reliable UDP and \emph{rbcast} protocol we use are explored in detail in $\S$ \ref{ssec:reliable_udp} and $\S$ \ref{ssec:rbcast}, respectively.  
        
        \item [\textbf{A4 - Probabilistically Synchronous}] \hfill \\
        Let $x_{mx}$ be the maximum delay estimated at time $t$ by observing $NT_P$ transmissions in the recent past: The delay $x_{mx}$ will not be exceeded in any of $NT_F$, $NT_F \leq NT_P$, transmissions to unfold after $t$ with probability $(1 - q)$; where $q$ can be estimated with reasonable accuracy.  The measurement of $x_{mx}$ and $q$ are presented in section \ref{ssec:dmc}.  
        
        \textbf{A4} is motivated by previous research conducted by Ezhilchelvan \emph{et al.} \citep{Ezhilchelvan:2010:LPR:1773912.1773927} into PSM, which proposes that the challenges of designing asynchronous distributed systems, namely the FLP impossibility, can be avoided by assuming that the underlying network communication is synchronous to a given probability.  This assumption is crucial to \textsf{Aramis}'s efforts in minimising the probability of G1 and G2 not being met.  Informally, the larger the estimated $q$, the more intensive the efforts made by \textsf{Aramis} to preserve these guarantees and \emph{vice versa}.   
        
        A consequence of A4, is that \textsf{Aramis} is not suitable for use over the Internet, or similar networks that are susceptible to large fluctuations in network delays over a short period of time.  This is because frequent occurrences of such fluctuations in $NT_F$ can lead to $q$ being underestimated, \emph{i.e.} more violations of $x_{mx}$ occur than indicated by $q$.          
    \end{description}
    
\section{ABcast Components}
In this section we detail the individual components required by the \textsf{ABcast} protocol.  For each component, we explain its purpose and design; with important implementation details highlighted where appropriate.  All of the protocols presented in this thesis are implemented in Java using the JGroups framework.  

    \begin{figure}[!h] 
        \centering    
         \includegraphics[width=0.8\textwidth]{components_no_fcc}
         \caption[\textsf{ABcast} Protocol Components Overview]{\textsf{ABcast} Protocol Components}
         \label{fig:abcast_components}
    \end{figure}
    
   Figure \ref{fig:abcast_components} provides an overview of all of the components required by the \textsf{ABcast} protocol; where GM is the Group Membership service provided by JGroups, DMC is the Delay Measurement Component (\ref{ssec:dmc}) and \emph{rbcast} is the Reliable broadcast Component (\ref{ssec:rbcast}).  

    \subsection{Clock Synchronisation}\label{ssec:clocksynch}
    In order to provide synchronised clocks between nodes executing \textsf{ABcast}, we implemented the probabilistic clock synchronisation algorithm presented in \citep{Cristian:1996:SA:227210.227231} as a dedicated protocol in JGroups.  Cristian's algorithm is a master/slave protocol, that utilises a single master node's clock time to synchronise all of the slave nodes; with each slave periodically issuing a clock synchronisation request to the master in order to synchronise their clocks.  
            
            At any moment a slave's clock value is synchronised with the master node with a maximum error rate of $\epsilon$, with probability $\mathcal{P}_\epsilon \geq (1- 10^{-5})$. All of the experiments presented in this thesis utilise clock synchronisation with $\epsilon$ estimated as $1$ millisecond (ms).  A major consideration when estimating $\epsilon$ is the worst-case rate of clock drift between successive synchronisations. Ultimately, the longer the synchronisation interval, the larger the drift rate between clocks.  Estimation of $\epsilon = 1$ usually assumes an interval of $45$ minutes between synchronisations, however we use a shorter $15$ minute interval in order to increase $\mathcal{P}_\epsilon$.
            
            As each slave node synchronises its clock value with that of the master, it is possible for any two slave nodes to have a maximum error rate of $2\epsilon$.  This is because a slave $N_i$ could synchronise its clock behind the master's clock value by $\epsilon$ time.  Whereas, another slave $N_j$ could synchronise its clock ahead of the master by $\epsilon$. Hence, it is possible that $N_j.clockValue - N_i.clockValue = 2\epsilon$.  

    \subsection{Group Membership}\label{ssec:jgroups_gm}
    JGroups provides a GM service, called GMS which simply stands for Group Membership Service. GMS works as follows: upon discovering that a new node has joined the group or a node failure has occurred, GMS issues a new view to all of the protocols in the JGroups stack.  It is then the responsibility of the individual protocols to take the appropriate action when a new view is issued.  For example, unblocking message delivery if the local node was waiting for an acknowledgement from a node that is no longer present in the newly issued view.     
    
    \subsection{Reliable UDP}\label{ssec:reliable_udp}
    JGroups provides a reliable UDP protocol, \textsf{UNICAST3}, which guarantees that all UDP messages sent by a protocol higher in the network stack arrive at their destinations when node crashes do not occur.  This reliable UDP layer is placed below \textsf{ABcast} in the network stack to ensure that when messages are broadcast they are received by all destinations; where a broadcast consists of $m$ being unicast via \textsf{UNICAST3} to each of its intended recipients.  
    
    As well as providing reliable UDP unicasts, the \textsf{UNICAST3} protocol provides \emph{node-to-node} ordering as default for each message sent.  This ordering means that if a node $N_i$ sends two consecutive unicast messages, $m_1$ followed by $m_2$, to $N_j$, then $N_j$ will not deliver $m_2$ until it has first delivered $m_1$.  This behaviour is not always appropriate, therefore \textsf{UNICAST3} allows for messages to be sent Out-Of-Band (OOB), which simply means that messages will be sent reliably but they will be delivered at a destination as soon as they are received, regardless of the messages that have (or have not) been delivered before it.  Unless stated otherwise, our explanations assume that a unicast is sent using the default \textsf{UNICAST3} behaviour \emph{i.e.} not OOB.  
    
    \subsection{Reliable Broadcast}\label{ssec:rbcast}
    In the event of a node failure reliable UDP alone is not sufficient to ensure that assumption A3 holds.  This is because it is possible for a messages originator, $m.o$, to crash during the unicasting of $m$.  Assume that $m.dst = \{N_i, N_j, N_k\}$ and $m.o = N_i$, if $N_i$ crashes after unicasting $m$ to $N_j$ only, then $N_k$ will never receive $m$.  Similarly, if $N_i$ crashes during the unicasting of $m$ to $N_j$ it is possible that $N_i$ managed to send $m$ before crashing, in which case $m$ may eventually be received by $N_j$.  Both scenarios highlight that an additional protocol is required to ensure that all $m.dst$ receive $m$ in the event of $m.o$ crashing.  
    
    To overcome the limitations of Reliable UDP we have implemented a Reliable Broadcast protocol, called  \emph{rbcast}, that sits above the Reliable UDP layer in the network stack.  This protocol is inspired by the work of  Ezhilchelvan \emph{et al.} \citep{ezhilchelvan2011near}, as it utilises redundant broadcasts in collaboration with PSM, to ensure that all destinations receive a broadcast.  Our \emph{rbcast} protocol has been designed specifically for use with PSM based protocols and consequently utilises some of the values from the DMC ($\S$ \ref{ssec:dmc}) as protocol parameters.  Utilising the values provided by DMC allows  \emph{rbcast} to place a probabilistic timeliness guarantee on each message that is reliably broadcast.  With a high probability, \emph{rbcast} can guarantee that an operative destination will receive a given $m$ within $D_m$ time even if $m.o$ crashes during the initial broadcast of $m$.  The remainder of this section describes the basic \emph{rbcast} protocol, whilst the calculations for $D_m$ are presented in $\S$ \ref{ssec:dmc}.
    
    \subsubsection*{The \emph{rbcast} protocol}
    All messages broadcast via \emph{rbcast} include a tuple $\{m.o, m.seq\#, m.ts\}$ that uniquely identifies the broadcast.  Where $m.o$, short for message originator, is the address of the node that initiates a broadcast message; $m.seq\#$ is a sequence number unique to each $m.o$ that is incremented after each broadcast and $m.ts$ is a timestamp of $m.o$'s synchronised clock.  Note that the first two values of the tuple are sufficient to uniquely identify a broadcast.
    
    The \emph{rbcast} protocol supports two primitives: $rbcast(m)$ and $rb.deliver(m)$.  The protocol uses a set of parameters provided by the DMC, these are:
    
    \begin{description}[leftmargin=1cm, labelindent=1cm]
        \item[$\bm{x_{mx}}$ \textnormal{and}  $\bm{q}$ - ]    As described in our assumptions.
        
        \item[$\bm{\rho}$ - ]    The number of redundant broadcasts required, where $\rho \geq 1$.
        
        \item[$\bm{\eta}$ - ]    The delay observed between redundant broadcasts of $m$, to ensure successive broadcasts are independent.  
        
        \item[$\bm{\omega}$ - ]    A node's estimate of the networks Packet Delay Variation (PDV).  
        \end{description}
    
    \subsubsection*{$\bm{rbcast(m)}$}
    The primitive $rbcast(m)$ works as follows:
    \begin{enumerate}[label=\roman*]
        \item    The message to be broadcast, $m$, is created with the parameters defined above added as fields \emph{e.g.} $m.\rho, m.\eta, \ldots$ 
        
        \item    The $m$ to be $rbcast(m)$ is then broadcast redundantly $(\rho + 1)$ times, with each successive transmission occurring $\eta$ time after the previous one.  
        
        \item    Each succesive transmission of $m$ is identified by the filed $m.copy$, where $m.copy = 0, \ldots, \rho$.  
    \end{enumerate}
    
     \subsubsection*{$\bm{rb.deliver(m)}$}
     As soon as a copy of $m$ is received by a node, it is delivered up the stack to the higher level protocol (\textsf{ABcast}).  \\
     
    So, a $rbcast(m)$ is considered a success if every operative $d \in m.dst$ performs $rb.deliver(m)$.  Figure \ref{fig:rbcast} shows the \emph{rbcast} primitives and their relationship with the DMC and the underlying reliable UDP layer.  
    
    \begin{figure}[!h] 
        \centering    
         \includegraphics[width=1\textwidth]{rbcast}
         \caption[Reliable Broadcast Interactions]{Reliable Broadcast Interactions}
         \label{fig:rbcast}
    \end{figure}
    
    After $rb.deliver(m)$ has been executed, any destination $N_sj$ that receives $m.copy = k < \rho$ cooperates to ensure this success.  A destination waits to receive $m.copy \geq k + 1 \geq$ within a timeout $t_1 = \eta + \omega$, where $\eta= m.\eta$ and $\omega=m.\omega$.  If $t_1$ expires, $N_sj$ assumes that $N_si$ has crashed while broadcasting $m.copy = k$ and starts broadcasting $m.copy = k, k+1,\ldots, \rho$ on behalf of $N_si$. Note, $N_sj$ rebroadcasts the same copy of $m$ that it received, $m.copy = k$, as it does not know if all other $d \in m.dst$ have also received $m.copy = k$.  To reduce the possibility that no one else is acting on behalf of $N_si$, $N_sj$ adds a further random wait, $\zeta$, which is uniformly distributed on ($0,\eta$), before broadcasting $m.copy = k$.  This process continues until all $d \in m.dst$ have received or broadcast $m$ with $m.copy = \rho$.  
    
    Note that if $N_i$ does not crash, or if it crashes and an operative $s$-node receives $m$, then $m$ is broadcast \emph{at least} ($\rho + 1$) times.  Conversely, if $N_i$ crashes during the initial broadcast of $m$, and no members of $m.dst - \{m.o\}$ receive a copy of $m$, then the broadcast has failed and $m$ is lost.  This is acceptable for \textsf{ABcast}, because if no $d \in m.dst - \{m.o\}$ receive $m$, then its not possible for any node to $rb.deliver(m)$, therefore it is not possible for \emph{abcast} guarantees G1 or G2 to be violated.  
    
    
    \subsubsection*{Implementation Optimisation}
    It is worth noting that in our implementations of \emph{rbcast}, every broadcast where $m.copy = 0$ is treated differently to subsequent broadcasts of $m$.  Copy 0 of $m$ is broadcast to all destinations using the default settings of Reliable UDP, i.e. messages are delivered in the same order that they were originally unicast from their source address.  This means that if node $N_si$ broadcasts $m$, followed by $m'$, it is not possible for any of the destinations to receive copy $0$ of $m'$ before it has received $m$.  This also implies that if the transmission of copy $0$ of $m$ is slow or becomes lost, than copy $0$ of $m'$ cannot be forwarded up the network stack to \emph{rbcast} protocol until $m$ has been received.  To overcome this issue all messages with $m.copy > 0$ are sent OOB to ensure that they are forwarded to the \emph{rbcast} protocol as soon as they are received at the destination node.  Therefore, if $m'.copy = 0$ has been received but not been forwarded to \emph{rbcast}, then $m'.copy = 1$ is forwarded as soon as it arrives, bypassing the backlog of messages.  
    
    The calculations used to produce $x_{mx}, \eta, \rho$ and $\omega$ are discussed in detail in $\S$ \ref{ssec:dmc}.  
    
    \subsection{Delay Measurement Component}\label{ssec:dmc}
        For the sake of clarity, assumption A4 is repeated below:   
        
        \begin{quotation}
            The maximum delay, $x_{mx}$, estimated by observing $NT_P$ number of transmissions from the recent past, will \emph{not} be exceeded during $NT_F$ number of future transmissions that unfold next, where $NT_F \leq NT_P$ with a high probability ($1 - q$).
        \end{quotation}
    
        The delay measurement component is responsible for monitoring and observing the network latency of $NT_P$ transmissions from the recent past.  This recent past of latencies is then used to calculate various parameters that are required by the \emph{rbcast} and \textsf{ABcast} protocols for executing \emph{abcasts} in the near future.  Being conservative, we use $NT_F = 10\%$ of $NT_P$ and $NT_P=1000$; so, a \textsf{ABcast} node freshly estimates $x_{mx}$ for every 100 new delays it observes.  Each fresh estimation of $x_{mx}$ results in the recalculation of the following parameters: $\eta, \rho, q$ and $\omega$.  
        
        Latencies are measured by the DMC based upon the timestamp $m.ts$, which is included in every message $m$ that is broadcast via the \emph{rbcast} protocol.  As the clocks of all nodes executing the \textsf{ABcast} protocol are synchronised, it is possible to measure the one-way latency of each message that is received by a node.  For example, a node $N_i$ sends an \emph{rbcast} $m$ to $N_j$, upon receiving $m$, $N_j$ immediately records the latency $x$:

        \begin{equation*}
             \begin{aligned}
                 x = (N_j.clockValue - m.ts) + 2\epsilon
             \end{aligned}
        \end{equation*}        
        
        It is necessary to add $2\epsilon$ to each latency to ensure that if $N_j.clockValue$ is behind $N_i.clockValue$ by the maximum error of $2\epsilon$, a positive latency value is still recorded; this assumes that neither $N_i$ or $N_j$ is the clock master, in which case it is not possible for the max clock difference to be $2\epsilon$, only $\epsilon$, however as \emph{rbcast} operates independent of the clock synch algorithm $2\epsilon$ is pessimistically added to all latencies.  
                                
        The remainder of this section explores each of the parameters provided by the DMC, explaining what they represent and how they are calculated.  
        
        \begin{description}
        \item[\Huge$\boldsymbol{x_{mx}}$] \hfill \\
        $x_{mx}$ is simply the largest latency out of the $NT_P$ latencies observed in the recent past.  
        
        \item[\Huge$\boldsymbol{q}$] \hfill \\
                The parameter $q$ is the estimated probability that a transmission delay observed in the near future will exceed $x_{mx}$.  We estimate $q$ by observing the ratio of observed delays from the recent past that exceed $95\%$ of the recorded $x_{mx}$; thus, when more delays are observed closer to $x_{mx}$, $q$ tends to be large.  Although unlikely, it is possible that $q$ can be calculated as $1$ if all latencies in the recent past are the same, to avoid this we set a maximum value for $q$, $ q \leq q_{mx}$.  $q_{mx}$ is calculated as: 

        \begin{equation*}
             \begin{aligned}
                  q_{mx} = \sqrt{1 - R^{(\frac{1}{n} - 1)}}
             \end{aligned}
        \end{equation*}
        
        Where $R$ is a constant defined before run-time that specifies to what probability the system will be \emph{reliable}.  As the system is probabilistic the following must be true $R < 1$.  Note that $q$ cannot be accurately estimated, as it relates to future delays.  Therefore, \textsf{ABcast} uses $q$ estimates in a manner that accounts for potential inaccuracies and only for estimating one other parameter - $\rho$.
    
        \item[\Huge$\boldsymbol{\rho}$] \hfill \\
        $\rho$ is the parameter that determines the number of redundant broadcasts that are sent by the \emph{rbcast} protocol, with each broadcast sent $\rho + 1$ times.  The DMC utilies a constant $\rho_{min}$ which determines the lowest possible value for $\rho$ and is configured before runtime.  $\rho_{min}$ allows an application to artificially increase the amount of redundancy provided by the \emph{rbcast} protocol if a greater level of redundancy is required; $\rho_{min} = 1$ by default.
        
        Note, $\rho = 1$ is sufficient for crash-tolerance, however we estimate it so that at least one copy of $m$ reaches all operative nodes, other than $m.o$, within $x_{mx}$ delay and with a probability $> R$; more precisely, $\rho$ is estimated as the smallest integer that satisfies $\rho \geq \rho_{min}$ and

        \begin{equation*}
            \begin{aligned}
                (1-q^{\rho+1})^{n-1} > R
            \end{aligned}
        \end{equation*}

        which can be re-arranged as:

        \begin{equation*}
            \begin{aligned}
                \rho > \frac{ln(1-R)}{ln(q)} -1 \qquad \mbox{\emph{where}} \qquad R=(R)^{\frac{1}{n-1}}
            \end{aligned}
        \end{equation*}

        The inequality assumes that $m$ is multicast exactly ($\rho +1$) times in a crash-free manner and all $n-1$  intended recipients are operative. Both assumptions lead to a \emph{conservative} estimate of $\rho$. Moreover, for a given $R$, an integer $I =\rho_{min}, \rho_{min} + 1, \ldots$, satisfies: 

        \begin{equation*}
            \begin{aligned}
                I < \frac{ln(1-R)}{ln(q)}-1 < I+1
            \end{aligned}
        \end{equation*}
        
        for a wide range of $q$ values; e.g., for $R \approx 0.9999$:

        \begin{equation*}
            \begin{aligned}
                \frac{ln(1-R)}{ln(q)}-1 < 1 \quad \forall \quad q < 0.01 = 1\%
            \end{aligned}
        \end{equation*}

        Thus, small inaccuracies in estimating $q$ may not adversely affect $\rho$ estimates.  

        \item[\Huge$\boldsymbol{\eta}$] \hfill \\
        $\eta$ is the parameter used by \emph{rbcast} to determine the amount of time to wait between each broadcast of a message copy and is calculated as the largest delay in $n - 1$ transmissions (of a given copy $m$) with probability $R$.  Assuming exponential distribution $n$ is calculated as follows:
        
        % \eta=-\bar{x}[ln(1-R^{\frac{1}{n-1}})]     
        % \eta=-\bar{x}[ln(1-R)]  
        \begin{equation*}
            \begin{aligned}
                \eta=-\bar{x}[ln(1-R^{\frac{1}{n-1}})]  
            \end{aligned}
        \end{equation*}
        
Where $\bar{x}$ is the exponential mean of $NT_P$ observed delays.

        \item[\Huge$\boldsymbol{\omega}$] \hfill \\
        $\omega$ is the parameter utilised by \emph{rbcast} to approximate the PDV encountered by the network.  $\omega$ is simply calculated as:
        
        \begin{equation*}
            \begin{aligned}
                \omega = \eta - \bar{x}
            \end{aligned}
        \end{equation*}        
        
        Again, we assume exponential distribution and that $\bar{x}$ is the exponential mean of $NT_P$ observed delays.
        \end{description}

        \subsection*{Probabilistic Timeliness Guarantee - $\boldsymbol{D_m}$}
        The \emph{rbcast} protocol utilies the parameters provided by the DMC to apply a probabilistic timeliness guarantee to each broadcast sent by a node.  This timeliness guarantee ensures, with a high probability that all recipient nodes receive a message within $D_m$ time after the initial broadcast of $m$ was sent.  The remainder of this subsection details how $D_m$ is calculated and the rationalee behind the values used.  First we consider the calculations required for a broadcast where the originator does not crash, before exploring the modifications required to tolerate such a crash.  

        \begin{figure}[hb]
                \centering    
                \centerline{\includegraphics[width=1.15\textwidth, trim=0.1cm .25cm .1cm .25cm, clip=true]{rbcast_calculations}}
                \caption[Rbcast Calculations Diagram In a Crash Free Scenario]{Rbcast Calculations In a Crash Free Scenario}
                \label{fig:rbcast_calc}
            \end{figure}    

        \subsubsection*{Crash Free Message Originator}
        A node, $N_i$ broadcasts a message $m$ at time $ts$, $m.ts = ts$.  If $N_i$ does not crash then $D$ would be $D_1 = x_{mx} + \rho\eta$, as assumption \textbf{A4} means that $x_{mx}$ is the largest delay that will be encountered by $m$ and $\rho\eta$ accounts for the delays between successive copies of m being broadcast ($m.copy = 0,\ldots,\rho$).  Figure \ref{fig:rbcast_calc} shows the workflow of a message $m$ being \emph{rbcast} to $m.dst = \{N_i, N_j, N_k\}$ in a crash free scenario with $\rho = 2$; the individual parameters of $D_1$ are illustrated to show their significance.  
        
        Let $g_{D_1}$ be the probability that a destination, $N_j$, does not receive a single copy of $m$ at or before $ts + D1$.  Suppose that no node disseminates a copy of $m$ on behalf of $N_i$, and $\mathcal{P}(x > \xi)$ is the probability that a copy of $m$ takes longer than $\xi$ time to reach $N_{j}$.  The probability that none of the copies of $m$ broadcast at $ts, ts+\eta, \ldots,  ts+\rho \eta$, reaches $N_{j}$ by time $ts+D_1$, is:
        
        \begin{equation*}
            \begin{aligned}
                g_{D_1}= \mathcal{P}(x > D_1) \times \mathcal{P}(x > D_1 - \eta) \times \ldots \mathcal{P}(x > D_1 - \rho \eta)
            \end{aligned}
        \end{equation*}

        
        Recall that $q$ is the probability that a delay in $NT_F=100$ future transmissions exceed the maximum $x_{mx}$ observed in the past $NT_P=1000$ transmissions. Since:
        
        \begin{equation*}
            \begin{aligned}
                \mathcal{P}(x > D_1 - \rho \eta) = \mathcal{P}(x > x_{mx}) = q
            \end{aligned}
        \end{equation*}
        
and $\mathcal{P}(x > \xi)$ decreases as $\xi$ increases, we have $g_{D_1} < q^{\rho+1}$.  Even if $q$ is as high as 1\%, we have $g_{D_1} < 10^{-4}$ when $\rho=1$. If some destination disseminates $m$ on behalf of $N_i$, $\rho$ effectively increases and therefore $g_{D_1}$ reduces further.

When $N_i$ does not crash, a crash of a destination node $N_{k}$ cannot undermine an operative destination $N_{j}$ from receiving $m$. So, the probability that all operative $s$-nodes receive $m$ at or before time $ts +D_1$ is:

        \begin{equation*}
            \begin{aligned}
                (1-Q_1)=(1-g_{D_1})^{n-1}>(1-q^{\rho+1})^{n-1}
            \end{aligned}
        \end{equation*}

%            \begin{figure}[hb] 
%                \centering    
%                \centerline{\includegraphics[width=1.1\textwidth]{rbcast_calculations}}
%                \caption[Rbcast Calculations Diagram In a Crash Free Scenario]{Rbcast Calculations In a Crash Free Scenario}
%                \label{fig:rbcast_calc}
%            \end{figure}   

        \clearpage
        \subsubsection*{Crashed Message Originator}
Suppose now that $N_i$ crashes before completing redundant transmissions of $m$ and $n>2$. Consider the worst case that only one node, $N_j$, has $m$ with $m.copy=0$. (If $N_u$ has $m.copy>0$, then $N_i$ crashed only after it completed broadcasting $m.copy >= 0$, therefore some node other than $N_j$ has $m$ with a high probability).

If copy $m.copy = 0$ takes at most $x_{mx}$ to reach $N_j$ (which occurs with probability ($1-q$)), $N_j$ would start disseminating on behalf of $N_i$ at or before time:
        
        \begin{equation*}
            \begin{aligned}
                ts+ x_{mx} + \eta + \omega +\zeta
            \end{aligned}
        \end{equation*}
        
Recall that $\zeta$ is the random wait that all disseminating nodes must observe before disseminating $m$, with $\zeta$ uniformly distributed on ($0, \eta$) .  Therefore, assuming that the observed $\zeta$ is the largest possible value possible, $\eta$, let us define: 

        \begin{equation*}
            \begin{aligned}
                D = x_{mx} + 2\eta + \omega + D_1
            \end{aligned}
        \end{equation*}
        
By assumption \textbf{A1}, no other node can crash until at least $ts+D$. Thus, $N_j$ disseminates $m$, like $N_i$ in the crash-free case, to operative destinations which can now be at most $(n-2)$; so, all operative nodes receive $m$ at or before $ts +D$ with a probability: 
        
        \begin{equation*}
            \begin{aligned}
                (1-Q) = (1-g_{D_1})^{n-2}\times (1-q)>(1-q^{\rho+1})^{n-2}\times(1-q)
            \end{aligned}
        \end{equation*}

Figure \ref{fig:rbcast_crash_calc} shows the workflow of a message $m$ being \emph{rbcast} by $N_i$ to $m.dst \{N_i, N_j, N_k\}$.  $N_i$ crashes after unicasting $m$ to $N_j$, therefore $N_j$ waits to receive $m.copy > 0$ from $N_i$ for $2\eta + \omega$ time, before disseminating $m$ to the remaining destination $N_k$.  

In the event that $n = 2$, only two scenarios are possible in the event of $N_i$ crashing during the original broadcast of $m$.  Either $m.copy = 0$ is never received by the other node $N_j$, in which case $m$ is lost forever; Or $m.copy \geq 0$ is received by $N_j$, in which case $N_j$ will continue to broadcast the remaining copies of $m$ for completeness, however no other node will receive $m$ as no operative nodes remain.  For both scenarios, the delivery delay is calculated as $D_1$ as dissemination is not possible, therefore the probability that all operative nodes receive $m$ at or before time $ts+D_1$ is $(1-Q_1)$.  

To put the two cases together, $n > 2$ and $n = 2$, let boolean $\beta = 1$ if $n>2$ and $0$ if $n=2$; further, let: 

        \begin{equation*}
            \begin{aligned}
                D_m=\beta\times(x_{mx}+2\eta+\omega)+D_1
            \end{aligned}
        \end{equation*}
        
When $N_i$ \emph{rbcast}s $m$ at its clock time $ts$, if some operative $s$-node receives $m$, then every operative $s$-node receives $m$ at or before time $m.ts+D_m$ (as per $N_i$'s clock) with a probability:

         \begin{equation*}
            \begin{aligned}
                \beta\times(1-Q)+(1-\beta)\times(1-Q_1)
            \end{aligned}
        \end{equation*}

        \begin{sidewaysfigure}
            \vspace*{-1cm}
            \strictpagecheck
            \checkoddpage
            \ifoddpage
                \hspace*{-2.5cm}
            \fi           
            \centering
                \includegraphics[width=1.1\textwidth]{rbcast_crash_calculations}
            \caption[Rbcast Calculation Diagram with a Crashed Message Originator]{Rbcast Calculations with a Crashed Message Originator}
            \label{fig:rbcast_crash_calc}
       \end{sidewaysfigure}

        %\paragraph{NMC Validation}\label{a4_validation}\hspace{0pt} \\
                %We conducted a series of experiments to test \textbf{A4}, which were executed on a local computer cluster.  These experiments consisting of a 
                % Describe Experiment and execution environment
                % Results
                % Analysis
                
\clearpage
\section{Atomic Broadcast Protocol}\label{sec:ABcast}
Hitherto this chapter has focused on the underlying assumptions made when designing the \textsf{ABcast} protocol and the components it requires to function.  This section focuses on how the hybrid protocol functions, detailing the specifics of both the \textsf{Aramis} and \textsf{Base} protocols.  First we explore the \textsf{Base} protocol, as this protocol will be responsible for the majority of message deliveries and is the more conventional of the two \emph{abcast} protocols.  We then explore the \textsf{Aramis} protocol, detailing how messages are delivered and rejected as well as other related properties.  

    \subsection{Base}
    The \textsf{Base} protocol is based upon the NewTop \citep{Ezhilchelvan:1995:NFG:876885.880005} algorithm discussed in $\S$ \ref{ssec:newtop}, with a few key differences motivated by our use of synchronised clocks and PSM.  
    
    The \textsf{Base} protocol works as follows when a node $N_i$ sends an \emph{abcast} $m$: $N_i$ \emph{rbcast}s $m$, and as per the \emph{rbcast} protocol $m$ is assigned an id tuple $\{m.o, m.seq\#, m.ts\}$.  This tuple is utilised by \textsf{Base} to specify the total order of $m$, with all destinations in $m.dst$ ordering messages in ascending order based upon their timestamp.  In the event of any two message's having the same $m.ts$ value, the address specified in $m.o$ is used for tie-breaking to ensure a total order; note this is highly unlikely in practice as $m.ts$ is recorded in nanoseconds.  Similarly, In our implementation it is not possible for the same node to \emph{rbcast} two messages with the same $m.ts$, as a single thread called the \emph{sender} thread, is used for sending all $m$ with $m.copy = 0$.  Like NewTop, delivery of $m$ is blocked until each $d' \in m.dst - \{m.o\}$ has acknowledged $m$ by sending $ack_{d'}(m)$ to every $d \in m.dst$ and all $d \in m.dst$ have received $ack_{d'}(m) \forall (m.dst \setminus \{m.o,d\})$, thus C1 ($\S$ \ref{ssec:atomic_broadcast}) is met.  With each acknowledgement consisting of the id tuple that belongs to the message being acknowledged.  
    
    The use of synchronised clocks to uniquely timestamp each message, removes the need for tentative timestamps to be shared between destinations.  Instead the ordering of a message is dictated from its inception based upon its timestamp.  However a message's final place in the total order is not known by a destination until it has received an acknowledgement from all other $d' \in (m.dst - \{m.o\})$.  Consequently, the use of a message's id and acknowledgements are not sufficient for ensuring that C2 is met and G3-G4 are respected.  Consider $N_i$ \emph{rbcasts} two messages, $m$ and $m'$ with $m.seq\# = 1$ and $m.seq\# = 2$, respectively.  If a destination $N_j$ receives $m'$ first, then it knows that it has missed $m.seq\# = 1$, therefore it waits to receive and deliver $m$ before delivering $m'.seq\# = 2$, hence C2 is resolved.  Now consider a second scenario where two different nodes each \emph{rbcast} a message, $N_i$ sends $m$ and $N_j$ sends $m'$, where $m.ts < m'.ts$.  It is possible that $N_j$ will receive $ack_{N_i}(m')$ before $m$, in which case it will not know that $m$ should precede $m'$ in the total order, and as all acknowledgements have been received, $N_j$ will deliver $m'$, therefore violating G4.  
    
    In order to overcome the issues described above, each $d \in m.dst$ must maintain a \emph{vector clock}\citep{Mattern88virtualtime, fidge1988timestamps} that details the last \emph{rbcast} sent by $d$ and the id of the latest message received by each $d \in m.dst$; where the latest message received is the message containing the largest timestamp from a given node.  This vector clock is then included in every $m$ ($m.vc$) and $ack_{d'}(m)$ ($ack_{d'}(m).vc$) sent by a node.  Now lets reconsider the second scenario detailed above.  If $N_j$ receives $ack_{N_i}(m')$ before $m$, it will inspect $ack_{N_i}(m').vc$ and see that the last message that was \emph{rbcast} by $N_i$ was $m$, whose $m.ts < m'.ts$, therefore $N_j$ knows that they must deliver $m$ before $m'$.  For any message $m$, regardless of whether it originated at the current node, it is necessary for the associated acknowledgements and vector clocks to be checked to see if a message is missing from the total order before $m$ can be delivered.  If a node $N_i$ has not received $m$, but has learnt of its existence via a \emph{vector clock} or an acknowledgement, then we consider $m$ to be \emph{known} by $N_i$.  The \textsf{Base} delivery conditions are formalised below:
    
    \paragraph{\textsf{Base} Delivery Rule:}\hspace{0pt} \\
        A node, $N_j$ delivers any $m$, via \textsf{Base}, only after $D1_B$ and $D2$ stated below are satisfied:
        \begin{description}[labelindent=1cm]
            \item[$\boldsymbol{D1_B}$] - $m$ is acknowledged by all nodes other than $m.o$. 
            \item[$\boldsymbol{D2}$] - all \emph{known} $m'$, with $m'.ts < m.ts$ have been delivered.
        \end{description}
    
    \subsubsection*{Acknowledgement Piggybacking}\label{ssec:base_ack_piggyback}
    In the explanation of \textsf{Base} we assume that each acknowledgement is explicitly sent as dedicated message, however in practice this is an expensive operation in terms of both latency and bandwidth.  Therefore in our implementation of \textsf{Base} we piggyback message acknowledgements on subsequent \emph{rbcast}s sent by an the acknowledging node; acknowledgements are piggybacked onto all copies of $m$, i.e. all $m.copy =0,\ldots,\rho$, or not at all.  Of course this is only appropriate if there is a message waiting to be \emph{rbcast}, otherwise deadlock will occur at all $d \in m.dst$ as an acknowledgement will never be sent.  Consider, node $N_j$ is attempting to send $ack_{N_j}(m)$ to $N_i$, if $N_j$ does not receive an \emph{abcast} request within $\mathcal{A}_d$ time, then an explicit acknowledgement message is sent to $N_i$ containing $ack_{N_j}(m)$, as well as any other pending acknowledgements.  We define $\mathcal{A}_d$ as $\mathcal{A}_d = 2\eta + \omega$ and an explicit acknowledgement as being a dedicated message $m_{ack}$ that is unicast to all $d \in m.dst$ and is not assigned a \emph{rbcast} id; hence only a single copy of $m_{ack}$ is broadcast via reliable UDP.  \footnote{Explicit acknowledgements are not \emph{rbcast} in order to further minimise the bandwidth cost of sending $m_{ack}$.}
    
    \subsubsection*{Aborting rho > 0}
    Another possible optimisation is for the \emph{rbcast}ing of a message $m$, to be aborted as soon as $m$ has been delivered at the local node via the \textsf{Base} protocol; this includes nodes that are disseminating $m$ or waiting on $m.copy > 0$.  The logic for this optimisation is as follows: If a node $N_i$ has delivered $m$ locally, then we know that all other $d' \in m.dst$ must have received $m$ as $N_i$ must have received $ack_{d'}(m)$ from all $d' \in m.dst$ in order for $m$ to have been delivered locally.  Therefore it is safe to abandon the \emph{rbcast}ing of $m$ as all $m.dst$ have received at least one copy of $m$. 
    
    Note, this optimisation has \textbf{not} been utilised in our implementation of the \textsf{ABcast} protocol.  This optimisation is not possible if a message has been delivered locally by the \textsf{Aramis} protocol as it is impossible to know for certain that all $d' \in m.dst$ have received $m$.  

    \subsection{Aramis}\label{ssec:aramis}
    As previously stated,  \textsf{Aramis} is a non-blocking probabilistic \emph{abcast} protocol that utilises a calculated delivery delay $\Delta_m$ to place an upper bound on message deliveries.  The \textsf{Aramis} protocol works in conjunction with \textsf{Base} to ensure that message delivery does not become blocked in the event of slow or crashed nodes.  However, the \textsf{Aramis} protocol does not simply deliver each received message after $\Delta_m$ has expired, as this could cause a \emph{known} message to be missed in the total order.  Instead, it utilises the acknowledgements and \emph{vector clocks} that are integral to \textsf{Base} to ensure that \emph{known} messages are not missed from the total order.  Therefore, if a message $m$'s $\Delta_m$ delay expires, the message can only be delivered after all \emph{known} messages that precede $m$ in the total order have been delivered.  The \textsf{Aramis} delivery conditions are formalised below:
    
    \paragraph{\textsf{Aramis} Delivery Rule:}\hspace{0pt} \\
        A node, $N_j$ delivers any $m$, via \textsf{Aramis}, only after $D1_A$ and $D2$ stated below are satisfied:
        \begin{description}[labelindent=1cm]
            \item[$\boldsymbol{D1_A}$] - The clock of $N_i > m.ts + \Delta_{m}$. 
            \item[$\boldsymbol{D2}$] - All \emph{known} $m'$, with $m'.ts < m.ts$ have been delivered.
        \end{description}
    
    
        \subsubsection*{Calculating $\boldsymbol{\Delta_m}$    }
        The delivery delay, $\Delta_m$ utilised by \textsf{Aramis} is calculated based upon the values utilised by \emph{rbcast} and produced by the DMC.  Consider the following scenario which explains the rationalee behind the delays calculation: When any operative node, $N_j$ receives $m$ \emph{rbcast} by $N_i$, all operative nodes receive $m$ with a high probability by time $m.ts + D_m$, as per $N_i$'s clock.  If each recipient piggybacks their $ack(m)$ onto one of its own \emph{rbcast}s within $\mathcal{A}_d$ time after receiving $m$, it is very likely that all $m.dst - \{N_i, N_j\}$ will receive $ack(m)$, as per $N_i$'s clock, before time:
        
        \begin{equation*}
            \begin{aligned}
                m.ts + 2\times{D_m} + \mathcal{A}_d
            \end{aligned}
        \end{equation*}
         
         Based upon assumption A2, $N_j$'s clock has a maximum difference of $2\epsilon$ compared to $N_i$'s clock, with  probability $\mathcal{P}_\epsilon$, therefore $\Delta_m$ must be increased to cater for this, becoming:
        
        \begin{equation*}
            \begin{aligned}
                \Delta_m = 2 \times D_m + \mathcal{A}_d + 2\epsilon
            \end{aligned}
        \end{equation*}     
        
        Thus, delivering $m$ via \textsf{Aramis} after a node's local clock time is greater than $m.ts + \Delta_m$ will meet \emph{abcast} guarantees G1-G4 with a high probability.
                
        \subsubsection*{Total Order Violations}\label{ssec:abcast_rejection}
        In the explanations above, we have only considered the conditions required for a message to be successfully \emph{abcast} to all nodes.  However, as \textsf{Aramis} is a probabilistic protocol it is possible for a message to be delivered in the wrong place in the total order, thus violating G4.  For example, a message $m$ arrives at a destination after its proceeding message $m'$ in the total order has already been delivered, such that the total order is $<m', m>$ when it should be $<m, m'>$.  In such a case, their are two course of action possible when $m$ arrives: 1) Deliver $m$ as soon as possible; resulting in two violations of the total order occurring as $m$ has missed its place and then taken another's in the total order.  2) Receive $m$ and throw an exception to the application; resulting in only one violation of the total order, as $m$ is simply missing, but at the expense of violating G1 and G2 as $m$ cannot be delivered by all destinations.  \footnote{Assumption A2 guarantees that all destinations will eventually receive $m$, so if a message is not rejected G1 and G2 always hold.}
        
        \textsf{Aramis} seeks to minimise the effects of a miss-ordered message, therefore it employs the second approach and throws an exception when a message has been missed in the total order; we refer to this process as a message being \emph{rejected}.  Explicitly \emph{rejecting} a miss-ordered message from the total order allows for higher levels in the network stack (e.g. \textsf{SCast}) to initiate an appropriate recovery mechanism to mitigate the effects of a miss-ordered message on the system's state.  Furtheremore, it is possible to include the contents of the rejected message in the \emph{rejection} exception in order to provide potential useful data to the application's recovery mechanism.  
        
        The remainder of this section focuses on the scenarios in which it is possible for a message to be \emph{rejected} by a receiving node.  When the number of nodes, $n$, involved in an \emph{abcast} is $n=2$ the conditions required for a message to be rejected are different to $n > 2$, therefore we explore $n=2$ and $n>2$ in isolation. Note, for both $n=2$ and $n>2$ it is not possible for a broadcasting node to reject its own message, as by definition a broadcasting node will always receive its own message, hence G2 is always violated if a single node \emph{rejects} a message.  
        
        %\paragraph{Assumptions:}\hspace{0pt} \\
        \subsubsection*{Message Rejections - $\boldsymbol{n = 2}$}
        A message, $m$, sent by node $N_i$, can only be \emph{rejected} by the other recipient, $N_j$, when a message $m'$ from $N_j$ has been incorrectly delivered before $m$ in the total order; where $m.ts < m'.ts$ but the total order delivers $m'$ first, hence $m$ is \emph{rejected}.  In order for a message to be miss-ordered, both of the two statements below must be true:
        
        \begin{enumerate}
            \item $N_j$ broadcasts $m'$ with $m'.ts > m.ts$ and $m$ does not arrive at $N_j$ before $m'.ts + \Delta_{m'}$.  
            \item $N_j$ does not receive an acknowledgement for $m$, or any subsequent broadcasts sent by $N_i$, before $m'.ts + \Delta_m'$.   
        \end{enumerate}
        
        If condition one is not true, i.e. $m$ arrives before $m'.ts + \Delta_{m'}$, then $N_j$ has received $m$ before $m'$'s delivery time and therefore $N_j$ will not miss $m$ in the total order.  Similarly, if condition two is not true, then $N_j$ will know that a message sent by $N_i$ is missing as soon as it inspects the received acknowledgement, attached \emph{vector clock} or the $seq\#$ included in the id of the received message.  For both conditions to hold, it is necessary for $m$ and $m'$ to be delivered via the \textsf{Aramis} protocol at all nodes.  This is because the \textsf{Base} protocol relies on acknowledgments to deliver messages, therefore if \textsf{Base} delivers a message condition two cannot be true.  
        
                \subsubsection*{Message Rejections - $\boldsymbol{n > 2}$}
        If we assume the same scenario as described in the previous section ($n=2$), but with the addition of a node $N_k$ that represents all other nodes that are not $N_i$ or $N_j$.  A message is miss-ordered if conditions $1$ and $2$ stated above are true, and one of the following is true:
        
        \begin{itemize}
            \item[3a.] $N_j$ and all $N_k$ do not receive $m$ before $m'.ts + \Delta_{m'}$.
            
            \item[3b.] $N_j$ does not receive an acknowledgement for $m$ from any $N_k$, or a message from any $N_k$ that has received $m$, before $m'.ts + \Delta_{m'}$.  
        \end{itemize}
        
        If conditions $1,2$ and $3a$ are true, then it is not possible for $m$ to become known to $N_j$ as $m$ will not appear in an acknowledgement or vector clock of any $N_k$.  Similarly, if conditions $1,2$ and $3b$ are true, then $N_j$ will not know of $m$ in time to prevent $m'$ from being delivered before $m$ in the total order.  Similar to when $n=2$, if all three conditions hold (either $3a$ or $3b$), we can guarantee that $m$ and $m'$ are delivered via \textsf{Aramis} at all nodes.  We know that none of the $N_k$ nodes can deliver $m$ or $m'$ via \textsf{Base}, as $N_j$ cannot acknowledge a message until it has received it.  
        
        From both cases, $n=2$ and $n>2$, it is clear to see that in order for a message to be misordered, a chain of events need to occur that are highly unlikely based upon the probabilistic estimations made by the DMC and \emph{rbcast}.  
        		
    \subsection{ABcast Delivery Rule}
    Lastly, we present a concise formalisation of the delivery rule for the entire \textsf{ABcast} protocol.  A node, $N_j$ delivers any $m$ via \textsf{ABcast}, only after both $D1$ and $D2$ stated below are satisfied:
	    \begin{description}[labelindent=1cm]
	        \item[$\boldsymbol{D1}$] - The clock of $N_i > m.ts + \Delta_{m}$ ($D1_A$), or $m$ is acknowledged by all nodes other than $m.o$ ($D1_B$). 
	        \item[$\boldsymbol{D2}$] - All \emph{known} $m'$, with $m'.ts < m.ts$ have been delivered.
	    \end{description}
        
        
    \subsection{Initialisation Period}
     Unlike traditional \emph{abcast} protocols, such as TOA, the \textsf{ABcast} protocol requires a \textquoteleft{}warm-up' period before \emph{abcast}s can be sent between nodes.  This period is required in order to to synchronise the clocks of all participating nodes in the view, and to ensure that each node's DMC has recorded at least $NT_p$ latencies.  The synchronising of clocks is unavoidable due to \textsf{ABcast}s assumptions, and consequently, synchronisation must be performed first as the DMC is dependent on this assumption.  Our solution to recording $NT_p$ latencies, is to incorporate a mandatory \emph{probing} period that must be observed by all nodes in the current view after their clocks have been synchronised and before \emph{abcast}ing can begin.  

    The probing period required during initialisation utilises \textquoteleft{}empty' probe messages to record $NT_p$ latencies at each node's DMC.  An empty probe consists of a message, with a payload the size of those expected during \emph{abcast}ing, being unicast to all $n$ nodes in the current view.  This requires each node in the view to send at least $\frac{NT_P}{n}$ probes, however in reality the number should be higher to take into account that nodes will start the initialisation process at a different time.  With each subsequent probe broadcast after $x$ time has elapsed since the previous broadcast was sent; with $x$ being a value determined before run-time that should be an approximation of the expected frequency of \emph{abcast}s.  Once all nodes in the view have sent their probes, and recorded at least $NT_p$ latencies, it is possible for this node to start executing \emph{abcast}s.  The main disadvantages of utilising empty probes is the time required for each node to record $NT_p$ latencies and the difficulty of accurately predicting the correct frequency at which probes should be broadcast.  The latter is difficult, as the specified $x$ delay may be greater or less than the delay encountered between an application's \emph{abcast} requests, resulting in the network latencies being over or underestimated respectively.  
    
    \subsection{Adding New Nodes}
    When utilising \textsf{ABcast}, adding additional nodes to the current view is not as trivial as in more traditional deterministic protocols such as TOA.  Instead, when a new view is issued containing a new node, $N_i$, it is necessary for $N_i$ to undergo an initialisation period similar to that described in the previous section.  Clock synchronisation is simple, as $N_i$ can just contact the designated master node (as per \citep{Cristian:1996:SA:227210.227231}) and initiate the synchronisation protocol.  Recording the required $NT_p$ latencies is slightly trickier, as utilising a probing period similar to the initialisation period could have an adverse effect on all other nodes in the view.  This is because the existing nodes will most likely be heavily loaded from application requests, hence the need for an additional $s$-node, and adding additional load over a short period of time would be detrimental to performance.  Therefore, we propose that a better solution would be for new nodes to be \emph{silent watchers} until $NT_p$ latencies have been recorded.  A silent watcher, is a node that receives \emph{abcast}s from all other nodes in the view, but is unable to initiate its own \emph{abcast}s until after it has received $NT_p$ \emph{abcast}s and thus recorded $NT_p$ latencies.  When a new view is issued by the GM service, existing nodes include $N_i$ in the destination set of subsequent \emph{abcast}s, resulting in $N_i$ eventually receiving $NT_p$ messages.  Note, while $N_i$ is considered a silent watcher, it is still possible for it to participate in the redundant \emph{rbcast}s of message copies as the required timeout values are transmitted along with the message itself.      
    
\section{Handling ABcast Rejections}
 Unlike traditional \emph{abcast} protocols, such as TOA, it is possible for \textsf{ABcast} to reject messages, thereby preventing them from being delivered to the application outside of the total order ($\S$ \ref{ssec:aramis}).  Performance evaluation in chapter \ref{ch:perf_eval} shows that the likelihood of rejections occurring is very small even when the number of \emph{abcast}s is large, however as such events are possible it is necessary for applications to implement additional logic to accommodate such occurrences.  
 
Consider the \textsf{SCast} protocol defined in $\S$ \ref{sec:scast_protocol}.  The current protocol assumes that no message misorderings can occur and that \emph{abcast} guarantees G1-G4 always remain true.  When \textsf{ABcast} is utilised for \emph{abcast}ing client requests between two $s$-nodes, $\{N_i, N_j\}$, it is possible for an \emph{abcast} message, $m_i$, sent by $N_i$ to be rejected by $N_j$, resulting in the predecessor data stored at $N_j$ being inconsistent with $N_i$'s data.  This inconsistency within the \textsf{AmaaS} service, can lead to conflicts between data replicas at the client level.  For example, Imagine that two clients, $C_1$ and $C_2$ have each started a transaction, $Tx_i$ and $Tx_j$, both of which have the same destination set $dst=\{C_1, C_2\}$.  Client $C_1$ issues a \emph{amcast} request to $N_i$ and $C_2$ does likewise to $N_j$, therefore $N_i$ \emph{abcast}s $Tx_i$ as $m_i$ and $N_j$ \emph{abcast}s $Tx_j$ as $m_j$.  If both \emph{abcast}s are successfully delivered by service nodes, in the order $<m_i, m_j>$, then the predecessor data attached to the response message, $rsp(Tx)$, for $Tx_i$ will be the id of the preceding transactions $Tx_h$, and for $Tx_j$'s response message it will be $Tx_i$.  Thus, if clients $C_1$ or $C_2$ receive $rsp(Tx_j)$ before $rsp(Tx_i)$, then the \textsf{SCast} protocol at each client is able to deduce that $Tx_j$ cannot be delivered until $Tx_i$ has been delivered based upon $Tx_i$ being specified in $Tx_j$'s predecessor data.  

Now consider a scenario where $m_i$ is rejected by $N_j$.  This would result in the predecessor data stored at nodes $N_i$ and $N_j$ being inconsistent; as $N_j$ has not delivered $m_i$, therefore $N_j$'s predecessor data will not include $m_i$.  Thus, when $N_j$ sends the response message of $Tx_j$, $rsp(Tx_j)$ to the clients, $Tx_j$'s predecessor data will state that $Tx_h$ preceded $Tx_j$ not $Tx_i$. Consequently, clients $C_2$ or $C_1$ can now deliver $Tx_j$ to their transaction manager before $Tx_i$, which could result in the two clients processing $Tx_i$ and $Tx_j$ in different orders, resulting in the clients' replicas being inconsistent.  We refer to an instance where a client delivers a message ahead of its place in the total order, for example $Tx_j$ being passed to the transaction manager before $Tx_i$, as an \emph{amcast} violation, as it is the guarantees of the \emph{amcast} protocol \textsf{SCast} that have been violated.  

Violations of the \emph{amcast} ordering, as described above, can only occur if the service's response message, $rsp(Tx_j)$, containing the incomplete predecessor data, is received by a client node before the response message containing the correct data, $rsp(Tx_i)$.  If $rsp(Tx_i)$ is received first by a client, $C_2$, then $Tx_i$ will be delivered to the application \footnote{Assuming $rsp(Tx_h)$ has already been received and $Tx_h$ delivered.}.  When $rsp(Tx_j)$ is received, $C_2$ is able to deduce that the predecessor of $Tx_j$ specified in $rsp(Tx_j)$, $Tx_h$, has already been delivered and that another transaction, $Tx_i$, exists with a smaller timestamp than $Tx_j$, therefore $Tx_j$ must come after $Tx_i$ in the total order and $Tx_j$ can safely be delivered. Conversely, if $rsp(Tx_j)$ is received first by $C_2$ it will delivered to the application as $Tx_h$ has already been delivered and $Tx_j$ is unaware of $Tx_i$.  However, when $C_2$ eventually receives $rsp(Tx_i)$, it becomes aware that $Tx_i$'s specified predecessor was not the last message to be delivered and that $Tx_i$ should have preceded the last delivered message $Tx_j$.  In such a scenario It is not possible for $C_2$ to undeliver $Tx_j$, however $C_2$ can throw an exception containing the missed transaction, $Tx_i$, in order to alert the transaction manager that a \emph{amcast} violation has occurred.  Therefore allowing steps to be taken to mitigate the consequences of an \emph{amcast} violation.  

In the previous paragraphs we have presented the conditions required for a \emph{amcast} violation to occur when utilising the \textsf{SCast} protocol.  We now present a solution for handling rejections between $s$-nodes, and for handling \emph{amcast} violations that occur as a result of these rejections.  In our explanations, all client nodes are assumed to be running the Infinispan application.  As in the previous examples, we consider a service consisting of only two nodes for simplicity and these nodes are called $N_i$ and $N_j$.  Furthermore, we assume that the $Tx.dst$ sets of all transactions have overlapping nodes, if no overlaps existed, then it would not be possible for a \emph{amcast} violation to occur.  More specifically, we assume that $Tx.dst = {C_1,C_2}$ for all transactions, unless stated otherwise.  

    \subsection{SCast - Handling Rejections in a Ordering Service}
    Assume that $s$-nodes, $N_i$ and $N_j$ \emph{abcast} messages $m_i$ and $m_j$ respectively, and the total order is $<m_i, m_j>$.  If $N_j$ rejects $m_i$ because it has delivered $m_j$ in its place, then $N_j$'s predecessor data would record $m_j$'s id as the last transaction that interacted with clients $C_1$ and $C_2$.  By definition $N_i$ is guaranteed to deliver its own message $m_i$.  Now assume that $N_i$ also delivers $m_j$, in this case $N_i$'s predecessor data will also state that the last transaction involving clients $C_1$ and $C_2$ was $m_j.id$.  Therefore, we state that when a message $m$ is rejected by \textsf{ABcast} at a node $N$, no updates are required to $N$'s stored predecessor data if the existing transaction $id$ associated with a client is greater than the $m.id$ \footnote{This rule is applicable to all clients in $N$'s predecessor data, however in reality we only need to check the $id$'s associated with clients specified in the destination set of the transaction contained within the rejected $m$}.  

    Alternatively, consider a scenario where $m_i$ contains a transaction $Tx_i$ with $Tx.dst = {C_1, C_2}$ and $m_j$ contains a transaction $Tx_j$ with $Tx.dst = {C_2, C_3}$.  $N_i$ delivers $m_i$ and $m_j$, but $N_j$ rejects $m_i$, with the correct total order of messages being $<m_i, m_j>$.  In such a scenario, $N_i$'s predecessor data would associate $Tx_i$ with $C_1$, and $Tx_j$ with $C_2$ and $C_3$.  Whereas, $N_j$ would associate $C_2$ and $C_3$ with $Tx_j$, but $C_1$'s value would be null or the previously associated transaction.  Therefore, when $N_j$ is made aware of $m_i$ via the rejection exception, it is necessary for its predecessor data to be updated so that $C_1$ is associated with $Tx_i$, as the previous transactions timestamp will be less than $Tx_i.ts$.  If $N$'s predecessor is not updated, then it is possible for the predecessor data associated with a future client response to be incorrect as $m_i.id$ would be missing from $N$'s records.  Hence, it would be possible for another \emph{amcast} violation to occur if the predecessor data was not updated in this way.  
        
    \subsection{SCast - Reducing the Chances of \emph{amcast} Violations}
    Assume that the $s$-nodes, $N_i$ and $N_j$ are utilised in the same scenario as the previous sections, with $m_i$ being rejected by $N_j$ and $N_i$ delivering both $m_i$ and $m_j$ in that order.  Recall, that it is only possible for an \emph{amcast} violation to occur, when $m_i$ is rejected by one of the $s$-nodes, and that it is not possible for a message originator to reject its own message.  This leads to the node $N_j$ associating incorrect predecessor data to $Tx_j$ when a response is sent to $Tx_j.c$, which causes a \emph{amcast} violation if $N_j$'s $rsp(Tx_j)$ is received by any of $Tx_j.dst$ before $N_i$'s $rsp(Tx_i)$.  Therefore, in order to reduce the chances of \emph{amcast} violations, it is necessary to reduce the possibility of $rsp(Tx_j)$ being delivered before $rsp(Tx_i)$ at a client node.  
    
    By the properties of \textsf{ABcast} we know that it in order for a message to be rejected, it is necessary for both messages to have been delivered by \textsf{Aramis} ($\S$ \ref{ssec:aramis}).  Assuming the worst case scenario that, $m_i$ and $m_j$ have identical timestamp values, we know that $N_i$ will deliver each message no sooner than $\Delta_{m_i}$ and $\Delta_{m_j}$ time, whereas $N_j$ will deliver $m_j$ at $\Delta_{m_j}$ and reject $m_i$ when it eventually arrives.  Therefore, we know that $N_i$ will unicast $rsp(Tx_i)$ at $\approx \Delta_{m_i}$ and $N_j$ will do likewise for $rsp(Tx_j)$ at $\approx \Delta_{m_j}$.  In this worse-case scenario it is likely that both response messages will be unicast from the service at approximately the same time, however in most cases the timestamp of messages $m_i$ and $m_j$ may differ by several milliseconds, meaning that it is probable that the response message associated with $m_i$ will have a headstart on $m_j$, thereby reducing the chances of $m_j$'s response being received first by clients.  
    
    Knowing that response messages with inconsistent predecessor data will be sent at approximately the same time, allows us to utilise an additional timeout period that must be observed by clients.  When a message, $m_i$, is delivered via \textsf{Aramis} at $m.o$ ($s$-node that received the request), it is possible that $m_i$ may have been rejected by another $s$-node.  Therefore, we propose that if $m_i$'s originator deliver the message via \textsf{aramis}, then the corresponding $rsp(Tx_i)$ should be flagged so that each client knows that a \emph{amcast} violation is possible.  Each client node must then observe a timeout period $T$ after receiving $rsp(Tx_i)$\footnote{$T$ is specified by the $s$-node sending $rsp(Tx_i)$, and is included in its message header.}, in order to allow for any \emph{amcast}'s that were rejected by $N_i$ to be received before delivering $Tx_i$.  For example, if a client $C_2$ first receives $rsp(Tx_j)$, which does not contain $Tx_i$ in its predecessor data, it sees that $rsp(Tx_j)$ has been flagged and waits $T$ time before delivering $Tx_j$. In this $T$ time is possible, but not guaranteed, that the missing predecessor message $Tx_i$ will be received and that the \emph{amcast} violation will be avoided.  
    
    There are two problems with utilising such a timeout.  The first, is that the values of $\Delta$ will be different for each message originator, therefore it is possible for $\Delta_{m_i} < \Delta_{m_j}$, which undermines our assumptions if this difference is greater than the difference between the timestamps of $m_i$ and $m_j$.  To overcome this limitation, it is necessary for each node to record the maximum difference between its own calculations of $\Delta$ and the values calculated by other $s$-nodes.  This time difference is then utilised as a provisional value of $T$, $T_p$, with the final value of $T$ being determined by a weighting specified by the system administrator before runtime.  For example, if the administrator wanted the system to be more pessimistic they could calculate the final $T$ value as $T=2 \times T_p$, or optimistically as $T=1.1 \times T_p$.  
    
    The second problem of utilising timeout $T$, is that it will increase the latency of an \emph{amcast} when the service had to utilise \textsf{Aramis} to \emph{abcast} its request.  Furthermore, it is possible that even with this increased latency an \emph{amcast} violation can still occur.  The amortised effect of $T$ over a large number of \emph{amcast}s should be minimal, as textsf{Aramis} deliveries are rare when $s$-node crashes are absent.  When crashes do occur, the total delivery delay, $Dd$ for an \emph{amcast} is $Dd \approx T + \Delta + \mu$; where $\mu$ is the sum of all latencies encountered by unicasts between $Tx.c$, the selected $s$-node and each destination.  Based upon our findings in $\S$ \ref{ch:perf_eval}, $Dd$ will still be smaller then the time required for the GM service to detect the crash, hence this solution still provides lower latency then the traditional deterministic approach when node failures occur.  The fact that \emph{amcast} violations cannot be avoided entirely is a consequence of the probabilistic approach utilised by \textsf{ABcast}, therefore it is necessary for additional provisions to be in place at the Infinispan level to cater for when such violations do occur.  The use of timeout $T$ should be considered an aid to the provisions required at the Infinispan level, as $T$ only aims to minimise the chances of \emph{amcast} violations occurring, not prevent them entirely.  In conclusion, the benefit of utilising $T$ is that it reduces the chances of Infinispan nodes having to recover from inconsistent transaction orderings.  
    
    \subsection{Infinispan - Recovering from \emph{amcast} Violations}
    In the event that one or more \emph{amcast} violations occur, it is necessary for the client application to be able to recover from such events.  In this subsection, we present a recovery mechanism that can be employed by the Infinispan transaction manager at each client node in the event of a violation occurring.  The exact methods of the recovery mechanism are determined by the isolation level chosen by the Infinispan clients before runtime; with one method required for \emph{Repeatable Read}, $RR$ and \emph{Read Committed}, $RC$ transactions, whilst an alternative method is required for $RR$ with WSC due to its dependence on a voting phase to commit/abort transactions.  Therefore, we first explored the method required for $RR$ and $RC$ before detailing the provisions for $RR$ with WSC.  
    
    \subsubsection*{Repeatable Read and Read Committed}
    When $RR$ or $RC$ isolation is utilised the outcome of a transaction is determined in a single phase, as each $d \in Tx.dst$ decides deterministically whether to commit or abort a transaction without requiring any additional communication between other destinations.  Consequently, when a transaction manager receives a $prepare(Tx)$ message via \emph{amcast} it is able to commit the transaction immediately.  However, if an \emph{amcast} violation has occurred and a client $C$ has received an \emph{amcast} message, $m_j$, the transaction manager will be unaware that a preceding transaction sent in $m_i$ has been missed and $m_j$ will be committed.  Therefore, when the \emph{amcast violation} is discovered the only course of action is for a roll-back procedure, similar to that utilised in compensating transactions \citep{Korth:1990:FAR:645916.671971}, to be initiated, with all operations that should have occurred after $m_i$'s transaction being reversed.  Once these operations have been reversed it is possible for $m_i$'s transaction to be executed and the previously rolled back transactions reapplied in the correct total order.  For example, if the original value of a key $k$ was $v=5$, the transaction associated with $m_i$ was $put(k, 10)$ and $m_j$'s transaction was $put(k, v+1)$.  When $m_i$ is missing, the outcome of $m_j$'s transaction would be $v = 6$, however after $m_i$ is discovered and the transactions are reapplied, the result of $m_j$ will be $v = 11$.  A consequence of this approach is that \textquoteleft{}stale' reads can occur as it is possible for an Infinispan application, or subsequent transaction, to read $v = 6$ when requesting the value of $k$, ultimately leading to \emph{write-skew}.  We consider this an acceptable risk for two reasons: firstly, the window of opportunity for such an occurrence is very small \footnote{We anticipate that the time between an \emph{amcast} violation occurring and being detected to be in the order of milliseconds.  Furthermore, \emph{amcast} violations are only possible when \textsf{ABcast} rejects a message and a client node receives the message with inconsistent predecessor data first.  Therefore, the probability of a \emph{write-skew} occurring is the product of several small probabilities.}.  Secondly, the Infinispan store is already susceptible to \emph{write-skew}s when the WSC is not utilised, therefore the business logic of the application utilising Infinispan should already be tolerant of such phenomena.  Ultimately, this solution results in the state of all $d \in Tx.dst$ eventually becoming consistent if one or more \emph{amcast} violations occur at any $d$.  
    
    \subsubsection*{Repeatable Read with WSC}
    When utilising transactions with $RR$ and WSC it is not possible to utilise the previous solution, as the outcome of each transaction affects the second voting phase that is required to avoid \emph{write-skews}.  However, the additional voting stage required by the WSC can be used to our advantage to prevent \emph{amcast} violations from affecting the consistency of Infinispan's key/values.  Recall that the WSC requires that a transaction coordinator, $Tx.c$, receives at least one commit vote for each of the distinct keys involved in a $Tx$ in order for it to be able to send a $commit(Tx)$ decision to all $Tx.dst$.  Whereas, $Tx.c$ only requires a single abort vote from any of $Tx.dst$ members in order to disseminate an $abort(Tx)$ decision.  Furthermore, for every key stored in the infinispan system there exists a single \emph{primary} replica and at least one \emph{backup} replica.  The solutions presented in this section require a slight modification to this existing WSC behaviour.  Our solutions require that: 

\begin{quotation}    
    For all transactions, votes to commit or abort a transaction can only be sent by the transaction manager of the client node which contains the \emph{primary} replica of a key.  
\end{quotation}

     In the previous section, our solution was applicable to all transactions executing with $RR$ or $RC$ isolation, however the behaviour of our WSC solution is determined by the number $d \in Tx.dst$ that suffer from \emph{amcast} violations, the order in which messages are delivered to the transaction manager and the operations involved in the conflicting transactions.  Consider the following scenario: two nodes, $C_1$ and $C_2$ are participating in transactions $Tx_i$ and $Tx_j$ which are associated with \emph{amcast}s $m_i$ and $m_j$ respectively, with the destination set of both transactions being equal to $Tx.dst = \{C_1, C_2, C_3\}$ with $C_3$ being the transaction coordinator for both transactions.  Client $C_1$ hosts the \emph{primary} replica for all keys \footnote{Unless stated otherwise.} and each transaction consists of the following operations:
    
    \begin{lstlisting}
		    	Tx.begin();
		    	    put(k1, x);
		    	    put(k2, y);
		    	Tx.commit();
    \end{lstlisting}
    
     In the above scenario \emph{amcast} violations can occur in three different ways:
    
    \begin{enumerate}
        \item All nodes involved in a transaction suffer an \emph{amcast} violation, with $m_j$ being delivered before $m_i$ at all nodes.      
        
        \item At least one node storing a backup replica of a key suffers an \emph{amcast} violation, but the \emph{primary} replica does not.  
        
        \item The node containing the \emph{primary} replica for a key suffers an \emph{amcast} violation.  
        
        \item A node containing a \emph{primary} replica of a key, suffers an \emph{amcast} violation, however another \emph{primary} replica involved in a transaction does not.
    \end{enumerate}
    
    The recovery mechanism for all three of these scenarios are detailed below.
    
    \begin{description}
         \item[Scenario 1 - $m_i$ is missed by all nodes] \hfill \\
         For example when both $C_1$ and $C_2$'s deliver $m_j$ ahead of $m_i$.  In this case the active transaction in both nodes' transaction manager is $Tx_j$.  The workflow for such a scenario is as follows: The \emph{primary} replica of $k1$ and $k2$, unicasts a $vote(Tx_j)$ to the transaction coordinator, $Tx_j.c$, for both keys.  Upon receiving a $vote(Tx_j)$ from $C_1$, $Tx_j.c$ deduces that it has received a $vote(Tx_j)$ from the \emph{primary} replica of each key in the transaction and sends a $commit(Tx_j)$ or $abort(Tx_j)$ decision to all $Tx_j.dst$.   When a transaction manager receives a decision from $Tx_j.c$, the decision is executed locally on $Tx_j$ and a subsequent transaction is processed.  
         
         During, or after, the processing of $Tx_j$, $m_i$ will be received by the lower level \textsf{SCast} protocol and an exception thrown upto the transaction manager.  In which case, the processing of the current transaction will be suspended and the exception will be handled by the transaction manager.  When an exception is caught, the transaction manager determines whether this node hosts a \emph{primary} replica of any of the keys contained within the transaction.  If a node does not host a \emph{primary} replica, then it must wait to receive an $abort(Tx_i)$ from $Tx_i.c$ before aborting $Tx_i$ and resuming the current transaction.  However if a node contains a \emph{primary} replica, then it sends an $abort(Tx_i)$ vote to $Tx_i.c$ and aborts the transaction locally, before resuming the current transaction $Tx_j$.  A final decision will be sent by $Tx_i.c$ to all $Tx_i.dst$, however this node can ignore this message as $Tx_i$ has already been aborted locally. 
         

         \item[Scenario 2 - $m_i$ is missed by a backup replica $C_2$] \hfill \\
         For example when $C_1$ delivers $m_i$ in the correct total order, but $C_2$ delivers $m_j$, resulting in the currently active transactions for $C_1$ and $C_2$ being $Tx_i$ and $Tx_j$ respectively.  The workflow for such a scenario is as follows: $C_1$ is the \emph{primary} replica for all keys in $Tx_i$, therefore it will send a $vote(Tx_i)$ to the transaction coordinator.  $Tx_i.c$ receives this vote, realises that a vote has been received from all primaries and issues a $commit(Tx_i)$ or $abort(Tx_j)$ message to all $Tx_i.dst$.  Upon executing the local commit or abort of $Tx_i$, $C_1$ starts processing $Tx_j$.  
         
         As $C_2$'s active transaction is $Tx_j$, $C_2$ only becomes aware of the missed transaction $Tx_i$ when it receives a decision from $Tx_i.c$ or when it catches the exception thrown by \textsf{SCast} for $m_i$ \footnote{It is guaranteed that a node participating in $Tx.dst$ will eventually receive an exception from \textsf{SCast} and a final decision from $Tx_i.c$ for $Tx_i$, if $Tx_i.c$ does not crash. In the event that $Tx_i.c$ does crash, $Tx_i$ will eventually abort due to the use of transaction timeouts detailed in scenario 4}.  In the event that the \textsf{SCast} exception is caught first, the active transaction $Tx_j$ is suspended and $Tx_i$ becomes the active transaction until a decision for $Tx_i$ is received from $Tx_i.c$.  Upon receiving a decision from $Tx_i.c$, $C_2$'s transaction manager commits or aborts $Tx_i$ depending on $Tx_i.c$'s instructions.  
         
         Conversely, if the decision for $Tx_i$ is received before the \emph{SCast} exception, then one of two outcomes are possible.  If the decision is to abort $Tx_i$, then $C_2$ aborts $Tx_i$ locally and continues to execute $Tx_j$; ignoring $Tx_i$'s exception when it is caught as $Tx_i$ has already been aborted.  However, if the decision is to commit $Tx_i$, it is necessary to temporarily suspend $Tx_j$ and set $Tx_i$ as the active transaction.  The transaction manager then waits until the \textsf{SCast} exception is caught, before committing $Tx_i$.  This wait is necessary, as $C_2$'s transaction manager does not receive the actual transaction $Tx_i$ until this exception is caught, as the decision sent by $Tx_i.c$ only contains the $id$ of a transaction and the decision to commit or abort.  
         
        Finally, it is possible for $Tx_i$'s decision or exception to be received after $Tx_j$ has already been delivered if $Tx_i$ and $Tx_j$ have distinct transaction coordinators.  In the event of such an occurrence, it is necessary for $C_2$ to execute the rollback mechanism \footnote{As detailed in our $RR$ and $RC$ solutions.} in order to restore the state of the backup replicas $k_1$ and $k_2$.  For example, if $Tx_j$ has already been committed when $C_2$ receives $Tx_i$'s exception, then it is possible for $Tx_j$ to be rolled back so that the values of $k_1$ and $k_2$ are returned to their pre $Tx_j$ values, at which point $Tx_i$ and $Tx_j$ are executed sequentially so that the state of both keys is the same as at the \emph{primary} replica.  Note, utilising such a mechanism on a node hosting backup replicas does not change the outcome of the WSC on any transaction, as all decisions to commit or abort a transaction are made by the \emph{primary} replica.  
        
        
		\item[Scenario 3 - $m_i$ is missed by the \emph{primary} replica $C_1$] \hfill \\
        For example when $C_1$ suffers and \emph{amcast} violation and delivers $m_j$ before $m_i$, but $C_2$ delivers $m_i$ in the correct total order, resulting in $C_1$ and $C_2$'s active transactions being $Tx_j$ and $Tx_i$ respectively.  The workflow for such a scenario is as follows: $C_1$ is the primary for all keys in both $Tx_i$ and $Tx_j$, therefore when $Tx_j$ is $C_1$'s active transaction, $C_1$ sends a $vote(Tx_j)$ to $Tx_j.c$.  $Tx_j.c$ receives $C_1$'s vote, realises that it has received a vote from all \emph{primary} replicas and sends a decision to all $Tx_j.dst$.  Upon receiving the decision for $Tx_j$, $C_1$ starts executing subsequent transactions.  
        
        As $C_2$'s active transaction is $Tx_i$ and $C_2$ is a backup replica, $C_2$ will wait to receive a decision from $Tx_i.c$ before processing subsequent transactions.  No progress can be made by $C_2$'s transaction manager until $C_1$ discovers that it has missed $Tx_i$ in the total order.  When $C_1$ catches an exception from \textsf{SCast} containing $Tx_i$, it realises that it has missed $Tx_i$ in the total order because $Tx_i$'s timestamp is less than the previously processed transaction $Tx_j$.  Therefore, $C_1$ aborts $Tx_i$ locally and sends an abort vote to $Tx_i.c$.  Upon receiving this abort vote, $Tx_i.c$ sends an abort decision to all $Tx_i.dst$.  At which point $C_2$ will abort $Tx_i$ and start executing $Tx_j$.  
         
		\item[Scenario 4 - Deadlock between two \emph{primary} replicas when $m_i$ is missed by $C_2$] \hfill \\
		Unlike in the previous scenarios, this scenario requires that the two keys, $k_1$ and $k_2$ involved in our transactions $Tx_i$ and $Tx_j$ have distinct \emph{primary} replicas; with $C_1$ and $C_2$ being the \emph{primary} for $k_1$ and $k_2$ respectively.  Now consider that $C_1$ and $C_3$'s active transaction is $Tx_i$ as no \emph{amcast} violations have occurred at these nodes, whereas $C_2$'s is $Tx_j$ as $m_i$ was missed by the \textsf{SCast} protocol at this node.  The workflow for such a scenario is as follows: $C_1$ is the primary for $k_1$, therefore it sends a $vote(Tx_i)$ to $Tx_i.c$.  Likewise, $C_2$ is the primary for $k_2$, therefore it sends a $vote(Tx_j)$ to $Tx_j.c$.  Recall that the transaction coordinator for both $Tx_i$ and $Tx_j$ is $C_3$, hence the votes sent by both transactions are sent to $C_3$.  As $C_3$'s active transaction is $Tx_i$, $C_3$ will receive $vote(Tx_i)$ from $C_1$, however it will then wait indefinitely for a $vote(Tx_i)$ from $C_2$, as $C_2$'s active transaction is $Tx_j$.  Hence, a deadlock has occurred between the two \emph{primary} replicas of $k_1$ and $k_2$.  
		
		In order to resolve such a deadlock, we propose that all transactions should utilise an abort timeout as per the Two-phase commit protocol ($\S$ \ref{ssec:2PC}).  Therefore, in the case described above, $Tx_i$ will eventually abort as no progress can be made until $Tx_i$ times out at $C_3$.  Furthermore, it is also probable that $Tx_j$ will be aborted as the duration of both transactions will be similar due to both transactions being ordered via \textsf{SCast}'s service.  
		
		It is worth noting that, although we have reintroduced deadlock into the total order commit protocol, such occurrences are very rare.  The probability of a transaction being aborted is equal to the product of several small probabilities, with deadlock only occurring if the \textsf{ABcast} protocol rejects a message and an \emph{amcast} violation occurs which happens to result in scenario 4.  
    \end{description}

    \subsection{Summary}
        In this section we have presented several strategies that can be utilised by \textsf{SCast} nodes, both service and clients, to reduce the chances of \emph{amcast} violations occurring.  We have also presented a solution that allows Infinispan transaction managers to tolerate such violations regardless of the isolation level utilised, but at the expense of aborting transactions.  Our proposed solution for reducing \emph{amcast} violations can add an additional delay to the delivery of \textsf{SCast} \emph{amcast}s, when the \textsf{ABcast} protocol delivers messages via \textsf{Aramis}.  Therefore, such a solution should only be adopted when the potential increase in latency is more desirable then an increase in the total number of aborted transactions.  

\section{Need for a new Flow Control Protocol}
The \textsf{ABcast} protocol described in this section functions as expected when each node's throughput is low, with the number of requests per second around five hundred.  However, in its current form the protocol has no flow-control, therefore as the number of requests per second increases the system starts to become saturated by requests.  The JGroups project provides an implementation of a credit based flow control protocol, however this is a generic solution that is aimed at deterministic protocols and as such does not take into account the unique requirements of the \textsf{ABcast} protocol nor does it take advantage of its probabilistic calculations.  Ultimately, a new bespoke flow control protocol is required for \textsf{ABcast} that utilises PSM and the measurements of the DMC.  Such a protocol is presented in the next chapter.  

\section{Summary}
This chapter presented \textsf{ABcast} - a new hybrid protocol that utilises both a deterministic (\textsf{Base}) and probabilistic (\textsf{Aramis}) protocol in order to create a non-blocking \emph{abcast} solution.  We detail the protocol's assumptions and required components, before detailing the delivery and rejection criteria of the two protocols.  This is followed by an in-depth description of the problems faced when utilising a probabilistic protocol such as \textsf{Aramis} and some example solutions.  Finally, we propose that a new flow control protocol is required in order to maximise the potential of the \textsf{ABcast} protocol.  