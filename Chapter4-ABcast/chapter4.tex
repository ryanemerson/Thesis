\chapter{ABcast}\label{ch:abcast}

% **************************** Define Graphics Path **************************
    \graphicspath{{Chapter4-ABcast/Figs/Vector/}{Chapter4-ABcast/Figs/}}

In this chapter we  introduce a hybrid \emph{abcast} protocol, called \textsf{ABcast}, which provides non-blocking message delivery in the presence of node failures and low-latency message delivery in their absence.  This protocol was designed for use amongst $N_s1 \ldots N_sn$ nodes within the \textsf{AmaaS} system model.  

The remainder of this chapter is structured as follows:  First we introduce the rationale behind utilising a Hybrid protocol and our design approach for \textsf{ABcast}, before detailing the protocol's requirements and assumptions.  This is followed by an in-depth look at the components required by \textsf{ABcast}, and how they have been implemented.  We then explore the two protocols used to create the hybrid solution in detail, outlining each protocol's delivery and rejection criteria for \emph{abcast} messages. Finally we discuss the need for additional application logic and a bespoke flow control algorithm when utilising \textsf{ABcast}.  

\section{Rationale}
    In the previous chapter we introduce \textsf{AmaaS}, a new system model that aims to increase the transactional throughput of distributed in-memory transactional systems.  This model depends on an \emph{abcast} protocol to maintain the replicated state between the service nodes which provide multicast ordering to client nodes; with each multicast request requiring a state change between service nodes.  For an \textsf{AmaaS} service to be viable it is vital that it provides low-latency responses to the requesting client nodes, as well as being able to handle an increasing number of client requests as the transactional system scales.  Furthermore, it is essential that such a service maintains high-availability, even in the presence of node failures, as an entire cluster of client nodes are dependent on the service.  Thus, it is essential that the underlying \emph{abcast} protocol utilised by the service can provide both non-blocking and low-latency message delivery in order to satisfy the clients requirements of highly-available and low-latency requests respectively.  
    
    \subsection{Existing Atomic Broadcast Solutions}
    The FLP impossibility \citep{Fischer:1985:IDC:3149.214121} dictates that in an asynchronous environment \emph{abcast} protocols must either admit blocking to meet its atomic guarantees or permit a likelihood of its guarantees not being met.  As  previously stated, known blocking protocols are of two types: GM dependent and Quorum based, both of which admit blocking in order to remain atomic.  The quorum based protocols block mildly due to false/valid suspicions of the leader node and GM protocols block severely but only in the presence of node failures.  Quorum based protocols provide non-blocking message delivery, however they only provide low levels of throughput as they are typically leader based, which ultimately limits the scalability of the transactional system.  Furthermore, there is also a non-zero probability that such protocols get stuck indefinitely in a cycle of leader elections after the previous leader node is falsely suspected of crashing\footnote{This is unlikely to occur in practice with adaptive or sufficiently long timeouts used for crash-suspicion.}.  On the contrary, GM based protocols provide very low latency \emph{abcast}'s that can handle high levels of throughput, however the severe blocking inherent in GM protocols would critically undermine an \textsf{AmaaS} service's availability in the event of a service node crash.  
    
    From the disadvantages stated above, it is clear that protocols belonging to the blocking category of protocols are not ideal when utilised within \textsf{AmaaS}.  Therefore it is necessary for a non-blocking approach to be utilised, that allows for the possibility that guarantees G1-G4 ($\S$ \ref{ssec:atomic_broadcast}) will not always be met in order to overcome the limitations of the FLP impossibility.  Utilising probabilistic guarantees on message delivery is an established technique for increasing the scalability of network multicasting systems\citep{Kermarrec:2003:PRD:766617.766623}, which has also been applied to \emph{abcast} protocols.  
    
    Felber \emph{et al.} \citep{Felber01probabilisticatomic} propose an \emph{abcast} protocol, \textsf{PABCast}, that provides probabilistic guarantees on both message \emph{safety} and \emph{liveness}.  If these probabilistic guarantees are not met, then it is possible for only a subset of the destination set to receive a broadcast, or for all destinations to deliver the broadcast but in an inconsistent ordering.  The aim of the  \textsf{PABCast} protocol is to provide increased scalability for atomic broadcasts across large numbers of destinations, not a small subset of nodes as required by the \textsf{AmaaS}.  As such the protocol does not consider throughput a primary concern.  The protocol uses \emph{rounds} to regulate when a node can initiate a broadcast.  A node cannot initiate a new broadcast until all broadcasts of the current round have been delivered locally.  Ultimately this protocol structure limits a sending node to a single broadcast, which clearly limits the protocol's throughput capabilities.  In the literature, the performance of \textsf{PABCast} is evaluated using a simulation that focuses on the scalability of the system in terms of message cost as well as the likelihood of a broadcast's \emph{safety} and \emph{liveness} being violated due to the probabilistic guarantees not being met. The performance evaluation presented in the paper does not consider the throughput or latency of the \textsf{PABCast} protocol, and the protocol is only evaluated using a simulation so it is not possible to ascertain how such a protocol will function in a live asynchronous system.  In conclusion, \textsf{PABCast} is not suitable for use in the \emph{AmaaS} system model.  
    
    \subsection{Our Approach}
    Our approach is to create a hybrid protocol that combines the leaderless GM-based protocol described in section \ref{ssec:newtop}, with a custom designed probabilistic \emph{abcast} protocol that is also leaderless.  As discussed earlier, the GM-based protocol provides the best possible performance in normal conditions, but blocks when nodes crash.  In these circumstances, probabilistic \emph{abcast} will be used to deliver \emph{abcast}s. We refer to the probabilistic protocol as \textsf{Aramis}, and the deterministic protocol as \textsf{Base}, which when combined creates the hybrid Atomic Broadcast protocol - \textsf{ABcast}.  
    
    It should be noted that both \textsf{Aramis} and \textsf{Base} work in parallel and additional overhead is minimal as both protocols are leaderless in nature.  Consequently, there is no protocol switch over in response to actual crashes; rather, both protocols attempt to deliver each \emph{abcast} in parallel; \textsf{Base} succeeds when there are no crashes and \textsf{Aramis} succeeds when \textsf{Base} is blocked.  
    
    \textsf{Aramis} is a non-blocking \emph{abcast} protocol that guarantees G1and G2 with a probability close to 1.  \textsf{Aramis} utilises the probabilistic synchronous model ($\S$ \ref{ssec:probabilistically_synchronous}), in conjunction with closely synchronised clocks, to calculate a probabilistic upper bound on \emph{abcast} delivery times; we refer to this upper bound as a message's delivery delay, $\Delta_m$.  
    
    \textbf{\textsf{Aramis: } An Informal Description.}  Upon receiving an \emph{abcast} message, a destination node waits for the calculated delivery delay to expire before delivering the message to the application.  If a message $m$ does not reach one of its destination, say $N_si$, before $\Delta_m$, then it is possible for $N_si$ to deliver a subsequent message $m'$ if $\Delta_{m'}$ expires before $m$ is received by $N_si$.  When such a scenario occurs the \emph{abcast} guarantees G1 will not be met and therefore the broadcast cannot be considered to be atomic.  Furthermore, $N_si$ will reject $m$ when $m$ arrives after $m'$ has been delivered so that G4 is met.  
    
    A key advantage of the \textsf{Aramis} approach is that no message acknowledgements are required for a message to be delivered, instead it depends entirely on the calculated delivery delay $\Delta_m$.  Relying solely on $\Delta_m$ ensures that faulty nodes have no effect on the delivery of a message at correct nodes and it is therefore impossible for a message's delivery to become blocked.  Furthermore, as no quorums or acknowledgements are required, it is possible for  \textsf{Aramis} to tolerate at most $(n - 1)$ destination crashes when $n$ nodes are involved in an \emph{abcast}.  
    
    The \textsf{Aramis} protocol was developed to be risk adverse, with all probabilistic calculations carried out pessimistically in order to ensure that $\Delta_m$ is rarely exceeded.  Furthermore, $\Delta_m$ always assumes the worst case scenario will happen when the protocol is executing (e.g. the originator node crashing during broadcast) to ensure that such situations are catered for.  A consequence of this pessimism, is that the latency of a \emph{abcast} message can be very large, typically 100-1000ms.  Note that these potentially large latencies, though not desirable, do not undermine \textsf{Aramis} from offering high throughput.  
    
    \textbf{\textsf{Aramis} and \textsf{Base}: An Informal Description.} To counteract the large delivery latencies of \textsf{Aramis} it is necessary to operate a low-latency \emph{abcast} protocol, \textsf{Base}, alongside \textsf{Aramis}.  The \textsf{Base} protocol is a GM based deterministic protocol, similar to NewTop\citep{Ezhilchelvan:1995:NFG:876885.880005}, that provides low-latency high throughput \emph{abcast}s at the expense of blocking when node failures occur.  In the context of an \textsf{AmaaS} ordering service, \textsf{Base} works as follows: A message's orginator, say $m.o = N_si$, broadcasts $m$ to every $N_sj$, which in turn broadcasts an $ack_j(m)$ to every node in the service.  Once a $s$-node has received $ack_j(m)$ from all $N_sj$, $N_sj \neq m.o$, $m$ becomes deliverable.  Note that if one $N_sj$ crashes during the \emph{abcast}ing of $m$, the delivery of $m$ will be blocked if the crashed node had not sent $ack_j(m)$ before crashing and the protocol must wait for the GM service to detect the crash so that it can unblock $m$.   

    
    In order to hone the advantages of both protocols it was necessary to create the hybrid protocol \textsf{ABcast}, where an \textsf{abcast} $m$ becomes deliverable either when $\Delta_m$ has elapsed (\textsf{Aramis}) or when $ack_j(m)$ is received from every $N_sj$ (\textsf{Base}).  This approach provides the application with the low-latency of \textsf{Base} for the majority of message deliveries, whilst ensuring that a missing acknowledgement is not waited upon for more than $\Delta_m$ time.  In the event of a node failure the \textsf{Base} protocol does not have to wait for the GM service to detect a crash before message delivery becomes unblocked.  Instead, messages will be delivered by \textsf{Aramis} after $\Delta_m$ expires.  Therefore, when node failures are present the \textsf{ABcast} protocol will always allow for a greater throughput of delivered messages than a traditional GM based protocol, assuming that $\Delta_m$ remains smaller than the time it takes the GM to detect a node failure.  In the worse case, if the GM delay is smaller than $\Delta_m$, then the \textsf{Base} protocol can simply unblock its message buffer and continue to deliver messages without the use of \textsf{Aramis}.  Finally, in normal working conditions, the \textsf{ABcast} protocol should have similar performance to a traditional GM based protocol as, in the majority of cases, \textsf{Aramis} is not used for message delivery.  

As the \textsf{ABcast} protocol utilises the probabilistic protocol \textsf{Aramis}, it is possible for a node not to have received an \emph{abcast} $m$ at all when that node uses \textsf{Aramis} for delivery.  However, as previously stated, \textsf{Aramis} is carefully designed to keep the probability of meeting G1 and G2 close to 1.  Furthermore, as \textsf{Aramis} is only used when \textsf{Base} is slow or a node failures occurs, the probability of an \emph{abcast} message $m$ being missed in the total order is the product of two very small probabilities; \textsf{Base} not being able to deliver $m$ and \textsf{Aramis} failing $m$.  Therefore, in reality the occurrence of a node not delivering an \emph{abcast} $m$ is rare.   

    \subsection{ABcast Guarantees}
    Below, we state the guarantees provided by the \textsf{ABcast} protocol.  
    
    \begin{description}
    \item [\textbf{G1-P}] - \emph{Probabilistic Validity}: If the source of $m_i$ does not crash until it \emph{abcast}s $m_i$, then all operative destinations of $m_i$ deliver $m_i$ with probability $R$ which is user-defined and is close to $1$.  
    
    \item [\textbf{G2-P}] - \emph{Probabilistic Agreement}: If the source of $m_i$ crashes while \emph{abcast}ing $m_i$, and if any destination delivers $m_i$, then all operative
destinations of $m_i$ deliver $m_i$ with probability $R$.  
    \item [\textbf{G3}] - \emph{Uniform Integrity}: As defined in section \ref{sec:atomic_guarantees}.
    \item [\textbf{G4}] - \emph{Uniform Total Order}: As defined in section \ref{sec:atomic_guarantees}.
\end{description}

\newpage
\section{Assumptions}
    This section first defines the four key assumptions made when designing the \textsf{Aramis} protocol. 

    \subsection*{Assumptions:}  
    \begin{description} 
    % ******** Is this the case? Why?
        \item [\textbf{A1 - Fault Tolerance}] \hfill \\
        At most ($n-1$) of $n$ nodes involved in a broadcast can crash. However, 2 or more nodes cannot crash within an interval of some finite duration $\Delta_m$ that is smaller than a few seconds.
        
        \item [\textbf{A2 - Synchronised Clocks}] \hfill \\
        At any moment, clocks of any two operative nodes utilising \textsf{ABcast} are synchronised within $2\epsilon$ with a probability at least as large as $(1-10^{-5})$.
        
        We meet \textbf{A2} by implementing the well known probabilistic clock synchronisation algorithm \citep{Cristian:1996:SA:227210.227231}.  The details of our implementation and the parameters used are explored in $\S$ \ref{ssec:clocksynch}.       
        
        \item [\textbf{A3 - Reliable Communication}] \hfill \\
        When an operative node broadcasts $m$ to all $m.dst$, all operative destinations $d \in m.dst$ will eventually receive $m$.  
        
        We use reliable UDP protocol to guarantee that all operative nodes receive $m$ in crash-free scenarios.  However, when a broadcasting node crashes, the use of reliable UDP alone is not enough to ensure that all of the operative destinations receive $m$.  Therefore, a reliable broadcast, \emph{rbcast}, protocol will be required.  The Reliable UDP and \emph{rbcast} protocol we use are explored in detail in $\S$ \ref{ssec:reliable_udp} and $\S$ \ref{ssec:rbcast}, respectively.  
        
        \item [\textbf{A4 - Probabilistically Synchronous}] \hfill \\
        Let $x_{mx}$ be the maximum delay estimated at time $t$ by observing $NT_P$ transmissions in the recent past: The delay $x_{mx}$ will not be exceeded in any of $NT_F$, $NT_F \leq NT_P$, transmissions to unfold after $t$ with probability $(1 - q)$; where $q$ can be estimated with reasonable accuracy.  The measurement of $x_{mx}$ and $q$ are presented in section \ref{ssec:dmc}.  
        
        \textbf{A4} is motivated by previous research conducted by Ezhilchelvan \emph{et al.} \citep{Ezhilchelvan:2010:LPR:1773912.1773927} into PSM, which proposes that the challenges of designing asynchronous distributed systems, namely the FLP impossibility, can be avoided by assuming that the underlying network communication is synchronous to a given probability.  This assumption is crucial to \textsf{Aramis}'s efforts in minimising the probability of G1 and G2 not being met.  Informally, the larger the estimated $q$, the more intensive the efforts made by \textsf{Aramis} to preserve these guarantees and \emph{vice versa}.   
        
        A consequence of A4, is that \textsf{Aramis} is not suitable for use over the Internet, or similar networks that are susceptible to large fluctuations in network delays over a short period of time.  This is because frequent occurrences of such fluctuations in $NT_F$ can lead to $q$ being underestimated, \emph{i.e.} more violations of $x_{mx}$ occur than indicated by $q$.          
    \end{description}
    
\section{ABcast Components}
In this section we detail the individual components required by the \textsf{ABcast} protocol.  For each component, we explain its purpose and design; with important implementation details highlighted where appropriate.  All of the protocols presented in this thesis are implemented in Java using the JGroups framework.  

    \begin{figure}[!h] 
        \centering    
         \includegraphics[width=0.8\textwidth]{components_no_fcc}
         \caption[\textsf{ABcast} Protocol Components Overview]{\textsf{ABcast} Protocol Components}
         \label{fig:abcast_components}
    \end{figure}
    
   Figure \ref{fig:abcast_components} provides an overview of all of the components required by the \textsf{ABcast} protocol; where GM is the Group Membership service provided by JGroups, DMC is the Delay Measurement Component (\ref{ssec:dmc}) and \emph{rbcast} is the Reliable broadcast Component (\ref{ssec:rbcast}).  

    \subsection{Clock Synchronisation}\label{ssec:clocksynch}
    In order to provide synchronised clocks between nodes executing \textsf{ABcast}, we implemented the probabilistic clock synchronisation algorithm presented in \citep{Cristian:1996:SA:227210.227231} as a dedicated protocol in JGroups.  Cristian's algorithm is a master/slave protocol, that utilises a single master node's clock time to synchronise all of the slave nodes; with each slave periodically issuing a clock synchronisation request to the master in order to synchronise their clocks.  
            
            At any moment a slave's clock value is synchronised with the master node with a maximum error rate of $\epsilon$, with probability $\mathcal{P}_\epsilon \geq (1- 10^{-5})$. All of the experiments presented in this thesis utilise clock synchronisation with $\epsilon$ estimated as $1$ millisecond (ms).  A major consideration when estimating $\epsilon$ is the worst-case rate of clock drift between successive synchronisations. Ultimately, the longer the synchronisation interval, the larger the drift rate between clocks.  Estimation of $\epsilon = 1$ usually assumes an interval of $45$ minutes between synchronisations, however we use a shorter $15$ minute interval in order to increase $\mathcal{P}_\epsilon$.
            
            As each slave node synchronises its clock value with that of the master, it is possible for any two slave nodes to have a maximum error rate of $2\epsilon$.  This is because a slave $N_i$ could synchronise its clock behind the master's clock value by $\epsilon$ time.  Whereas, another slave $N_j$ could synchronise its clock ahead of the master by $\epsilon$. Hence, it is possible that $N_j.clockValue - N_i.clockValue = 2\epsilon$.  

    \subsection{Group Membership}\label{ssec:jgroups_gm}
    JGroups provides a GM service, called GMS which simply stands for Group Membership Service. GMS works as follows: upon discovering that a new node has joined the group or a node failure has occurred, GMS issues a new view to all of the protocols in the JGroups stack.  It is then the responsibility of the individual protocols to take the appropriate action when a new view is issued.  For example, unblocking message delivery if the local node was waiting for an acknowledgement from a node that is no longer present in the newly issued view.     
    
    \subsection{Reliable UDP}\label{ssec:reliable_udp}
    JGroups provides a reliable UDP protocol, \textsf{UNICAST3}, which guarantees that all UDP messages sent by a protocol higher in the network stack arrive at their destinations when node crashes do not occur.  This reliable UDP layer is placed below \textsf{ABcast} in the network stack to ensure that when messages are broadcast they are received by all destinations; where a broadcast consists of $m$ being unicast via \textsf{UNICAST3} to each of its intended recipients.  
    
    As well as providing reliable UDP unicasts, the \textsf{UNICAST3} protocol provides \emph{node-to-node} ordering as default for each message sent.  This ordering means that if a node $N_i$ sends two consecutive unicast messages, $m_1$ followed by $m_2$, to $N_j$, then $N_j$ will not deliver $m_2$ until it has first delivered $m_1$.  This behaviour is not always appropriate, therefore \textsf{UNICAST3} allows for messages to be sent Out-Of-Band (OOB), which simply means that messages will be sent reliably but they will be delivered at a destination as soon as they are received, regardless of the messages that have (or have not) been delivered before it.  Unless stated otherwise, our explanations assume that a unicast is sent using the default \textsf{UNICAST3} behaviour \emph{i.e.} not OOB.  
    
    \subsection{Reliable Broadcast}\label{ssec:rbcast}
    In the event of a node failure reliable UDP alone is not sufficient to ensure that assumption A3 holds.  This is because it is possible for a messages originator, $m.o$, to crash during the unicasting of $m$.  Assume that $m.dst = \{N_i, N_j, N_k\}$ and $m.o = N_i$, if $N_i$ crashes after unicasting $m$ to $N_j$ only, then $N_k$ will never receive $m$.  Similarly, if $N_i$ crashes during the unicasting of $m$ to $N_j$ it is possible that $N_i$ managed to send $m$ before crashing, in which case $m$ may eventually be received by $N_j$.  Both scenarios highlight that an additional protocol is required to ensure that all $m.dst$ receive $m$ in the event of $m.o$ crashing.  
    
    To overcome the limitations of Reliable UDP we have implemented a Reliable Broadcast protocol, called  \emph{rbcast}, that sits above the Reliable UDP layer in the network stack.  This protocol is inspired by the work of  Ezhilchelvan \emph{et al.} \citep{ezhilchelvan2011near}, as it utilises redundant broadcasts in collaboration with PSM, to ensure that all destinations receive a broadcast.  Our \emph{rbcast} protocol has been designed specifically for use with PSM based protocols and consequently utilises some of the values from the DMC ($\S$ \ref{ssec:dmc}) as protocol parameters.  
    
    Below, we state the guarantees provided by the \emph{rbcast} protocol.  In stating them, we assume two primitives $rbcast(m)$ and $rb.deliver(m)$, which are described in detail later on in this section.
    
    \subsubsection*{Probabilistic Guarantees of \emph{rbcast}}
    
        \begin{description}
		    \item [\textbf{RB1}] - \emph{Probabilistic RB-Validity}: If the source of $m$ does not crash until it completes \emph{rbcasting} $m$, then all operative $s$-nodes $rb.deliver(m)$ within a delay of at most $D$ with probability $R$.  
		    
		    \item [\textbf{RB2}] - \emph{Probabilistic Agreement}: If $m.o$ crashes during $rbcast(m)$ and if any $s$-node $rb.delivers$ $m$, then all operative $s$-nodes $rb.deliver$ $m$ within $D$ of $rbcast(m)$ with probability $R$.  
		
		
		    \item [\textbf{RB3}] - \emph{Uniform Integrity}: Any $s$-node $rb.delivers$ $m$ at most once.  
\end{description}
    
    RB1, RB2 and RB3 are directly used when implementing \textsf{Aramis}, to provide G1-P, G2-P and G3, as defined in section \ref{ssec:aramis}.  The remainder of this section describes the basic \emph{rbcast} protocol, whilst the calculations of $D$ are presented in $\S$ \ref{ssec:dmc}.
    
    \subsubsection*{The \emph{rbcast} protocol}
    All messages broadcast via \emph{rbcast} include a tuple $\{m.o, m.seq\#, m.ts\}$ that uniquely identifies the broadcast.  Where $m.o$, short for message originator, is the address of the node that initiates a broadcast message; $m.seq\#$ is a sequence number unique to each $m.o$ that is incremented after each broadcast and $m.ts$ is a timestamp of $m.o$'s synchronised clock.  Note that the first two values of the tuple are sufficient to uniquely identify a broadcast.
    
    The \emph{rbcast} protocol supports two primitives: $rbcast(m)$ and $rb.deliver(m)$.  The protocol uses a set of parameters provided by the DMC, these are:
    
    \begin{description}[leftmargin=1cm, labelindent=1cm]
        \item[$\bm{x_{mx}}$ \textnormal{and}  $\bm{q}$ - ]    As described in our assumptions.
        
        \item[$\bm{\rho}$ - ]    The number of redundant broadcasts required, where $\rho \geq 1$.
        
        \item[$\bm{\eta}$ - ]    The delay observed between redundant broadcasts of $m$, to ensure successive broadcasts are independent.  
        
        \item[$\bm{\omega}$ - ]    A node's estimate of the networks Packet Delay Variation (PDV).  
        \end{description}
    
    \subsubsection*{$\bm{rbcast(m)}$}
    The primitive $rbcast(m)$ works as follows:
    \begin{enumerate}[label=\roman*]
        \item    The message to be broadcast, $m$, is created with the parameters defined above added as fields \emph{e.g.} $m.\rho, m.\eta, \ldots$ 
        
        \item    The $m$ to be $rbcast(m)$ is then broadcast redundantly $(\rho + 1)$ times, with each successive transmission occurring $\eta$ time after the previous one.  
        
        \item    Each succesive transmission of $m$ is identified by the filed $m.copy$, where $m.copy = 0, \ldots, \rho$.  
    \end{enumerate}
    
     \subsubsection*{$\bm{rb.deliver(m)}$}
     As soon as a copy of $m$ is received by a node, it is delivered up the stack to the higher level protocol (\textsf{ABcast}).  \\
     
    So, a $rbcast(m)$ is considered a success if every operative $d \in m.dst$ performs $rb.deliver(m)$.  Figure \ref{fig:rbcast} shows the \emph{rbcast} primitives and their relationship with the DMC and the underlying reliable UDP layer.  
    
    \begin{figure}[!h] 
        \centering    
         \includegraphics[width=1\textwidth]{rbcast}
         \caption[Reliable Broadcast Interactions]{Reliable Broadcast Interactions}
         \label{fig:rbcast}
    \end{figure}
    
    After $rb.deliver(m)$ has been executed, any destination $N_sj$ that receives $m.copy = k < \rho$ cooperates to ensure this success.  A destination waits to receive $m.copy \geq k + 1 \geq$ within a timeout $t_1 = \eta + \omega$, where $\eta= m.\eta$ and $\omega=m.\omega$.  If $t_1$ expires, $N_sj$ assumes that $N_si$ has crashed while broadcasting $m.copy = k$ and starts broadcasting $m.copy = k, k+1,\ldots, \rho$ on behalf of $N_si$. Note, $N_sj$ rebroadcasts the same copy of $m$ that it received, $m.copy = k$, as it does not know if all other $d \in m.dst$ have also received $m.copy = k$.  To reduce the possibility that no one else is acting on behalf of $N_si$, $N_sj$ adds a further random wait, $\zeta$, which is uniformly distributed on ($0,\eta$), before broadcasting $m.copy = k$.  This process continues until all $d \in m.dst$ have received or broadcast $m$ with $m.copy = \rho$.  
    
    Note that if $N_i$ does not crash, or if it crashes and an operative $s$-node receives $m$, then $m$ is broadcast \emph{at least} ($\rho + 1$) times.  Conversely, if $N_i$ crashes during the initial broadcast of $m$, and no members of $m.dst - \{m.o\}$ receive a copy of $m$, then the broadcast has failed and $m$ is lost.  This is acceptable for \textsf{ABcast}, because if no $d \in m.dst - \{m.o\}$ receive $m$, then its not possible for any node to $rb.deliver(m)$, therefore it is not possible for \emph{abcast} guarantees G1 or G2 to be violated.  
    
    
    \subsubsection*{Implementation Optimisation}
    It is worth noting that in our implementations of \emph{rbcast}, every broadcast where $m.copy = 0$ is treated differently to subsequent broadcasts of $m$.  Copy 0 of $m$ is broadcast to all destinations using the default settings of Reliable UDP, i.e. messages are delivered in the same order that they were originally unicast from their source address.  This means that if node $N_si$ broadcasts $m$, followed by $m'$, it is not possible for any of the destinations to receive copy $0$ of $m'$ before it has received $m$.  This also implies that if the transmission of copy $0$ of $m$ is slow or becomes lost, than copy $0$ of $m'$ cannot be forwarded up the network stack to \emph{rbcast} protocol until $m$ has been received.  To overcome this issue all messages with $m.copy > 0$ are sent OOB to ensure that they are forwarded to the \emph{rbcast} protocol as soon as they are received at the destination node.  Therefore, if $m'.copy = 0$ has been received but not been forwarded to \emph{rbcast}, then $m'.copy = 1$ is forwarded as soon as it arrives, bypassing the backlog of messages.  
    
    The calculations used to produce $x_{mx}, \eta, \rho$ and $\omega$ are discussed in detail in $\S$ \ref{ssec:dmc}.  
    
    \subsection{Delay Measurement Component}\label{ssec:dmc}
        For the sake of clarity, assumption A4 is repeated below:   
        
        \begin{quotation}
            Let $x_{mx}$ be the maximum delay estimated at time $t$ by observing $NT_P$ transmissions in the recent past: The delay $x_{mx}$ will not be exceeded in any of $NT_F$, $NT_F \leq NT_P$, transmissions to unfold after $t$ with probability $(1 - q)$; where $q$ can be estimated with reasonable accuracy.  
        \end{quotation}
    
        The delay measurement component is responsible for monitoring and observing the network latency of $NT_P$ transmissions from the recent past.  These latencies are then used to calculate various parameters that are required by \emph{rbcast} (and by \textsf{Aramis}) for executing \emph{abcasts} in the near future.  Being conservative, we use $NT_F = 10\%$ of $NT_P$ and $NT_P=1000$; so, a \textsf{ABcast} node freshly estimates $x_{mx}$ for every 100 new delays it observes.  Each fresh estimation of $x_{mx}$ results in the recalculation of the following parameters: $\eta, \rho, q$ and $\omega$.  
        
        Latencies are measured by the DMC based upon the timestamp $m.ts$, which is included in every message $m$ that is broadcast via the \emph{rbcast} protocol.  As the clocks of all nodes executing the \textsf{ABcast} protocol are synchronised, it is possible to measure the one-way latency of each message that is received by a node.  For example, a node $N_i$ sends an \emph{rbcast} $m$ to $N_j$, upon receiving $m$, $N_j$ immediately records the latency $x$:

        \begin{equation}
             \begin{aligned}
                 x = (N_j.clockValue - m.ts) + 2\epsilon
             \end{aligned}
        \end{equation}        
        
        It is necessary to add $2\epsilon$ to each latency to ensure that if $N_j.clockValue$ is behind $N_i.clockValue$ by the maximum error of $2\epsilon$, a positive latency value is still recorded.  
                                
        The remainder of this section explores each of the parameters provided by the DMC, explaining what they represent and how they are calculated.  
        
        \begin{description}
        \item[\Huge$\boldsymbol{x_{mx}}$] \hfill \\
        $x_{mx}$ is simply the largest latency out of the $NT_P$ latencies observed in the recent past.  
        
        \item[\Huge$\boldsymbol{\rho}$] \hfill \\
        $\rho$ is the parameter that determines the number of redundant broadcasts sent by the \emph{rbcast} protocol, with a given $m$ being broadcast $\rho + 1$ times.  To determine $\rho$, we define $R$ as the probability that at least one of $(\rho + 1)$ transmissions of $m$ by $m.o$ reaches all operative destinations within $x_{mx}$; where $R$ is a configuration parameter specified before runtime.  
        
        The probability that a given operative destination receives at least one of $(\rho + 1)$ transmissions from $m.o$, is:
        
        \begin{equation}
            \begin{aligned}
                1 - q^{\left( \rho + 1 \right)}
            \end{aligned}
        \end{equation}
        
        Given that there can be $(n - 1)$ operative destinations at any time, we require:
        
        \begin{equation}
            \begin{aligned}
                \left[1 - q^{\left(\rho + 1\right)}\right]^{\left(n - 1\right)} \quad > \quad R
            \end{aligned}
        \end{equation}
        
        \begin{equation} \label{eq:q_mx}
            \begin{aligned}
                \left[1 - q^{\left(\rho + 1\right)}\right]^{\left(n - 1\right)} \quad > \quad R ^{\left(\frac{1}{n - 1}\right)} \quad = \quad \tilde{R}
            \end{aligned}
        \end{equation}
        
        Rearranging, taking in both sides and accounting for the fact that $\ln(a) < 0$ for  $0 < a < 1$, we set:
        
        \begin{equation}
            \begin{aligned}
                \left(\rho + 1\right) \quad > \quad \frac{\ln\left( 1 - \tilde{R} \right)}{\ln\left(q \right)}, \quad \forall \quad q \neq 1
            \end{aligned}
        \end{equation}

        \emph{i.e}:        
        
        \begin{equation} \label{eq:rho_ie}
            \begin{aligned}
                \rho \quad > \quad \frac{\ln\left(1 - \tilde{R}\right)}{\ln(q)} - 1, \quad \forall \quad q < 1
            \end{aligned}
        \end{equation}
        
        Later, we show that $\rho$ needs to be at least one to tolerate the crash of $m.o$.  So, $\rho$ is the smallest integer that satisfies:
        
        \begin{equation}
            \begin{aligned}
                \rho \quad > \quad maximum \left\{ \rho_{min}, \quad \frac{\ln\left(1 - \tilde{R}\right)}{\ln(q)} - 1 \right\}
            \end{aligned}
        \end{equation}
        
        Where $\rho_{min} \geq 1$ is a configuration parameter specified before runtime.  Observe that, for a given $R$, an integer $I =\rho_{min}, \rho_{min} + 1, \ldots$, satisfies: 

        \begin{equation}
            \begin{aligned}
                I \quad < \quad  \frac{ln \left(1 - \tilde{R}\right)}{ln(q)} - 1 \quad < \quad I + 1
            \end{aligned}
        \end{equation}
        
        for a wide range of $q$ values; e.g., for $R \approx 0.9999$ and $n = \ldots$

        \begin{equation}
            \begin{aligned}
                \frac{ln(1- \tilde{R})}{ln(q)}-1 < 1 \quad \forall \quad q < 0.01 = 1\%
            \end{aligned}
        \end{equation}

        This implies that small inaccuracies in estimating $q$ may not adversely affect $\rho$ estimates.  

        \item[\Huge$\boldsymbol{q}$] \hfill \\
                The parameter $q$ is the estimated probability that a transmission delay observed in the near future will exceed $x_{mx}$.  We estimate $q$ by assuming that each $x$ in $NT_p$ increases by $5\%$ in the near future.  So, the estimated set of transmission delays (with regards to $x_{mx}$) in the near future is:
                
        \begin{equation}
            \begin{aligned}
                \left\{ x \text{ in } NT_p \quad  | \quad 1.05 \times x \quad > \quad x_{mx} \right\}
            \end{aligned}
        \end{equation}
                
        Note that:
         \begin{equation}
            \begin{aligned}
                1.05 \times x \quad > \quad x_{mx} \quad \Rightarrow \quad \frac{x_{mx}}{1 + 0.05 } \approx 0.95 x_{mx}
            \end{aligned}
        \end{equation}
                
        So,
            \begin{equation}
            \begin{aligned}
                q  = \frac{\text{Number of Transmissions in } NT_p \text{ that exceeds } 95\% x_{mx}}{|NT_P|}
            \end{aligned}
        \end{equation}
        
                It is possible that $q$ is calculated close to $1$ if many latencies in the recent past are within $95\%$ of $x_{mx}$.  As $q -> 1, \rho -> \infty$ from equation \ref{eq:rho_ie}.  To deal with such extreme cases, we fix an upper bound $\rho_{mx}$, $\rho_{mx} > \rho_{mn}$.  We use $\rho_{mx}$ in equation \ref{eq:q_mx} and find $q$ to be the largest value that satisfies:
                
       \begin{equation}
            \begin{aligned}
                q \quad < \quad \left(1 - \tilde{R}\right)^{\left(\frac{1}{\rho_{mx} + 1} \right)}
            \end{aligned}
        \end{equation}

        \item[\Huge$\boldsymbol{\eta}$] \hfill \\
        $\eta$ is the parameter used by \emph{rbcast} to determine the amount of time to wait between each broadcast of a message copy and is calculated as the largest delay in $(n - 1)$ transmissions (of a given copy $m$) with probability $R$.  Assuming exponential distribution $n$ is calculated as follows:
        
        \begin{equation}
            \begin{aligned}
                \eta=-\bar{x}[ln(1-\tilde{R})]  
            \end{aligned}
        \end{equation}
        
Where $\bar{x}$ is the exponential mean of $NT_P$ observed delays.

        \item[\Huge$\boldsymbol{\omega}$] \hfill \\
        $\omega$ is the parameter utilised by \emph{rbcast} to approximate the PDV encountered by the network.  $\omega$ is simply calculated as:
        
        \begin{equation}
            \begin{aligned}
                \omega = \eta - \bar{x}
            \end{aligned}
        \end{equation}        
        
        Again, we assume exponential distribution and that $\bar{x}$ is the exponential mean of $NT_P$ observed delays.
        \end{description}

        \subsection*{Calculating \emph{rbcast} Guarantees}
        For simplicity, let us consider a single $m$ \emph{rbcast} by $s$-node $N_i$ as per its clock time $ts$; we will also observe time as per the clocks of $N_i$ (which is assumed to function correctly even when $N_i$ is crashed).  Let $D = x_{mx} + 2\eta + \omega + D_1$ and $D_1 = \rho\eta + x_{mx}$; Let $x$, as before, denote a delay.  

        \begin{figure}[hb]
                \centering    
                \centerline{\includegraphics[width=1.15\textwidth, trim=0.1cm .25cm .1cm .25cm, clip=true]{rbcast_calculations}}
                \caption[Rbcast Calculations Diagram In a Crash Free Scenario]{Rbcast Calculations In a Crash Free Scenario}
                \label{fig:rbcast_calc}
            \end{figure}    

        \subsubsection*{RB1 - Probabilistic RB-Validity}
        When $N_i$ does not crash until it completes $rbcast(m)$, it carries out $(\rho + 1)$ broadcasts at time $ts, (ts + \eta) \ldots (ts + \rho\eta)$.
        
        Let $g_{D_1}$ be the probability that a destination, $N_j$, does not receive a single copy of $m$ at or before $ts + D_1$.  Suppose that $\mathcal{P}(x > \xi)$ is the probability that a copy of $m$ takes longer than $\xi$ time to reach $N_{j}$.  The probability that none of the copies of $m$ broadcast at $ts, ts+\eta, \ldots,  ts+\rho \eta$, reaches $N_{j}$ by time $ts+D_1$, is:
        
        \begin{equation}
            \begin{aligned}
                g_{D_1}= \mathcal{P}(x > D_1) \times \mathcal{P}(x > D_1 - \eta) \times \ldots \mathcal{P}(x > D_1 - \rho \eta)
            \end{aligned}
        \end{equation}

        
        Recall that $q$ is the probability that a delay in $NT_F=100$ future transmissions exceed the maximum $x_{mx}$ observed in the past $NT_P=1000$ transmissions. Since $\mathcal{P}(x > \xi)$ decreases as $\xi$ increases, we have:
        
        \begin{equation}
            \begin{aligned}
                \mathcal{P}(x > D_1 - \rho \eta) \geq \mathcal{P}(x > x_{mx}) = q
            \end{aligned}
        \end{equation}
        
    and

        \begin{equation}
            \begin{aligned}
                g_{D_1} < q^{(\rho + 1)}
            \end{aligned}
        \end{equation}        
        
    So, the probability that all operative $s$-nodes receive a $m$ at or before time $ts + D$ is:        

        \begin{equation}\label{eq:rb_validity}
            \begin{aligned}                
                \left[1 - g_{D_1}\right] ^ {(n - 1)} > \left[1 - q ^ {(\rho + 1)} \right] ^{(n - 1)}
            \end{aligned}
        \end{equation}
        
        Recall that $\left[1 - q ^ {(\rho + 1)} \right] ^{(n - 1)} > R$ is used to estimate $\rho$ when $q \leq 1$.  Therefore, the probability that RB1 is met for $D > D_1$ is larger than the user specified $R$.  In fact, the power $(\rho + 1)$ in \ref{eq:rb_validity} is a lower bound and assumes that no destination timesout and acts on behalf of $N_i$.  
        
        Figure \ref{fig:rbcast_calc} shows the workflow of a message $m$ being \emph{rbcast} to $m.dst = \{N_i, N_j, N_k\}$ in a crash free scenario with $\rho = 2$; the individual parameters of $D_1$ are illustrated to show their significance.  
                
        \clearpage
        \subsubsection*{RB2 - Probabilistic Agreement}
Suppose now that $N_i$ crashes before completing the redundant transmissions of $m$ and $n>2$.  Suppose also that only one node, $N_j$, has $m$ with $m.copy=0$. This is the worst case to be considered because if $N_j$ $m.copy>0$, then $N_i$ crashed only after it completed broadcasting $m.copy \geq 0$, therefore some node other than $N_j$ also has $m$; the more destinations which receive some copy of $m$, the more likely it is that $rbcast(m)$ is completed.  On the other hand, if no destination receives any copy of $m$ from the crashed $N_i$, then the case for discussion does not exist.  

If copy $m.copy = 0$ takes at most $x_{mx}$ to reach $N_j$ (which occurs with probability ($1-q$)), $N_j$ would start disseminating on behalf of $N_i$ at or before time:
        
        \begin{equation}
            \begin{aligned}
                ts+ x_{mx} + \eta + \omega +\zeta
            \end{aligned}
        \end{equation}
        
Recall that $\zeta$ is the random wait that all disseminating nodes must observe before disseminating $m$, with $\zeta$ uniformly distributed on ($0, \eta$) .  Therefore, assuming that the observed $\zeta$ is the largest possible value possible, $\eta$, let us define: 

        \begin{equation}
            \begin{aligned}
                D = x_{mx} + 2\eta + \omega + D_1
            \end{aligned}
        \end{equation}
        
By assumption \textbf{A1}, no other node can crash until at least $ts+D$. Thus, $N_j$ disseminates $m$, like $N_i$ in the crash-free case, to operative destinations which can now be at most $(n-2)$; so the approximate probability that all operative nodes receive $m$ at or before $ts +D$ is:

%                (1-Q) = (1-g_{D_1})^{n-2}\times (1-q)>(1-q^{\rho+1})^{n-2}\times(1-q)
        
        \begin{equation}
            \begin{aligned}
                \left[1-Q\right] &= \left[1 - g_{D_1}\right] ^{(n - 2)} \times \left[1-q\right] > \left[1-q ^ {(\rho + 1)}\right] ^ {(n - 2)} \times \left[1-q\right] \\[2em]
                & = \frac{1 - q}{1-q^{(\rho + 1)}} \times \left[1 - q ^ {(\rho + 1)}\right] ^ {(n - 1)}
            \end{aligned}
        \end{equation}

Figure \ref{fig:rbcast_crash_calc} shows the workflow of a message $m$ being \emph{rbcast} by $N_i$ to $m.dst \{N_i, N_j, N_k\}$.  $N_i$ crashes after unicasting $m$ to $N_j$, therefore $N_j$ waits to receive $m.copy > 0$ from $N_i$ for $2\eta + \omega$ time, before disseminating $m$ to the remaining destination $N_k$.  

In the event that $n = 2$, only two scenarios are possible in the event of $N_i$ crashing during the original broadcast of $m$.  Either $m.copy = 0$ is never received by the other node $N_j$, in which case $m$ is lost forever; Or $m.copy \geq 0$ is received by $N_j$, in which case $N_j$ will continue to broadcast the remaining copies of $m$ for completeness, however no other node will receive $m$ as no operative nodes remain.  For both scenarios, the delivery delay is calculated as $D_1$ as dissemination is not possible, therefore the probability that all operative nodes receive $m$ at or before time $ts+D_1$ is $(1-Q_1)$.  

To put the two cases together, $n > 2$ and $n = 2$, let boolean $\beta = 1$ if $n>2$ and $0$ if $n=2$; further, let: 

        \begin{equation}
            \begin{aligned}
                D=\beta\times(x_{mx}+2\eta+\omega)+D_1
            \end{aligned}
        \end{equation}
        
When $N_i$ \emph{rbcast}s $m$ at its clock time $ts$, if some operative $s$-node receives $m$, then every operative $s$-node receives $m$ at or before time $m.ts+D$ (as per $N_i$'s clock) with a probability:

         \begin{equation}
            \begin{aligned}
                \beta\times(1-Q)+(1-\beta)\times(1-Q_1)
            \end{aligned}
        \end{equation}

        \begin{sidewaysfigure}
            \vspace*{-1cm}
            \strictpagecheck
            \checkoddpage
            \ifoddpage
                \hspace*{-2.5cm}
            \fi           
            \centering
                \includegraphics[width=1.1\textwidth]{rbcast_crash_calculations}
            \caption[Rbcast Calculation Diagram with a Crashed Message Originator]{Rbcast Calculations with a Crashed Message Originator}
            \label{fig:rbcast_crash_calc}
       \end{sidewaysfigure}
                
\clearpage
\section{Atomic Broadcast Protocol}\label{sec:ABcast}
Hitherto this chapter has focused on the underlying assumptions made when designing the \textsf{ABcast} protocol and the components it requires to function.  This section focuses on how the hybrid protocol functions, detailing the specifics of both the \textsf{Aramis} and \textsf{Base} protocols.  First we explore the \textsf{Base} protocol, as this protocol will be responsible for the majority of message deliveries and is the more conventional of the two \emph{abcast} protocols.  We then explore the \textsf{Aramis} protocol, detailing how messages are delivered and rejected as well as other related properties.  

    \subsection{Base}
    The \textsf{Base} protocol is based upon the NewTop \citep{Ezhilchelvan:1995:NFG:876885.880005} algorithm discussed in $\S$ \ref{ssec:newtop}, with a few key differences motivated by our use of synchronised clocks and PSM.  
    
    The \textsf{Base} protocol works as follows when a node $N_i$ sends an \emph{abcast} $m$: $N_i$ \emph{rbcast}s $m$, and as per the \emph{rbcast} protocol $m$ is assigned an id tuple $\{m.o, m.seq\#, m.ts\}$.  This tuple is utilised by \textsf{Base} to specify the total order of $m$, with all destinations in $m.dst$ ordering messages in ascending order based upon their timestamp.  In the event of any two messages having the same $m.ts$ value, the address specified in $m.o$ is used for tie-breaking to ensure a total order; note this is highly unlikely in practice as $m.ts$ is recorded in nanoseconds.  Similarly, In our implementation it is not possible for the same node to \emph{rbcast} two messages with the same $m.ts$, as a single thread called the \emph{sender} thread, is used for sending all $m$ with $m.copy = 0$.  Like NewTop, delivery of $m$ is blocked until each $d' \in m.dst - \{m.o\}$ has acknowledged $m$ by sending $ack_{d'}(m)$ to every $d \in m.dst$ and all $d \in m.dst$ have received $ack_{d'}(m) \forall (m.dst \setminus \{m.o,d\})$, thus C1 ($\S$ \ref{ssec:atomic_broadcast}) is met.  With each acknowledgement consisting of the id tuple that belongs to the message being acknowledged.  
    
    The use of synchronised clocks to uniquely timestamp each message, removes the need for tentative timestamps to be shared between destinations.  Instead the ordering of a message is dictated from its inception based upon its timestamp.  However,  a message's final place in the total order is not known by a destination until it has received an acknowledgement from all other $d' \in (m.dst - \{m.o\})$.  
    
    In order to overcome C2, as described in \ref{sec:atomic_guarantees}, and ensure that G3 and G4 are respected, it is necessary for each $d \in m.dst$ to maintain a vector clock \citep{Mattern88virtualtime, fidge1988timestamps}; with each $d$'s vector clock stating the last $rbcast(m)$ sent by $d$ as well as the the latest $m$ to be $rb.delivered$, by $d$, that originated from each $d' \in (m.dst - d)$ \footnote{Where the latest $m$, is defined as the message containing the largest timestamp from a given $d'$}.  This vector clock is then included in every $m$ and $ack_{d'}(m)$ sent by a node.  
    
    For any message $m$, regardless of whether it originated at the current node, it is necessary for the associated acknowledgements and vector clocks to be checked to see if a message is missing from the total order before $m$ can be delivered.  If a node $N_i$ has not received $m$, but has learnt of its existence via a \emph{vector clock} or an acknowledgement, then we consider $m$ to be \emph{known} by $N_i$.  The \textsf{Base} delivery conditions are formalised below:
    
    \paragraph{\textsf{Base} Delivery Rule:}\hspace{0pt} \\
        A node, $N_j$ delivers any $m$, via \textsf{Base}, only after $D1_B$ and $D2$ stated below are satisfied:
        \begin{description}[labelindent=1cm]
            \item[$\boldsymbol{D1_B}$] - $m$ is acknowledged by all nodes other than $m.o$. 
            \item[$\boldsymbol{D2}$] - all \emph{known} $m'$, with $m'.ts < m.ts$ have been delivered.
        \end{description}
    
    \subsubsection*{Acknowledgement Piggybacking}\label{ssec:base_ack_piggyback}
    In the explanation of \textsf{Base} we assume that each acknowledgement is explicitly sent as dedicated message, however in practice this is an expensive operation in terms of both latency and bandwidth.  Therefore in our implementation of \textsf{Base} we piggyback message acknowledgements on subsequent \emph{rbcast}s sent by an the acknowledging node; acknowledgements are piggybacked onto all copies of $m$, i.e. all $m.copy =0,\ldots,\rho$, or not at all.  Of course this is only appropriate if there is a message waiting to be \emph{rbcast}, otherwise deadlock will occur at all $d \in m.dst$ as an acknowledgement will never be sent.  Consider, node $N_j$ is attempting to send $ack_{N_j}(m)$ to $N_i$, if $N_j$ does not receive an \emph{abcast} request within $\mathcal{A}_d$ time, then an explicit acknowledgement message is sent to $N_i$ containing $ack_{N_j}(m)$, as well as any other pending acknowledgements.  We define $\mathcal{A}_d$ as $\mathcal{A}_d = 2\eta + \omega$ and an explicit acknowledgement as being a dedicated message $m_{ack}$ that is unicast to all $d \in m.dst$ and is not assigned a \emph{rbcast} id; hence only a single copy of $m_{ack}$ is broadcast via reliable UDP.  \footnote{Explicit acknowledgements are not \emph{rbcast} in order to further minimise the bandwidth cost of sending $m_{ack}$.}
    
%    \subsubsection*{Aborting rho > 0}
%    Another possible optimisation is for the redundant multicasting of $rbcast(m)$, to be aborted once $m$ is known to have been acknowledged by all destinations; this includes destinations that are disseminating $m$ or waiting on $m.copy > 0$.  The logic for this optimisation is as follows: If a node $N_i$ has delivered $m$ locally, then we know that all other $d' \in m.dst$ must have received $m$ as $N_i$ must have received $ack_{d'}(m)$ from all $d' \in m.dst$ in order for $m$ to have been delivered locally.  Therefore it is safe to abandon the \emph{rbcast}ing of $m$ as all $m.dst$ have received at least one copy of $m$. 
%    
%    Note, this optimisation has \textbf{not} been utilised in our implementation of the \textsf{ABcast} protocol.  This optimisation is not possible if a message has been delivered locally by the \textsf{Aramis} protocol as it is impossible to know for certain that all $d' \in m.dst$ have received $m$.  

    \subsection{Aramis}\label{ssec:aramis}
    As previously stated,  \textsf{Aramis} is a non-blocking probabilistic \emph{abcast} protocol that utilises a calculated delivery delay $\Delta_m$ to place an upper bound on message deliveries.  The \textsf{Aramis} protocol works in conjunction with \textsf{Base} to ensure that message delivery does not become blocked in the event of slow or crashed nodes.  However, the \textsf{Aramis} protocol does not simply deliver each received message after $\Delta_m$ has expired, as this could cause a \emph{known} message to be missed in the total order.  Instead, it utilises the acknowledgements and \emph{vector clocks} that are integral to \textsf{Base} to ensure that \emph{known} messages are not missed from the total order.  Therefore, if a message $m$'s $\Delta_m$ delay expires, the message can only be delivered after all \emph{known} messages that precede $m$ in the total order have been delivered.  The \textsf{Aramis} delivery conditions are formalised below:
    
    \paragraph{\textsf{Aramis} Delivery Rule:}\hspace{0pt} \\
        A node, $N_j$ delivers any $m$, via \textsf{Aramis}, only after $D1_A$ and $D2$ stated below are satisfied:
        \begin{description}[labelindent=1cm]
            \item[$\boldsymbol{D1_A}$] - The clock of $N_i > m.ts + \Delta_{m}$, where $\Delta_{m} = 2(D + \epsilon) + \mathcal{A}_d$.
            
            \item[$\boldsymbol{D2}$] - All \emph{known} $m'$, with $m'.ts < m.ts$ have been delivered.
        \end{description}
    
    
        \subsubsection*{Calculating $\boldsymbol{\Delta_m}$}
        Let us, for simplicity, assume that $\epsilon = 0$.  The explanation is twofold.  First, recall that \emph{rbcast} guarantees on agreement: if $N_i$ \emph{rbcast}s $m$ and if any operative $s$-node (be it $N_i$ or otherwise) $rb.delivers$ $m$, all destination nodes $rb.delivers$ $m$ before $m.ts + D$ with probability $R$.  
        
        Secondly, the aim of \textsf{Aramis} to aid delivery of \emph{abcast} messages when \textsf{Base} is blocked due to node crashes.  So, delivery by \textsf{Aramis} is delayed until all acknowledgements are $rb.delivered$ when there are no crashes.  Since an operative node can acknowledge at most $\mathcal{A}_d$ time after $rb.delivering$ $m$, we have:
        
        \begin{equation}
            \begin{aligned}
                \Delta_m \geq D + \mathcal{A}_d + D
            \end{aligned}
        \end{equation}
        
        Accounting for the clock synchronisation error rate of $2\epsilon$, we calculate $\Delta_m$ as:
        
        \begin{equation}
            \begin{aligned}
                \Delta_m = 2(D + \epsilon) +  \mathcal{A}_d
            \end{aligned}
        \end{equation}
        
        \textbf{Note:} By delivering $m$ at $m.ts + \Delta_m$, $\Delta_m > 2 \times D$, \textsf{Aramis} indeed meets  the validity (G1-P) and agreement (G2-P) guarantees with a probability much larger than the user specified value of $R$.  
                
        \subsubsection*{Total Order Violations}\label{ssec:abcast_rejection}
        In the explanations above, we have only considered the conditions required for a message to be successfully \emph{abcast} to all nodes.  However, as \textsf{Aramis} is a probabilistic protocol it is possible for a message to be delivered in the wrong place in the total order, thus violating G4.  For example, a message $m$ arrives at a destination after its proceeding message $m'$ in the total order has already been delivered, such that the total order is $<m', m>$ when it should be $<m, m'>$.  In such a case, their are two course of action possible when $m$ arrives: 1) Deliver $m$ as soon as possible; resulting in two violations of the total order occurring as $m$ has missed its place and then taken another's in the total order.  2) Receive $m$ and throw an exception to the application; resulting in only one violation of the total order, as $m$ is simply missing, but at the expense of violating G1 and G2 as $m$ cannot be delivered by all destinations.  \footnote{Assumption A2 guarantees that all destinations will eventually receive $m$, so if a message is not rejected G1 and G2 always hold.}
        
        \textsf{Aramis} seeks to minimise the effects of a miss-ordered message, therefore it employs the second approach and throws an exception when a message has been missed in the total order; we refer to this process as a message being \emph{rejected}.  Explicitly \emph{rejecting} a miss-ordered message from the total order allows for higher levels in the network stack (e.g. \textsf{SCast}) to initiate an appropriate recovery mechanism to mitigate the effects of a miss-ordered message on the system's state.  Furtheremore, it is possible to include the contents of the rejected message in the \emph{rejection} exception in order to provide potential useful data to the application's recovery mechanism.  
        
        The remainder of this section focuses on the scenarios in which it is possible for a message to be \emph{rejected} by a receiving node.  When the number of nodes, $n$, involved in an \emph{abcast} is $n=2$ the conditions required for a message to be rejected are different to $n > 2$, therefore we explore $n=2$ and $n>2$ in isolation. Note, for both $n=2$ and $n>2$ it is not possible for a broadcasting node to reject its own message, as by definition a broadcasting node will always receive its own message, hence G2 is always violated if a single node \emph{rejects} a message.  
        
        %\paragraph{Assumptions:}\hspace{0pt} \\
        \subsubsection*{Message Rejections - $\boldsymbol{n = 2}$}
        A message, $m$, sent by node $N_i$, can only be \emph{rejected} by the other recipient, $N_j$, when a message $m'$ from $N_j$ has been incorrectly delivered before $m$ in the total order; where $m.ts < m'.ts$ but the total order delivers $m'$ first, hence $m$ is \emph{rejected}.  In order for a message to be miss-ordered, both of the two statements below must be true:
        
        \begin{enumerate}
            \item $N_j$ broadcasts $m'$ with $m'.ts > m.ts$ and $m$ does not arrive at $N_j$ before $m'.ts + \Delta_{m'}$.  
            \item $N_j$ does not receive an acknowledgement for $m$, or any subsequent broadcasts sent by $N_i$, before $m'.ts + \Delta_m'$.   
        \end{enumerate}
        
        If condition one is not true, i.e. $m$ arrives before $m'.ts + \Delta_{m'}$, then $N_j$ has received $m$ before $m'$'s delivery time and therefore $N_j$ will not miss $m$ in the total order.  Similarly, if condition two is not true, then $N_j$ will know that a message sent by $N_i$ is missing as soon as it inspects the received acknowledgement, attached \emph{vector clock} or the $seq\#$ included in the id of the received message.  For both conditions to hold, it is necessary for $m$ and $m'$ to be delivered via the \textsf{Aramis} protocol at all nodes.  This is because the \textsf{Base} protocol relies on acknowledgments to deliver messages, therefore if \textsf{Base} delivers a message condition two cannot be true.  
        
                \subsubsection*{Message Rejections - $\boldsymbol{n > 2}$}
        If we assume the same scenario as described in the previous section ($n=2$), but with the addition of a node $N_k$ that represents all other nodes that are not $N_i$ or $N_j$.  A message is miss-ordered if conditions $1$ and $2$ stated above are true, and one of the following is true:
        
        \begin{itemize}
            \item[3a.] $N_j$ and all $N_k$ do not receive $m$ before $m'.ts + \Delta_{m'}$.
            
            \item[3b.] $N_j$ does not receive an acknowledgement for $m$ from any $N_k$, or a message from any $N_k$ that has received $m$, before $m'.ts + \Delta_{m'}$.  
        \end{itemize}
        
        If conditions $1,2$ and $3a$ are true, then it is not possible for $m$ to become known to $N_j$ as $m$ will not appear in an acknowledgement or vector clock of any $N_k$.  Similarly, if conditions $1,2$ and $3b$ are true, then $N_j$ will not know of $m$ in time to prevent $m'$ from being delivered before $m$ in the total order.  Similar to when $n=2$, if all three conditions hold (either $3a$ or $3b$), we can guarantee that $m$ and $m'$ are delivered via \textsf{Aramis} at all nodes.  We know that none of the $N_k$ nodes can deliver $m$ or $m'$ via \textsf{Base}, as $N_j$ cannot acknowledge a message until it has received it.  
        
        From both cases, $n=2$ and $n>2$, it is clear to see that in order for a message to be misordered, a chain of events need to occur that are highly unlikely based upon the probabilistic estimations made by the DMC and \emph{rbcast}.  
        		
    \subsection{ABcast Delivery Rule}
    Lastly, we present a concise formalisation of the delivery rule for the entire \textsf{ABcast} protocol.  A node, $N_j$ delivers any $m$ via \textsf{ABcast}, only after both $D1$ and $D2$ stated below are satisfied:
	    \begin{description}[labelindent=1cm]
	        \item[$\boldsymbol{D1}$] - The clock of $N_i > m.ts + \Delta_{m}$ ($D1_A$), or $m$ is acknowledged by all nodes other than $m.o$ ($D1_B$). 
	        \item[$\boldsymbol{D2}$] - All \emph{known} $m'$, with $m'.ts < m.ts$ have been delivered.
	    \end{description}
        
        
    \subsection{Initialisation Period}
     Unlike traditional \emph{abcast} protocols, such as TOA, the \textsf{ABcast} protocol requires a \textquoteleft{}warm-up' period before \emph{abcast}s can be sent between nodes.  This period is required in order to to synchronise the clocks of all participating nodes in the view, and to ensure that each node's DMC has recorded at least $NT_p$ latencies.  The synchronising of clocks is unavoidable due to \textsf{ABcast}s assumptions, and consequently, synchronisation must be performed first as the DMC is dependent on this assumption.  Our solution to recording $NT_p$ latencies, is to incorporate a mandatory \emph{probing} period that must be observed by all nodes in the current view after their clocks have been synchronised and before \emph{abcast}ing can begin.  

    The probing period required during initialisation utilises \textquoteleft{}empty' probe messages to record $NT_p$ latencies at each node's DMC.  An empty probe consists of a message, with a payload the size of those expected during \emph{abcast}ing, being unicast to all $n$ nodes in the current view.  This requires each node in the view to send at least $\frac{NT_P}{n}$ probes, however in reality the number should be higher to take into account that nodes will start the initialisation process at a different time.  With each subsequent probe broadcast after $x$ time has elapsed since the previous broadcast was sent; with $x$ being a value determined before run-time that should be an approximation of the expected frequency of \emph{abcast}s.  Once all nodes in the view have sent their probes, and recorded at least $NT_p$ latencies, it is possible for this node to start executing \emph{abcast}s.  The main disadvantages of utilising empty probes is the time required for each node to record $NT_p$ latencies and the difficulty of accurately predicting the correct frequency at which probes should be broadcast.  The latter is difficult, as the specified $x$ delay may be greater or less than the delay encountered between an application's \emph{abcast} requests, resulting in the network latencies being over or underestimated respectively.  
    
    \subsection{Adding New Nodes}
    When utilising \textsf{ABcast}, adding additional nodes to the current view is not as trivial as in more traditional deterministic protocols such as TOA.  Instead, when a new view is issued containing a new node, $N_i$, it is necessary for $N_i$ to undergo an initialisation period similar to that described in the previous section.  Clock synchronisation is simple, as $N_i$ can just contact the designated master node (as per \citep{Cristian:1996:SA:227210.227231}) and initiate the synchronisation protocol.  Recording the required $NT_p$ latencies is slightly trickier, as utilising a probing period similar to the initialisation period could have an adverse effect on all other nodes in the view.  This is because the existing nodes will most likely be heavily loaded from application requests, hence the need for an additional $s$-node, and adding additional load over a short period of time would be detrimental to performance.  Therefore, we propose that a better solution would be for new nodes to be \emph{silent watchers} until $NT_p$ latencies have been recorded.  A silent watcher, is a node that receives \emph{abcast}s from all other nodes in the view, but is unable to initiate its own \emph{abcast}s until after it has received $NT_p$ \emph{abcast}s and thus recorded $NT_p$ latencies.  When a new view is issued by the GM service, existing nodes include $N_i$ in the destination set of subsequent \emph{abcast}s, resulting in $N_i$ eventually receiving $NT_p$ messages.  Note, while $N_i$ is considered a silent watcher, it is still possible for it to participate in the redundant \emph{rbcast}s of message copies as the required timeout values are transmitted along with the message itself.      

\section{Flow Control}
The \textsf{ABcast} protocol described in this section functions as expected when each node's throughput is low and the number of requests per second is approximately five hundred.  However, the \textsf{ABcast} protocol discussed thus far has no flow-control, therefore as the number of requests per second increases, the protocol starts to become saturated by requests and performance deteriorates.  If the broadcast rate of a node is not restricted in any network protocol, it is possible for a \emph{congestive collapse} \citep{CongestiveCollapse, Jacobson:1988:CAC:52324.52356} to occur.  A congestive collapse is a situation whereby the current load on the network has saturated the underlying network, resulting in little to no throughput due to the rate of packet loss and the overall delay encountered by packets increasing.  The increase in network delays is typically caused by the need for data packets to be queued in a buffer at both the sending and receiving node, whilst the increase in packet loss is attributed to the overflowing of said buffers; packet loss exasperates the problem as additional network traffic is required when the lost packet is retransmitted.  For a congestive collapse to occur, its necessary for a network's input rate to exceed its output rate over an extended period of time, therefore in order to avoid such a collapse, it is necessary for the input rate of each node in the network to be throttled so that the network's average rate of input is approximately equal to its output rate.  

Congestive collapse is the worse case scenario for the network, however it is possible for similar symptoms to temporarily manifest themselves if the input rate of one, or more, nodes' sporadically increases.  Sporadic increases in a node's input rate is likely to cause large \emph{bursts} of messages to be flooded into the network over a very short period of time, placing more stress on the buffers of both the sending and receiving nodes.  This burstiness can cause the network to become temporarily congested, which does not lead to a total congestive collapse, however it is liable to cause increased packet loss and delays, resulting in a loss of throughput.  Therefore it is important for a flow control protocol to not only ensure that a congestive collapse does not occur, but also to ensure that each node transmits data at a consistent and stable rate.  

Due to its reliance on the PSM model and assumption \textbf{A4}, the \textsf{ABcast} protocol is more susceptible to the effects of network congestion than traditional deterministic protocols.  As the underlying network starts to become congested, then it becomes harder for the DMC to produce accurate estimates of future performance due to the variance in network delays and increased likelihood of packet losses.  Whilst assumption \textbf{A3} ensures that all nodes will eventually receive a lost packet, it is still possible for an increase in packet loss to have an adverse effect on the overall system performance, as it is still necessary for the missing packets to be retransmitted by the reliable UDP protocol.  Therefore additional packets are sent across the already congested network and the latencies recorded by the DMC will become larger.  The unpredictability of a congested network can cause the DMC to underestimate the latencies that will be encountered by future transmissions, leading to assumption \textbf{A4} being compromised.  Such an underestimation can result in more messages being delivered by \textsf{Aramis}, which in turn, increases the probability that the \emph{abcast} guarantess of \textsf{ABcast} will not be met due to messages being \emph{rejected} from the total order.  

Ultimately, a flow-control mechanism is required by \textsf{ABcast} nodes to ensure that the number of requests issued by a node, per second, does not adversely effect the performance of the underlying communication network.  The P2P \emph{abcast} protocol currently used by Infinispan, TOA, utilises a flow control scheme provided by JGroups, called \textsf{UFC} \citep{JGroupsUFC}, to control each node's broadcast rate.  \textsf{UFC} is based upon the \emph{sliding-window} approach to flow control \citep{bertsekas1992DataNetworksFC}, however in the JGroups literature they describe the sliding window concept in terms of a finite number of \emph{credits} that are maintained by each node; for completeness we utilise the same terminology when referring to \textsf{UFC}.  

The \textsf{UFC} protocol is based on the premise that a node's credits are expended when a new broadcast is sent and reimbursed when a destination node confirms receipt of the original broadcast.  Informally, the \textsf{UFC} protocol works as follows: A receiving node, $N_j$ reimburses the sending node $N_i$'s credits, by sending a response message to $N_i$ with $x$ amount of credits; where $x$ is equal to the number of bytes received by $N_j$.  If a sending node  attempts to broadcast a message equal to $y$ bytes, but its remaining credits $rc < y$ then the sending of a message $m$ becomes blocked until a a receiving node reimburses the sender for its earlier broadcasts or a configurable timeout period expires.  

The \textsf{UFC} approach works well for deterministic \emph{abcast} protocols such as TOA, however it is not well suited for use with \textsf{ABcast}, due \textsf{Aramis}'s reliance on the DMC's calculations for generating its probabilistic guarantees.  As previously stated, the \textsf{Aramis} protocol is heavily reliant on assumption \textbf{A4}, consequently, it is necessary to ensure that the DMC's observed latencies do not fluctuate unpredictably in a manner that would undermine \textbf{A4}.  The \textsf{UFC}'s independence from the DMC, ultimately means that it cannot determine whether the current load on the network is having an adverse effect on the latencies been measured by the DMC, and hence, \textsf{UFC} cannot take action in the event of the DMC's measurements deteriorating.  Similarly, the \textsf{UFC} approach can become overly-restrictive when utilised by \textsf{ABcast}, as it is possible for a node to block the broadcasting of a message due to insufficient credits even if the DMC is operating as expected and no large increases in network latencies have been observed.  Finally, the \textsf{UFC} protocol requires additional messages to reimburse each node's credit when a broadcast has been received by a destination.  This additional bandwidth requirement could alternatively be utilised by the underlying \emph{abast} protocol to increase its throughput if it were not required by \textsf{UFC}.  

Ultimately, a bespoke flow-control scheme is required by \textsf{ABcast} to ensure that optimal levels of throughput are maintained under heavy loads.  Such a protocol should utilise the DMC's measurements to control the send rate of \emph{abcast}s and to preserve the validity of assumption A4 where possible.  The remainder of this chapter details the design and implementation of such a protocol.  

    \subsection{Protocol Design}\label{sec:afc_protocol}
    In contrast to the \textsf{UFC} approach to flow control, our approach does not require additional messages, or the concept of a finite number of credits to restrict a node's transmission rate.  Instead, our solution depends entirely on the latencies measured by the DMC and its associated calculations.  Consequently, our flow control protocol is tightly coupled with the \textsf{ABcast} protocol and is not applicable for more traditional based \emph{abcast} protocol such as TOA.  The remainder of this section describes the rational behind our approach.  
    
    Assumption \textbf{A4} states:
    
    \begin{quotation}
            The maximum delay, $x_{mx}$, estimated by observing $NT_P$ number of transmissions from the recent past, will \emph{not} be exceeded during $NT_F$ number of future transmissions that unfold next, where $NT_F \leq NT_P$ with a high probability ($1 - q$).
        \end{quotation}
    
    As previously stated, when \textsf{ABcast} nodes start to become overwhelmed by broadcasts it is possible for \textbf{A4} to be undermined, resulting in future transmissions exceeding $x_{mx}$.  Therefore, we propose a new flow control protocol that utilises a \emph{rate-based} scheme \citep{bertsekas1992DataNetworksFC} that reduces a node's current broadcast rate if it starts to observe latencies greater than the last $x_{mx}$ value calculated.  We call this protocol ABcast Flow Control (AFC), and the basic design concept is as follows: A node is sending and receiving broadcast messages between a fixed destination set of nodes and the latencies encountered by each broadcast is recorded by all recipient nodes\footnote{As required by the \textsf{ABcast} protocol.}.  In the event that one or more of these latencies exceed the current $x_{mx}$ value, it is the responsibility of AFC to ensure that the local node adopts a lower broadcasting rate until a new $x_{mx}$ value is calculated.  At which time, the newly calculated $x_{mx}$ takes into account the large latencies observed in the recent past, reducing the probability of assumption \textbf{A4} being compromised by the latencies encountered in the near future.  If latencies continue to exceed $x_{mx}$ then the node's broadcast rate will become increasingly restricted, whereas if no violations of $x_{mx}$ occur, then no restrictions are placed on the node's broadcast rate. Thus a node's broadcast rate is only restricted when violations of assumption \textbf{A4} have occurred recently.  

    Unlike \textsf{UFC}, our approach restricts the sending rate of a node, $N_i$, based upon the messages it receives, not the rate at which $N_i$'s messages are received at other nodes in the network.  This may seem counterintuitive, however the \textsf{ABcast} protocol is based upon the assumption that the latencies observed by a given node, $N_i$, are representative of the latencies that will be encountered by $N_i$'s broadcasts sent to other nodes.  Therefore the AFC protocol simply utilises this assumption and applies it to flow control.  The AFC protocol is designed upon the assumption that if $N_i$'s observed latencies repeatedly exceed $N_i.x_{mx}$, then it is highly probable that $N_i$'s message buffer or the underlying network is approaching saturation.  In which case It is very likely that another node, $N_j$, will also be observing increased latencies due to similar circumstances, therefore it is necessary for $N_i$'s broadcast rate to be lowered in order to reduce the load on $N_j$.  This assumption is especially apt in the \emph{AmaaS} model when the \textsf{SCast} protocol is used, as each $c$-node randomly selects an $s$-node when sending a multicast request, resulting in client requests being evenly distributed between $s$-nodes.  Therefore each $s$-node is likely to issue approximately the same number of \emph{abcast}s, and thus, each node receives approximately the same number of messages.          
    
    Figure \ref{fig:abcast_components_afc} shows the new \textsf{ABcast} components diagram with the AFC protocol included.  Note that the AFC protocol is the highest in the stack, i.e. closest to the application, as all \emph{abcast} messages must be sent via the AFC protocol.  
        
    \begin{figure}[!h] 
        \centering    
         \includegraphics[width=0.8\textwidth]{components_with_fcc}
         \caption[\textsf{ABcast} Protocol Components with AFC]{\textsf{ABcast} Protocol Components with AFC}
         \label{fig:abcast_components_afc}
    \end{figure}        
    
    The remainder of this section focuses on the calculations used by a node to regulate its broadcast rate.  
    
    \subsection*{AFC Protocol}     
     This section introduces the variables utilised by AFC, explaining their significance and why they are required, before detailing the calculations that utilise these variables to alter a node's broadcast rate.  The calculations presented in this section assume that a single message $m$ is being broadcast.  
         
     The steps required by AFC to initiate a broadcast of $m$ are as follows: An application thread sends $m$ down the protocol stack, and upon receipt of $m$, the AFC protocol performs all of the calculations presented in this section in order to calculate a flow control delay.  This delay must be observed between the time of a node's previous broadcast and the broadcasting of $m$, with neither this thread or another application thread able to broadcast a subsequent message until $m$ has observed its delay.  Upon expiration of this delay, $m$ can be sent to \textsf{ABcast} for broadcasting.  Once $m$ has been broadcast, the applications request has been completed and its possible for the previously engaged thread to initiate a new broadcast.  Note, it is possible for other application threads to submit messages to the AFC protocol whilst $m$ is being handled, or waiting for its delay to expire, however these messages will not be processed until $m$ has been sent to \textsf{ABcast} for broadcasting.  
   
    A common technique for implementing rate-based flow control schemes, is the use of the \emph{leaky bucket scheme} \citep{Jain:1996:CCT:244118.244120, bertsekas1992DataNetworksFC} to regulate the sending of many messages.  In this scheme, messages are placed into a \textquoteleft{}bucket', until they are assigned a permit allowing them to be transmitted.  The rate at which permits are dispensed to messages, determines the broadcast rate of the node and is equivalent to our use of a broadcast delay.  It is common for  leaky bucket implementations to utilise buckets that consists of several messages, with the transmission of the bucket being delayed until all of its messages have acquired a permit; hence a broadcast delay has been observed by all nodes in the bucket.  This leads to messages being sent in batches from each node, with a large delay observed between subsequent buckets, opposed to individual messages being sent at a consistent rate.  
    
    As insinuated earlier, the AFC protocol does not utilise buckets, rather each individual message observes a broadcast delay that is dictated by a node's calculated broadcast rate.  AFC does not utilise buckets because of\textsf{ABcast}'s reliance on techniques such as acknowledgement piggybacking and vector clocks.  Sending messages in batches, means that \textsf{ABcast} is more likely to resort to sending explicit acknowledgement messages as \emph{abcast}s will not be initiated frequently by a node.  Instead large amounts of messages will be initiated over a short period of time, and in the interim period between consecutive buckets there will be no \emph{abcast}s for acknowledgements to be piggybacked on.  Similarly, \textsf{ABcast} is reliant on regularly receiving vector clocks attached to \emph{abcast}s from other nodes, to ensure that no messages in the total order have been missed.  If \emph{abcast}s were sent in batches, these clocks would not be received at a constant rate, rather they would be received in large batches semi-frequently.  In this interim period a node would be more likely to violate the \emph{abcast} total order as the node may not have received a vector clock from a given node for a large period of time, and hence it would be unaware of some messages that were broadcast in the recent past.  
   
   \subsubsection*{Protocol Parameters}  
   When the AFC protocol receives $m$ from the application, it polls the DMC to determine the number of latencies that have exceeded $x_{mx}$ at the present time.  These latencies are used to calculate the flow control delay for $m$, which ensures that $m$ and subsequent messages, are broadcast at the newly calculated rate.  Recall that the DMC utilises a $NT_F = 10\%$ of $NT_P$, where $NT_P = 1000$, and a new $x_{mx}$ value is calculated after every $NT_F$ latency has been recorded; in this case a new $x_{mx}$ value is calculated after every $100$ latencies observed.    We consider a latency $x$ to have exceeded $x_{mx}$ if $x > x_{mx}$ and $x$ is recorded between subsequent calculations of $x_{mx}$.  When $NT_F$ latencies have been recorded since the last calculation of $x_{mx}$, it is necessary for a new $x_{mx}'$ value to be calculated that incorporates the latency values that previously exceeded $x_{mx}$.  
       
    When a latency $x$ exceeds $x_{mx}$, we refer to this as a Marginal Peak $Mp$, as $x_{mx}$ is the boundary (margin) value and $Mp$ a latency that has peaked beyond the margin; we record $Mp$ as the difference between $x$ and $x_{mx}$, thus $Mp = x - x_{mx}$.  It is possible that multiple $x$ values will exceed $x_{mx}$, in which case we record all $Mp$ values and refer to the total number of $Mp$ values as $\#Mp$.   Once all $Mp$ values have been recorded, we calculate the variable $\mu$; which along with the current $x_{mx}$ value determines the amount that a node's broadcast rate should be restricted.  We calculate $\mu$ as:
    
    \begin{equation}
		     \begin{aligned}
		         \mu = \frac{Mp + Mp'+,\ldots,+ Mp''}{Max(\#Mp, Ss)}
		     \end{aligned}
    \end{equation} 
    
    Where $Ss$, which stands for sample size, is an integer constant defined before runtime that is used as a divisor when $\#Mp < Ss$.  The purpose of $Ss$ when calculating $\mu$, is to reduce the effects of a small number ($\#Mp < Ss$) of $Mp$ values from severely restricting a node's broadcast rate.  For example, consider  $x_{mx} = 2ms$ and a single $Mp$ occurs where latency $x$, $x = 4ms$, this would result in $Mp = 2ms$.  If the calculation of $\mu$ did not utilise $Ss$, then $\mu = 2$, which would produce a large flow control delay, which in turn would result in the broadcast rate being reduced significantly and the flow control becoming overly restrictive.  However, when we utilise the $Ss$ constant, the influence of a small number of latencies on the calculated $\mu$ value can be reduced; a large $Ss$ value marginalises their impact, whilst a small $Ss$ value promotes it.  The level of flow control required by a system is determined by its use case, therefore we defined $Ss$ as a constant specified before runtime in order to allow system administrators greater control of the protocol's behaviour.  
    
    The $\mu$ variable is used alongside $x_{mx}$ to calculate $\gamma$, where $\gamma$ is calculated as:
    
    \begin{equation}
		     \begin{aligned}
		         \gamma = \frac{x_{mx} + \mu}{x_{mx}}
		     \end{aligned}
    \end{equation} 
    
    \subsubsection*{Calculating a New Broadcast Rate}
    Let $\lambda_1$ represent the current broadcast rate of a given node, which we calculate as follows:

    \begin{equation}
		     \begin{aligned}
		         \lambda_1 = \frac{1}{m'.Fc_{ts} - m''.Fc_{ts}}
		     \end{aligned}
    \end{equation} 
    
    Where $m.FC_{ts}$ is a timestamp recorded for each message as it is sent to \textsf{ABcast} for broadcasting and $m''$ and $m'$ are the two messages that were handled by AFC prior to $m$, where $m''.Fc_{ts} < m'.Fc_{ts}$.  If $m''$ and $m'$ do not exist it is not possible to calculate the current broadcast rate, as no two messages have previously been sent, instead a default delay is utilised between broadcasts.  
    
    The timestamps $m''.Fc_{ts}$ and $m'.Fc_{ts}$ are recorded by a node's local clock, however it is irrelevant whether a node's local clock or the  synchronised clock provided by \textsf{ABcast} is utilised for recording the timestamp as calculating the broadcast rate is a local operation.  Therefore, as long as the same clock is used for recording all AFC timestamps the difference between message broadcast times will be adequate for calculating the current broadcast rate.   
    
    Once the current broadcast rate has been ascertained, it can be used alongside the $\gamma$ variable to calculate a new broadcast rate for this node and the duration of the delay that needs to be observed by $m$ to implement it.  A new delay duration for $m$ is not required if $\gamma = 0$ as this means that no $Mp$ values have been observed and $\lambda_1$ does not need to be reduced, therefore the delay value utilised by the previous message is still valid.  Conversely if $\gamma > 1$, we know that at least one $Mp$ has occurred and it is necessary for $\lambda_1$ to be reduced to a smaller rate.  We represent this new smaller rate as $\lambda_2$, where $\lambda_2 < \lambda_1$ and $\lambda_2$ is calculated as follows:
    
      \begin{equation}
		     \begin{aligned}
		         \lambda_2 = \lambda_1 \times e ^{ ({\frac{1-\gamma}{C}})}
		     \end{aligned}
    \end{equation} 
    
    To reduce a node's broadcast rate it is necessary to increase the delay observed between each subsequent message broadcast.  This is achieved by increasing $\lambda_1$ by $\delta$ such that:
    
    \begin{equation} \label{eq:rate_plus_delta}
		     \begin{aligned}
		         \frac{1}{\lambda_1} + \delta = \frac{1}{\lambda_2}
		     \end{aligned}
    \end{equation} 
    
    \begin{equation}
		     \begin{aligned}
		         \delta = (\frac{1}{\lambda_2} - \frac{1}{\lambda_1}) = \frac{\lambda_1 - \lambda_2}{\lambda_1 \times \lambda_2}
		     \end{aligned}
    \end{equation} 
        
    \begin{equation}
		     \begin{aligned}
		         \lambda_1 - \lambda_2 = \lambda_1 \times [1 - e ^{ ({\frac{1-\gamma}{C}})}]
		     \end{aligned}
    \end{equation} 
    
    Where $C$ is an integer constant defined before runtime, with a higher value corresponding to a more restrictive flow control protocol.  
    Therefore:
    
    \begin{equation}
		     \begin{aligned}
		         \delta = \frac{1 - e ^{ ({\frac{1-\gamma}{C}})}}{\lambda_2} = \frac{1}{\lambda_2} \times [1 - e ^{ ({\frac{1-\gamma}{C}})}]
		     \end{aligned}
    \end{equation}
    
    \begin{equation}
		     \begin{aligned}
		         \lambda_2 = \lambda_1 \times e ^{ ({\frac{1-\gamma}{C}})}
		     \end{aligned}
    \end{equation}     
    
    \begin{equation}
		     \begin{aligned}
		         \frac{1}{\lambda_2} = \frac{1}{\lambda_1} \times \frac{1}{e ^{ ({\frac{1-\gamma}{C}})}}
		     \end{aligned}
    \end{equation}
    
    Thus:
    
     \begin{equation}
		     \begin{aligned}
		         \delta = [1 - e ^{ ({\frac{1-\gamma}{C}})}] \times \frac{1}{\lambda_1} \times \frac{1}{e ^{ ({\frac{1-\gamma}{C}})}}
		     \end{aligned}
    \end{equation}
    
    Hence:
    
        \begin{equation}
		     \begin{aligned}
		         \delta = \frac{1}{\lambda_1}  \times   \frac{1 - e ^{ ({\frac{1-\gamma}{C}})}}{e ^{ ({\frac{1-\gamma}{C}})}}
		     \end{aligned}
    \end{equation}
    
    We then calculate the delay for $m$ simply as $m.Fc_ts = m'.Fc_ts + \delta$, based upon equation \ref{eq:rate_plus_delta} and assuming that the timestamp and delta are calculated using the same unit of time.  If the previous message, $m'$ was sent longer than $\delta$ time in the past, then $m$ can be broadcast instantly as the rate between broadcasts is still maintained.  
    
    The calculation of $\delta$ presented above is a very dynamic solution that can result in a node's $\delta$ value fluctuating dramatically over a period of time.  Although a dynamic flow control solution that reacts to the changing conditions of the network is desirable, early experiments showed that calculating $\delta$ as above resulted in very small delays being calculated for the majority of broadcasts.  This resulted in the broadcast rate of sending nodes not being restricted sufficiently over an extended period of time, causing larger numbers of $Mp$ values to suddenly appear. This sudden emergence of $Mp$ values results in the calculated $\delta$ value being extraordinarily high and the system's throughput becoming excessively restricted.  Over time, as new $x_{mx}$ values were calculated and $Mp$ values stopped appearing, the flow control began to increase the node's broadcast rate, however we found that $\delta$ eventually become too small again, causing a cycle to occur that consistently repeated itself.  Ultimately, solely utilising $\delta$ caused AFC to retrospectively react to a congested network, when the purpose of AFC is to be proactive and stop congestion from occurring.  
    
    Our solution, was to propose two new constants $\delta_{min}$ and $\delta_{max}$, which set a lower and upper bound on the calculated $\delta$ value, respectively.  Therefore the new $\delta$ must satisfy $\delta_{min} \leq \delta \leq \delta_{max}$.  The purpose of the lower bound $\delta_{min}$ is to ensure that all broadcasts are sent at a constant, predictable rate, in order to stabilise $x_{mx}$ and ensure that the system does not become excessively \emph{bursty}.  Conversely, the upper bound $\delta_{max}$ ensures that if a large number of $Mp$s occur between $x_{mx}$ calculations, the calculated $\delta$ value will not be excessively large and thus wont overly restrict a node's broadcast rate.  Like the other constants used by AFC, $\delta_{min}$ and $\delta_{max}$ are determined before runtime, and therefore appropriate values can be set for each depending on the network environment and the expected throughput of data. 

    \subsection{Limitations}
    The AFC protocol detailed in this section has proved to be an effective flow control protocol for use with \textsf{ABcast}, with the broadcast rates of nodes in the cluster being restricted sufficiently to prevent congestive collapse and minimise \textsf{Aramis} rejections ($\S$ \ref{ch:perf_eval}).  However, the AFC has  two key limitations.   
    
    The first limitation, is that the protocol is a rate-based scheme and therefore the limitations inherent with such an approach can be attributed to AFC by default.  Specifically, such an approach does not guarantee that no buffer overflows will occur as it is possible that the calculated rate of broadcast is too large.  This is especially true if the conditions of the network change dramatically over a short period, and in the case of \textsf{ABcast} this will also effect our $\Delta_m$ calculations.  However, the use of a relatively small $NT_p$ ensures that the calculated broadcast rate and $\Delta_m$ are frequently recalculated to reflect these changes.  
    
    The second limitation of AFC, is the use of $\delta_{min}$ and $\delta_{max}$ as upper and lower bounds on delays.  These bounds are required to ensure that the system is not overly restrictive, or permissive, of \emph{abcast} requests, however in the current design these values are specified before run-time and do not change based upon the networks current condition.  Therefore, it is possible that $\delta_{min}$ may be overly restrictive when the system is lightly loaded, whereas $\delta_{max}$ may permit too many broadcasts to be sent when the system is heavily loaded.  

\section{Summary}
This chapter presented \textsf{ABcast} - a new hybrid protocol that utilises both a deterministic (\textsf{Base}) and probabilistic (\textsf{Aramis}) protocol in order to create a non-blocking \emph{abcast} solution.  We detail the protocol's assumptions and required components, before detailing the delivery and rejection criteria of the two protocols.  This is followed by an in-depth description of the problems faced when utilising a probabilistic protocol such as \textsf{Aramis} and some example solutions.  Finally, we propose that a new flow control protocol is required in order to maximise the potential of the \textsf{ABcast} protocol.  