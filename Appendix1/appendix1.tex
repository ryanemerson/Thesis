\section{Protocol Details}\label{sec:decoupled_protocol}
% Change protocol name from Dcup
We have developed a protocol for the \textsf{AmaaS} model, which we call \textsf{SCast}; as the protocol offers multicast as a service, hence Service Multicast - \textsf{SCast}.  \textsf{SCast} satisfies all of the requirements specified in $\S$ \ref{sec:absaas_requirements} with the exception of S2.  S2 cannot be satisfied by our protocol directly, rather \textsf{SCast} relies on a \emph{abcast} protocol to guarantee S1, S3 and S4, therefore in order for S2 to be satisfied, the underlying \emph{abcast} protocol must not block in the presence of node failures.  

\textsf{SCast} consists of five distinct phases, each of which are explored below.  In the explanation below we assume that Infinispan is executing a 1-Phase Total Order transaction, without a second WSC phase, and that the transaction has already been successfully executed locally.  Furthermore, we assume that a reliable network protocol is being utilised as the underlying communication mechanism, for example TCP\citep{Cerf:2005:PPN:1064413.1064423} or Reliable UDP\citep{ReliableUDP}.  Finally, we refer to a collection of $s$-nodes providing the \emph{amcast} service as the \emph{multicast service}.  

	\begin{description}
		\item[1. Client Request - Client] \hfill \\
		Once a transaction coordinator, $Tx_i.c$, has completed its local execution of $Tx_i$ it is ready to \emph{amcast} a $prepare(Tx_i)$ message to $Tx_i.dst$ as required by the total order commit protocol.  In \textsf{SCast} \emph{amcasts} are initiated by the $Tx_i.c$ unicasting a \emph{amcast} request, $req(Tx_i)$, to all  $s$-nodes in the \emph{ordering service}.  The request, $req(Tx_i)$, contains the contents of a transaction's $prepare(Tx_i)$ message and the addresses of $Tx_i.dst$.  Each client request is associated with a unique id that consists of the $c$-node's address and a sequence number that is incremented after each request from this client.    
		
		\item[2. Receive Request - Multicast Service] \hfill \\
		Upon receiving $req(Tx_i)$, each $s$-node places the request in its \emph{Abcast Request Pool} (ARP), which is a bounded queue for storing requests before they are \emph{abcast} to all $s$-nodes.  If an $s$-node's ARP becomes full, subsequent requests from $c$-nodes are rejected until space becomes available in the ARP.  When a $c$-node request is rejected a \emph{reject} response is sent to $Tx_i.c$.  If $Tx_i.c$ receives a \emph{reject} response from all $s$-nodes, then it can either abort $Tx_i$ or resend the \emph{amcast} request after a configurable amount of time.    
		
		The ARP is necessary to ensure that if the \emph{ordering service} starts to become overloaded by client requests their is a 'feedback' mechanism that makes clients aware of the services current limitations, allowing clients to restrict user operations if necessary.  Utilising an ARP is also essential for providing message bundling, which as described in \ref{ssec:abaas_optimisations} is an effective optimisation for improving the throughput of the \emph{multicast service}.  
		
		\item[3. Process ARP - Multicast Service] \hfill \\
		A single thread, called the \emph{send} thread, is utilised for retrieving requests from the ARP and \emph{abcast}ing them to all $s$-nodes for ordering.  The \emph{send} thread retrieves ordering requests from the ARP in their arrival order, and bundles them into a single message bundle $mb$, before \emph{abcast}ing $mb$ to all $s$-nodes.  If the ARP is empty, then the \emph{send} thread waits for the ARP to become non-empty before resuming \emph{abcast}ing.  
		
		A configurable upper limit is placed on the maximum size (number of messages or \emph{bytes}) of a bundle message.  If this upper limit is reached and the ARP still has available requests, then the \emph{send} thread will \emph{abcast} the next message bundle $mb'$ once $mb$ has been \emph{abcast}.  If message bundling is not enabled, then a upper limit of one message is set for all bundles.    
		
		All \emph{abcast} $mb$ sent by an $s$-node have an originator field that is set to equal the sending node's address $N_s$, $mb.o$ = $N_s$, this is necessary for the next phase of the protocol.  
		
		\item[4. Process Requests and Multicast - Multicast Service] \hfill \\
		When an $s$-node, $N_s$, receives a request bundle $mb$, it 'un-bundles' $mb$ and processes each ordering request $req(Tx)$ in the order that they arrived in the ARP at $mb.o$.  If $N_s$ has already received $req(Tx)$ in a previous \emph{abcast} message, we discard this request and take no further action.  It is possible to discard a repeat request, as we know that all other $s$-nodes have, or will eventually, handle(d) the same copy of the request as $N_s$ due to the guarantees provided by \emph{abcast}.  Each accepted $req(Tx)$ is associated with a global timestamp $ts$: $req(Tx_i).ts = m.ts\oplus m.o \oplus$\emph{sequence number} of $req(Tx_i)$ within the bundle; where $\oplus$ is the append operator and $m.ts$ is the final timestamp provided by the underlying \emph{abcast} protocol utilised between $s$-nodes.  
		
		The $s$-node who's copy of $req(Tx_i)$ was first received by the $s$-nodes, and thus accepted, is responsible for multicasting a response message, $Rsp(Tx_i)$, containing the transaction and associated ordering data to all $Tx.dst$.  Delegating the multicasting of requests in this manner prevents $Rsp(Tx_i)$ being multicast by all $s$-nodes.  
		
		In addition to  the actual transaction, a multicast response $Rsp(Tx_i)$ consists of two types of ordering data: $ts$ agreed by $s$-nodes for $req(Tx_i)$, and $req(Tx_i)$'s \emph{immediate} predecessor data.  The latter is the identity of $Rsp(Tx_j)$ whose delivery at the specified $c$-node must \emph{precede} \emph{immediately} before delivery of $Rsp(Tx_i)$.  More precisely, all $d \in Rsp(Tx_i).dst$ must not deliver $Rsp(Tx_i)$ until they have delivered $Rsp(Tx_j)$, and only $Rsp(Tx_i)$ can be delivered immediately after $Rsp(Tx_j)$.
		
		The storage of \emph{immediate} predecessor data works as follows: All $s$-nodes maintain a map that stores a transaction history by mapping a $c$-node address with the id of the last transaction they were associated with, hence its \emph{immediate} predecessor.  So for each $req(Tx)$ the associated $req(Tx).ts$ is stored in the map for each $d \in Tx.dst$. When a $s$-node receives an \emph{abcast} bundle $mb$, it knows that all other $s$-nodes have received, or will receive, $mb$ in the same order.  Therefore, when $mb$ is processed by an $s$-node, it is guaranteed that all other $s$-nodes will have processed $mb$ in the exact same order, hence we know that the transaction history will be consistent across all $s$-nodes.  
		
		Note that the immediate predecessor of $Rsp(Tx_i)$ is applicable to \emph{all} \emph{amcast}s directed at a given $d$ - not just those that originate from $Tx_i.c$ nor just those that are handled by one $s$-node. Thus, it is specific to each $d \in Tx_i.dst$ and ensures that delivery at every $d$ is per the finalized $Rsp(Tx).ts$. To illustrate this, let $Tx_i.c$ send $req(Tx_i)$ to the \emph{multicast service}, with $N_s$'s \emph{abcast} copy being accepted , $Tx_j.c$ then sends $req(Tx_j)$ and $N_{s'}$'s copy is accepted by the service.  Assume $d \in Tx_i.dst \cap Tx_j.dst$ and the \emph{multicast service} orders $req(Tx_j)$ before $req(Tx_i)$, if $d$ receives $Rsp(Tx_i)$ before $Rsp(Tx_j)$ it will not deliver $Rsp(Tx_i)$ until it has delivered $Rsp(Tx_j)$.
    
    \item[5. Receive Multicast - Client] \hfill \\
    Upon receiving $Rsp(Tx_i)$, a $c$-node, $c$, will check the \emph{immediate} predecessor data that is applicable to $c$, in this case $Rsp(Tx_j)$.  If $Rsp(Tx_j)$ has been received by $c$ then $Rsp(Tx_i)$ can be delivered by $c$ and the $prepare(Tx_i)$ operation is executed.  However, if $Rsp(Tx_j)$ has not yet been received by $c$ then $Rsp(Tx_i)$ cannot be delivered locally, and $c$ must wait to receive $Rsp(Tx_j)$ before delivering $Rsp(Tx_i)$.  
    
    A single $ts$ is provided for each $d \in Tx.dst$ in the predecessor data, opposed to a list of past timestamps for each $d$, in order to reduce the size of each $Rsp(Tx)$.  This results in a cascading wait occurring if multiple messages have not yet been received by $c$.  For example, if $c$ has received $Rsp(Tx_i)$ but has not received its predecessors $Rsp(Tx_j)$ and $Rsp(Tx_k)$, $c$ is only aware of $m.j$, however when $Rsp(Tx_j)$ arrives, it reads $Rsp(Tx_j)$'s predecessor data and becomes aware that it has not yet received $Rsp(Tx_k)$ and must therefore wait for $Rsp(Tx_k)$ before delivering $Rsp(Tx_j)$ and $Rsp(Tx_i)$.  
    
   	\end{description}

	\subsection{Fault-Tolerance}
	Fault-tolerance in \textsf{SCast} must consider the consequences of both crashed $c$-nodes and $s$-nodes.  Here we explore the consequences of both $c$-node and $s$-node crashes during various stages of a \textsf{SCast} \emph{amcast}.  For the sake of simplicity, we only consider node crashes from the perspective of a single transaction, however it is worth noting that each $c$-node would typically have multiple transactions executing concurrently.  
	
	\paragraph{Client Node Crash}
	\begin{description}
         \item[\emph{Local Tx Execution}]  \hfill \\
         If a $c$-node, $Tx.c$, crashes during or directly after the local execution of $Tx_j$, then no action needs to be taken for this $Tx$, as no interactions with other nodes has occurred.  
		
	    \item[\emph{Phase 1}]  \hfill \\
	    If $Tx.c$ crashes during the unicasting of a request to the \emph{multicast service}, then three scenarios are possible: 
		    \begin{itemize}
			    \item    No $s$-nodes receive the request, in which case the multicast will never complete and no further actions are required.
			    \item    Not all of the $s$-nodes receive the original request, in which case the nodes who do receive the request will execute as normal.  All $s$-nodes will eventually receive details of the transaction, as the other $s$-nodes \emph{abcast} their copy of the request between service members, and the transaction will be multicast as normal.  
			    \item    Only one $s$-node receives a copy of the request and that $s$-node also crashes, in which case no further action is required as no other $s$-node is aware of the request.  
		    \end{itemize}        	
		
	    \item[\emph{Phase 2-5}]  \hfill \\
	    Finally, its possible for $Tx.c$ or any other $d \in Tx.dst$ to crash after the \emph{multicast service} has received the original request, in which case the service will continue to process the request as normal and multicast the message to all of the operative destinations.  
    \end{description}
    
	\paragraph{Service Node Crash}
	\begin{description}
       \item[\emph{Phase 1}] \hfill \\
       If a $s$-node crashes while a $Tx.c$ is issuing a service request, then the \emph{amcast} can still succeed as the client request is unicast to all $s$-nodes, therefore one of the remaining $s$-nodes will handle the request.  
	
       \item[\emph{Phase 2-3}] \hfill \\
       If a $s$-node crashes after receiving a request $req(Tx_i)$, then another $s$-node will eventually receive $req(Tx_i)$, as $Tx.c$ unicasts the request to all $s$-nodes.  
    
        \item[\emph{Phase 4}] \hfill \\
	    Its possible for a $s$-node, $N_s$, to crash just after it has been designated as the multicasting $s$-node for $Rsp(Tx_i)$.  In which case, it is necessary for the remaining $s$-nodes to take responsibility for multicasting $Rsp(Tx_i)$ to ensure that all $Tx_i.dst$ receive the transaction.  
	    
	    The remaining $s$-nodes can determine which requests still require multicasting if meta data is piggybacked onto each \emph{abcast} sent between $s$-nodes.  For example, if each operative $s$-node: 
	    \begin{enumerate}
	    \item    Piggybacks the timestamp of the latest request it has successfully responded to; where success is defined as the $Rsp$ message being multicast to all destinations.
	    \item    Maintains a recent history of client requests that have been accepted; storing the transaction as well as the address of the $s$-node whose \emph{abcast} request was accepted by the service.  
	    \end{enumerate}. 
	    
	    When an $s$-node crash is detected the remaining $s$-nodes iterate through their recent history of client requests, starting at the timestamp of the last confirmed multicast to be completed by the crashed $s-$node.  Each subsequent client request that was the responsibility of the crashed $s$-node to respond to, which has not been completed, is then handled by the operative $s$-node.  This will potentially cause multiple $s$-nodes to multicast the same $Rsp(Tx_i)$ message, however $c$-nodes can simply discard any duplicate transmissions that are received from the \emph{multicast service}.  
	    
	    \item[\emph{Stage 5}] \hfill \\
	    A $s$-node crash at this stage of the protocol has no effect on the outcome of the \emph{amcast}, as the $Rsp$ message has already been multicast to all destinations.  
    \end{description}